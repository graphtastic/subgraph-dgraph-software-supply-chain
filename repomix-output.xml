This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
compose/
  dgraph.yml
  guac.yml
  mesh.yml
  tools.yml
docs/
  design/
    design--faq-discarded-ideas.md
    design--guac-to-dgraph.md
    on--dgraph-docker-compose.md
    on--node-union-antipattern.md
    on--object-identification-in-graphql.md
    on--running-multiple-docker-compose-stacks.md
  guac-demo-compose.yaml
guac-mesh-graphql/
  scripts/
    extractor.ts
    extractor.wip.ts
  .gitignore
  Dockerfile
  package.json
  README.md
  tsconfig.json
tools/
  github-supply-chain-analyzer/
    README.md
.env.example
.gitignore
CONTRIBUTING.md
CONTRIBUTORS.md
docker-compose.yml
LICENSE
LICENSE.code
LICENSE.docs
Makefile
NOTICE
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="tools/github-supply-chain-analyzer/README.md">
# GitHub Supply Chain Security Analyzer

A powerful command-line tool for analyzing release artifacts and CI/CD workflows in GitHub repositories to identify software supply chain security metadata like SBOMs.

## Features

-   **Comprehensive Data Collection**: Fetches repository details, descriptions, and links.
-   **Release Analysis**: Gathers the last 3 releases and inspects all associated artifacts.
-   **Artifact Identification**: Automatically flags potential SBOMs (SPDX, CycloneDX), signatures (`.sig`, `.asc`), and other attestations.
-   **CI/CD Workflow Inspection**: Enumerates all GitHub Actions workflows and analyzes their content for common SBOM generation tools (e.g., `syft`, `trivy`, `cdxgen`).
-   **Type-Safe API Calls**: Uses GraphQL Code Generator to create a fully-typed TypeScript SDK for the GitHub API.
-   **Smart Caching**: Caches API responses to speed up subsequent runs and reduce API usage.
-   **Dual-Format Reporting**: Generates a detailed `report.json` and an easy-to-use `report.csv`.

## Prerequisites

-   **Node.js**: Version 18.x or later.
-   **npm**: Comes bundled with Node.js.
-   **GitHub Personal Access Token (PAT)**: You need a PAT with the `repo` scope to query repository data.
    -   Go to [GitHub Developer Settings](https://github.com/settings/tokens) to generate a new token (classic).
    -   Ensure it has `repo` scope to access public and private repository data.

## Installation & Setup

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd github-supply-chain-analyzer
    ```

2.  **Install dependencies:**
    ```bash
    npm install
    ```

3.  **Configure Environment Variables:**
    Create a `.env` file in the root of the project and add your GitHub PAT:
    ```env
    GITHUB_PAT=ghp_YourPersonalAccessTokenHere
    ```

4.  **Define Target Repositories:**
    Open `src/config.ts` and add the owner/repo pairs you want to analyze to the `repositories` array.
    ```typescript
    // src/config.ts
    export const repositories = [
      { owner: 'kubernetes', name: 'kubernetes' },
      { owner: 'sigstore', name: 'cosign' },
      { owner: 'anchore', name: 'syft' },
      // Add more repositories here
    ];
    ```

5.  **Generate the GraphQL SDK:**
    This step introspects the GitHub GraphQL schema and generates a typed SDK based on your queries.
    ```bash
    npm run codegen
    ```
    You only need to re-run this if you change the GraphQL queries in `src/graphql/`.

## Running the Analysis

Execute the main script from the project root:

```bash
npm start
</file>

<file path="docs/design/design--faq-discarded-ideas.md">
### **Design FAQ: Architectural Alternatives & Discarded Paths**

This document serves as an appendix to the primary design for the `subgraph-dgraph-software-supply-chain` Spoke. Its purpose is to record the architectural due diligence performed during the design phase. By documenting the alternatives considered and the explicit rationale for their rejection, we provide clarity on the chosen path and create a valuable resource that preserves the context of our architectural decisions.

#### **1. Q: Why not just use the GraphQL API with batched mutations for all data loading instead of a file-based bulk loader?**

**A:** This was the primary architectural trade-off considered for data ingestion. While using batched GraphQL mutations is the standard pattern for real-time and incremental updates, it is fundamentally ill-suited for the initial, large-scale data seeding required by this project. The RDF N-Quad + Dgraph Loader approach was chosen specifically to handle the massive performance requirements of the initial load.

The optimal production architecture is a hybrid approach: use the bulk loader for the initial seed, and reserve batched mutations for potential future near-real-time synchronization needs.

**Comparative Analysis: Ingestion Strategy**

| Feature Axis | Strategy A: Batched GraphQL Mutations | Strategy B: RDF N-Quads + Dgraph Loaders |
| :--- | :--- | :--- |
| **Performance (Initial Load)** | **Poor to Moderate.** Limited by network latency and transactional overhead. Would be orders of magnitude too slow for terabyte-scale seeding. | **Excellent.** `dgraph bulk` is an offline tool optimized for maximum ingestion speed, bypassing the live API. |
| **Implementation Complexity** | **Higher.** Requires custom logic for buffering, batching, API error handling, and retries. | **Lower.** The extractor's logic is simple (stream text to a file). Complexity is handled by Dgraph's mature, purpose-built CLI tools. |
| **Transactional Guarantees** | **Strong & Granular.** Each batch is a single, atomic transaction. | **Different Model.** `dgraph bulk` is an all-or-nothing offline operation. |
| **Coupling / Portability** | **Low.** The extractor only depends on the standard GraphQL endpoint. | **High.** The output format and loading mechanism are tightly coupled to Dgraph's specific tooling. |

---

#### **2. Q: Why use RDF N-Quads as the intermediate format instead of a more GraphQL-native JSON format?**

**A:** The choice of an intermediate format required balancing fidelity to the GraphQL object model, portability, and the practical requirements of Dgraph's high-performance loaders. RDF N-Quads were selected as the optimal pragmatic choice. They are a true graph format, are a portable industry standard, and are natively supported by Dgraph's loaders. While formats like JSON-LD are conceptually closer to the GraphQL ecosystem, they would require an extra, costly transformation step before they could be loaded into Dgraph, negating their benefits.

**Comparative Analysis: Intermediate Data Format**

| Format | Fidelity to GraphQL API Model | Portability / Interoperability | Performance for Dgraph Loading |
| :--- | :--- | :--- | :--- |
| **RDF N-Quads** | Good. Maps well to a graph, but is a triple model, not an object model. | High. A W3C standard for graph data interchange. | **Excellent.** Natively supported and highly optimized in Dgraph loaders. |
| **JSON-LD** | Very Good. It's JSON, which is native to the ecosystem, but with full graph semantics. | **Excellent.** A W3C web standard. | Moderate. Requires a pre-processing step to convert to N-Quads or Dgraph JSON. |
| **Dgraph JSON** | Very Good. Maps almost 1:1 to GraphQL objects. | **None.** Completely proprietary to Dgraph. | **Excellent.** Native format for Dgraph. |
| **GraphQL JSON Result**| Perfect. It *is* the API model. | Poor. Not a graph format; requires complex re-hydration logic. | Poor. Would require a custom loader to parse and convert into mutations. |

---

#### **3. Q: Could we use a hybrid `JSON-LD -> Dgraph JSON` pipeline to stay in the JSON ecosystem longer?**

**A:** This hybrid approach was considered as a way to leverage JSON-native tooling. However, the analysis concluded that this pipeline would introduce significant, unnecessary complexity. The central step, transforming from the JSON-LD graph format to the Dgraph JSON graph format, cannot be accomplished with simple tools like `jq`. It requires writing a new, custom, and stateful transformation application responsible for mapping entity identifiers and restructuring the graph. This custom tool would be complex to build and maintain, directly contradicting the goal of avoiding "bespoke hilarity." The original RDF pipeline is simpler, more performant, and leverages mature, existing tools more effectively.

---

#### **4. Q: Why not use a high-performance binary format like Jelly for the intermediate data artifact?**

**A:** A binary format like Jelly was explored for its potential performance and compression benefits. The idea was discarded for several critical, practical reasons. Most importantly, Dgraph's loaders do not support Jelly, which would necessitate an extra conversion step from Jelly back to N-Quads, eliminating any performance gain. Furthermore, the opacity of a binary format severely hinders debuggability compared to a simple, human-readable text format like N-Quads.

**Comparative Analysis: Intermediate Artifact Format**

| Evaluation Criteria | Gzipped N-Quads (Chosen Path) | Jelly (Discarded Path) | Winner |
| :--- | :--- | :--- | :--- |
| **End-to-End Performance** | **High.** `GraphQL -> N-Quad Text -> Dgraph`. One simple transformation step. | **Lower.** `GraphQL -> Jelly Binary -> N-Quad Text -> Dgraph`. Requires two full transformation steps. | **N-Quads** |
| **Implementation Complexity** | **Low.** The extractor script performs simple, stateless string formatting. | **High.** Requires a specialized library and a new Jelly-to-N-Quad converter tool. | **N-Quads** |
| **Developer Experience (Debuggability)** | **Excellent.** Can be inspected with standard shell tools (`zcat`, `head`, `grep`). | **Poor.** Opaque binary blob. Requires a specialized dumper tool for inspection. | **N-Quads** |
| **Portability / Interoperability**| **Excellent.** N-Quads are a W3C standard supported by the entire RDF ecosystem. | **Low.** Jelly is a niche format not directly usable by most other graph systems. | **N-Quads** |
</file>

<file path="docs/design/on--node-union-antipattern.md">
# An Architectural Analysis of the Node Union Anti-Pattern in GraphQL

## Introduction to Abstract Types in GraphQL Schema Design

### The GraphQL Schema as a Contract

At the core of any GraphQL-powered architecture lies the schema. It is more than a mere description of data; it is a formal, strongly-typed contract that governs all interactions between clients and the server. This contract meticulously defines the API's capabilities, enumerating the available data types, the relationships between them, and the operations (queries, mutations, subscriptions) that clients can perform. The rigidity and clarity of this contract are GraphQL's primary strengths, enabling powerful developer tooling, fostering independent client and server development, and eliminating entire classes of bugs common in less structured APIs.

However, the power of this contract is directly proportional to the quality of its design. A well-designed schema can solve the vast majority of challenges often attributed to GraphQL itself, such as performance bottlenecks, excessive complexity, and difficult onboarding for new developers. Conversely, a poorly designed schema, one that fails to accurately model the business domain or anticipate the needs of its consumers, can create a brittle, inefficient, and frustrating system. The fundamental principle of effective schema design is to "think in graphs"—to model the business domain as an interconnected graph of entities and their relationships, rather than a simple projection of underlying database tables. The choices made during this modeling process, particularly regarding how to represent complex relationships, have profound and lasting architectural consequences.

### The Need for Polymorphism: Modeling Heterogeneous Data

Real-world business domains are rarely composed of simple, homogeneous data structures. A social media feed is a prime example: it is a heterogeneous list that might contain text updates, photo galleries, video posts, and shared articles. Similarly, a platform's search functionality might need to return results from a wide array of distinct entity types, such as users, products, articles, or events. A schema must be able to model these scenarios where a single field can return one of several different object types. This capability is known as polymorphism.

Without a mechanism for polymorphism, developers would be forced into clumsy workarounds, such as returning a separate list for each possible type (e.g., `bookResults`, `movieResults`, `albumResults`). This approach is deeply flawed, as it prevents server-side ranking and pagination across the entire result set and burdens the client with the complex task of merging and sorting these disparate lists. To address this fundamental requirement, the GraphQL specification provides two distinct mechanisms for defining polymorphic fields: union types and interface types.

### A Primer on Abstract Types

Union and interface types are collectively known as abstract types because they do not, by themselves, represent a concrete set of data. Instead, they define a range of possibilities for what a field can return.

*   **Union Types:** A union is a schema construct that declares a field can return one of a specific, closed set of object types. The member types of a union are completely independent and are not required to share any common fields. A union essentially models an "OR" relationship: a `SearchResult` can be a `Book` OR an `Author`.
*   **Interface Types:** An interface defines a contract in the form of a set of fields. Any object type that "implements" this interface must include all of the fields defined by that contract, with matching types and nullability. An interface models an "IS A" relationship: a `Textbook` IS A `Book`, and a `ColoringBook` IS A `Book`.

The choice between these two abstract types is not merely a matter of syntax or preference. It is a foundational architectural decision that dictates the degree of coupling between the server and its clients, profoundly influencing the system's capacity for graceful evolution. A union creates a tight coupling by defining a closed, explicit list of possibilities; any change to this list on the server necessitates a corresponding change on the client. An interface, by contrast, creates a looser coupling through an open, implicit contract based on shared capabilities; the server can introduce new implementing types without breaking clients that are programmed to the contract. This report will critically analyze a common misuse of union types—the "node union" anti-pattern—and demonstrate why leveraging interface types leads to more robust, scalable, and maintainable GraphQL APIs.

## A Deep Dive into GraphQL Union Types

To understand why using a union can be an anti-pattern, one must first appreciate its intended purpose and mechanics. When used correctly, the union type is an exceptionally powerful tool for creating expressive and type-safe schemas. This section provides a detailed technical examination of its anatomy and showcases its canonical use case.

### Anatomy of a Union Type

A union type is a composite type that allows a field to return one of several distinct object types. Its definition and consumption involve specific mechanisms on the server and client.

#### Schema Definition (SDL)

In the GraphQL Schema Definition Language (SDL), a union is declared with the `union` keyword, followed by its name and a list of the concrete object types it can represent, separated by a pipe (`|`).

```graphql
# A union representing possible search result types
union SearchResult = Book | Author

type Book {
  title: String!
}

type Author {
  name: String!
}

type Query {
  search(contains: String): [SearchResult]
}
```

In this example, the `search` field can return a list containing both `Book` objects and `Author` objects. A critical characteristic of unions is that their member types must be concrete object types (not scalars, enums, or other unions) and they are not required to share any fields. `Book` and `Author` are entirely independent types.

#### Server-Side Resolution

Because a union field can return different types, the GraphQL server needs a mechanism to determine the specific GraphQL type of a given data object at runtime. This is accomplished by implementing a `__resolveType` function in the resolver map for the union type. This function receives the resolved object and must return a string that matches the name of one of the union's member types.

The logic within `__resolveType` typically inspects the object for unique properties to discriminate between the possible types.

```typescript
// Example resolver map for the SearchResult union
const resolvers = {
  SearchResult: {
    __resolveType(obj, context, info) {
      // Check for a property unique to the Author type
      if (obj.name) {
        return 'Author';
      }
      // Check for a property unique to the Book type
      if (obj.title) {
        return 'Book';
      }
      // If the type cannot be determined, return null
      // This will result in a GraphQLError being sent to the client
      return null;
    },
  },
  Query: {
    search: (parent, { contains }) => {
      //... logic to fetch books and authors
      // Example return data that matches the union types
      return [
        { name: "J.R.R. Tolkien" },
        { title: "The Hobbit" },
        { name: "George R.R. Martin" }
      ];
    },
  },
};
```

If the `__resolveType` function returns `null` or a string that does not correspond to a valid member type of the union, the GraphQL execution engine will produce an error for that field. An alternative to implementing `__resolveType` is to ensure that the resolved data objects include a `__typename` property containing the correct type name; the default resolver will use this property if it exists.

#### Client-Side Consumption

From the client's perspective, a field returning a union type presents an ambiguity that must be resolved within the query itself. Since the union itself guarantees no common fields, the client cannot select fields directly on the `SearchResult` type. Instead, it must use inline fragments (`... on TypeName`) to specify which fields to retrieve for each possible concrete type.

```graphql
query GetSearchResults {
  search(contains: "Tolkien") {
    # The __typename meta-field is crucial for client-side logic
    __typename
    ... on Book {
      title
    }
    ... on Author {
      name
    }
  }
}
```

In this query, the client asks for the `title` if the returned object is a `Book`, and the `name` if it is an `Author`. It is a strongly recommended best practice to always query for the `__typename` meta-field when dealing with abstract types. This field returns the name of the concrete object type as a string (e.g., `"Book"` or `"Author"`), allowing client-side code to correctly parse the response and apply the appropriate logic.

### The Canonical Use Case: Type-Safe Error Handling

The defining characteristic of a union—that it represents a closed, explicit set of possible types—makes it the ideal tool for a powerful schema design pattern: treating business logic errors as first-class citizens of the API. Traditionally, GraphQL errors are returned in a top-level `errors` array in the response, often with generic messages that lack the rich, structured context needed for sophisticated client-side error handling.

The "Response Type Pattern" (also known as the "Result Union Pattern") elevates error handling by modeling the possible outcomes of an operation, both success and failure, directly within the schema. Instead of a mutation returning either the data or `null` with a generic error, it returns a union that encompasses a success payload and one or more specific, typed errors.

Consider a mutation to create a new user:

```graphql
type Mutation {
  createUser(input: CreateUserInput!): CreateUserResult!
}

input CreateUserInput {
  email: String!
  fullName: String!
}

# The success payload
type CreateUserSuccess {
  user: User!
}

# A specific, structured error type
type DuplicateEmailError {
  message: String!
  email: String!
}

# Another specific error type
type InvalidInputError {
  message: String!
  field: String!
}

# The union of all possible outcomes
union CreateUserResult = CreateUserSuccess | DuplicateEmailError | InvalidInputError

# Assuming a User type exists for completeness
type User {
  id: ID!
  email: String!
  fullName: String!
}
```

This pattern is exceptionally powerful. The union provides a complete, discoverable, and type-safe enumeration of every possible business outcome for the `createUser` mutation. The client knows, by inspecting the schema, that it must handle not only the success case but also the specific failure modes of a duplicate email or an invalid input field. This design transforms error handling from a reactive, string-parsing exercise into a proactive, type-safe process.

The utility of unions in this context is deeply connected to the concept of sum types (also known as tagged or discriminated unions) found in many functional and statically-typed programming languages like TypeScript, OCaml, or Rust. These language features are used to model a state space that is a discrete and exhaustive set of possibilities. A union in GraphQL serves the exact same purpose. A client written in a language like TypeScript can leverage code generation tools to create a corresponding discriminated union type, enabling exhaustive pattern matching (e.g., via a `switch` statement) on the `__typename` of the result. If the server later introduces a new error type by adding it to the `CreateUserResult` union, the client's type-checker will immediately flag the switch statement as non-exhaustive, forcing the developer to consciously handle the new failure case. This compile-time safety is a profound benefit. In this scenario, the closed and explicit nature of the union is its greatest strength, ensuring that the contract between client and server for all possible outcomes remains robust and unambiguous.

## The "Node Union" as an Anti-Pattern: A Critical Analysis

While union types are a powerful tool for modeling discrete outcomes, their application to represent collections of core domain entities—a practice referred to as the "Node Union Anti-Pattern"—is a significant architectural misstep. This pattern introduces brittleness, inefficiency, and conceptual ambiguity into a schema, undermining the long-term goals of scalability and maintainability.

### Defining the Anti-Pattern

The Node Union Anti-Pattern is the use of a union to model a polymorphic field that returns a collection of primary domain entities, or "nodes." This pattern is most frequently encountered in the design of search functionalities or any feature that presents a heterogeneous list of items.

The canonical example of this anti-pattern is the `SearchResult` union:

```graphql
# The Node Union Anti-Pattern in action
union SearchResult = Book | Movie | Album

type Query {
  search(term: String!): [SearchResult]!
}

# Assume Book, Movie, Album types are defined elsewhere for completeness
type Book { id: ID!, title: String! }
type Movie { id: ID!, title: String! }
type Album { id: ID!, title: String! }
```

Here, `Book`, `Movie`, and `Album` are distinct, core entities within the application's domain. On the surface, this union seems like a logical way to represent that a search can yield items of these different types. However, this approach fundamentally misunderstands the architectural implications of union types and conflates the problem of modeling a closed set of operational outcomes (like errors) with the very different problem of modeling an open, extensible set of domain entities. The consequences of this design choice are severe and manifest across the entire system lifecycle.

### Consequence 1: Schema Brittleness and Poor Evolvability

The most damaging consequence of the Node Union Anti-Pattern is the tight coupling it creates between the server's schema and its clients. The definition of a union is a closed, exhaustive list of its members. This means that any addition of a new member to the union constitutes a breaking change for all consuming clients.

Consider the evolution of the `SearchResult` union. If the business decides to make podcasts searchable, the schema must be updated:

```graphql
# A breaking change to the schema
union SearchResult = Book | Movie | Album | Podcast

# Assume Podcast type is defined elsewhere for completeness
type Podcast { id: ID!, title: String! }
```

While this change is syntactically valid, it breaks the contract with existing clients. A client application that was built to handle only `Book`, `Movie`, and `Album` types is now unprepared for the possibility of receiving a `Podcast` object. In the best-case scenario, if the client is using a strongly-typed language and code generation, this change will cause a compilation error, forcing a developer to update the client's handling logic. In the worst-case scenario—a dynamically typed client without exhaustive checks—the new `Podcast` type will be ignored, leading to silent data loss where search results simply fail to appear in the UI without any obvious error.

This tight coupling forces a lock-step deployment process: the schema cannot be evolved without a coordinated, and often complex, update and release of all client applications. In an ecosystem with multiple independent client teams (e.g., iOS, Android, Web), a public-facing API, or a federated architecture with numerous downstream consumers, this level of coupling is untenable. It stifles innovation, increases development friction, and makes the entire system brittle and resistant to change.

### Consequence 2: Inefficient and Verbose Client-Side Querying

The second major drawback of the Node Union Anti-Pattern is the cumbersome and inefficient querying experience it imposes on clients. Because a union does not and cannot enforce any shared fields among its members, clients are forced into verbose and repetitive query structures.

Even if `Book`, `Movie`, and `Album` all logically possess a `title` and a `coverImage`, the client cannot query for these fields directly on `SearchResult`. The fields must be requested explicitly within an inline fragment for each and every possible type.

```graphql
# Verbose and repetitive query required by the union
query FindMedia {
  search(term: "space") {
    __typename
    ... on Book {
      title
      coverImage { url }
    }
    ... on Movie {
      title
      coverImage { url }
    }
    ... on Album {
      title
      coverImage { url }
    }
  }
}
```

This verbosity increases the size of the query document, which can have performance implications, and it places a significant cognitive burden on the developer, who must remember to request the same common fields for every member of the union. This is a frequent source of subtle bugs, where a developer might add a new type to their handling logic but forget to query for one of the common fields, leading to missing data in the UI.

Furthermore, this pattern is fragile when dealing with fields that share a name but have conflicting types. For example, if `Book` had `title: String!` and `Movie` had `title: String`, the GraphQL validation rules would reject the query because the `title` field would have a conflicting response shape. Unions provide no mechanism to resolve this; the schema itself would be considered invalid if it tried to group such types.

### Consequence 3: Impediment to a Cohesive Domain Model

Perhaps the most subtle but architecturally significant flaw of the Node Union Anti-Pattern is that it reflects a failure to model the business domain cohesively. This pattern often emerges from a "bottom-up" or database-first approach to schema design, where the GraphQL types are simple, one-to-one mappings of underlying database tables or microservice responses. This is the opposite of the "demand-oriented" or "client-centric" design philosophy that is a hallmark of effective GraphQL adoption.

The `SearchResult` union describes *what the things are* (a book, a movie, an album) but completely fails to capture the more important abstraction: *what they have in common from the perspective of a search result*. A client application displaying search results does not primarily care that one item is a `Book` and another is a `Movie`; it cares that both items are "searchable," that both have a "display title," a "URL to their detail page," and a "preview snippet." The union is incapable of modeling this shared conceptual identity.

By failing to create this shared abstraction, the schema offloads the work of conceptual modeling onto every single client. Each client must independently reconstruct the notion of a "searchable item" through its repetitive fragment logic. This leads to duplicated logic, a weaker and less expressive domain model, and a schema that is merely a data-access layer rather than a powerful, self-describing model of the business domain. The anti-pattern is therefore not just a technical misuse of a language feature; it is a philosophical failure to leverage GraphQL's full potential as a tool for building well-designed, product-centric APIs.

## The Superior Alternative: GraphQL Interfaces

The weaknesses exposed by the Node Union Anti-Pattern are directly and elegantly solved by GraphQL's other abstract type: the interface. An interface is the correct tool for modeling collections of heterogeneous domain entities because it is designed to capture shared capabilities and promote a loosely coupled, evolvable schema.

To frame the discussion, the following table provides a concise, at-a-glance comparison of the critical differences between union and interface types, highlighting their distinct purposes and architectural implications.

| Feature Axis                | Union Type                                                              | Interface Type                                                                     |
| :-------------------------- | :---------------------------------------------------------------------- | :--------------------------------------------------------------------------------- |
| **Core Concept**            | A closed set of distinct object types. "This field returns A OR B."     | An open contract of shared fields. "This field returns something that IS A."       |
| **Shared Field Enforcement** | None. Member types are independent.                                     | Strict. Implementing types MUST include all interface fields.                      |
| **Client Querying Pattern** | Must use inline fragments for all fields, even if shared. Verbose.      | Can query common fields directly on the interface. Concise.                        |
| **Schema Evolution Impact** | Adding a new type is a **breaking change** for clients.                 | Adding a new implementing type is **non-breaking** for clients.                    |
| **Server-Side Contract**    | Explicitly lists all possible member types.                             | Defines a set of fields; does not know all its implementers.                       |
| **Primary Use Cases**       | Type-safe error handling, discrete state representation.                | Modeling shared capabilities (e.g., `Searchable`, `Node`), polymorphism.         |
| **Federated Architecture**  | Problematic. A central union definition creates tight coupling across subgraphs. | Essential. Interfaces are key to defining shared entities across subgraphs.        |

### Anatomy of an Interface

An interface defines a set of fields that serves as a common contract for object types. Any object type that implements the interface is guaranteed to have those fields.

#### Schema Definition (SDL)

An interface is declared with the `interface` keyword. Object types then use the `implements` keyword to declare that they adhere to the interface's contract.

```graphql
# An interface defining the contract for any searchable entity
interface Searchable {
  id: ID!
  displayTitle: String!
  url: String!
}

# Object types now implement the Searchable contract
type Book implements Searchable {
  id: ID!
  displayTitle: String!
  url: String!
  author: Author!
}

type Movie implements Searchable {
  id: ID!
  displayTitle: String!
  url: String!
  director: String!
}

# Assuming Author type exists for completeness
type Author {
  name: String!
}
```

The core principle of an interface is the contract: `Book` and `Movie` *must* include the `id`, `displayTitle`, and `url` fields, with the exact types and nullability specified in the `Searchable` interface. This enforcement is validated by the GraphQL server. In addition to the required fields, implementing types are free to add their own specific fields, such as `author` for `Book` and `director` for `Movie`.

#### Server-Side Resolution

Similar to union types, fields that return an interface type also require a `__resolveType` function in the resolver map. The purpose is identical: to inspect the resolved data object and return a string representing its concrete GraphQL type name, allowing the server to correctly apply type-specific field resolvers.

### Refactoring the Anti-Pattern: From SearchResult Union to Searchable Interface

Applying this understanding, the `SearchResult` anti-pattern can be refactored into a robust, interface-based design.

**Before (Anti-Pattern):**

```graphql
union SearchResult = Book | Movie | Album
```

**After (Refactored with Interface):**

```graphql
interface Searchable {
  id: ID!
  displayTitle: String!
  url: String!
  previewImage: Image
}

type Book implements Searchable {
  # Required fields from Searchable
  id: ID!
  displayTitle: String!
  url: String!
  previewImage: Image

  # Book-specific fields
  author: Author!
}

type Movie implements Searchable {
  # Required fields from Searchable
  id: ID!
  displayTitle: String!
  url: String!
  previewImage: Image

  # Movie-specific fields
  director: String!
}

type Album implements Searchable {
  # Required fields from Searchable
  id: ID!
  displayTitle: String!
  url: String!
  previewImage: Image

  # Album-specific fields
  artist: Artist!
}

type Query {
  search(term: String!): [Searchable]!
}

# Placeholder types for completeness
type Image { url: String! }
type Author { name: String! }
type Artist { name: String! }
```

This refactoring fundamentally changes the client's interaction with the schema. The verbose, repetitive query required by the union is replaced by a clean, concise, and more intuitive query.

```graphql
# Clean and efficient query enabled by the interface
query FindMedia {
  search(term: "space") {
    __typename
    # Common fields are queried ONCE, directly on the interface
    id
    displayTitle
    url
    previewImage { url }

    # Inline fragments are used ONLY for type-specific fields
    ... on Book {
      author { name }
    }
    ... on Movie {
      director
    }
    ... on Album {
      artist { name }
    }
  }
}
```

The ability to query common fields directly on the `Searchable` field is a dramatic improvement. It reduces query complexity, eliminates redundancy, and makes the client's intent much clearer.

### How Interfaces Solve the Problems

The interface-based approach directly addresses each of the critical flaws of the Node Union Anti-Pattern.

#### Evolvability

The schema is no longer brittle. If the business decides to introduce a new searchable entity, such as `Podcast`, the change is additive and non-breaking.

```graphql
type Podcast implements Searchable {
  # Required fields from Searchable
  id: ID!
  displayTitle: String!
  url: String!
  previewImage: Image

  # Podcast-specific fields
  host: String!
}
```

No changes are required to the search query or the `Searchable` interface itself. Existing clients that query for the common fields defined by `Searchable` will continue to function perfectly. When a `Podcast` object is returned by the search query, these clients will gracefully render its `id`, `displayTitle`, and other shared fields without any code modification. This loose coupling is the hallmark of a well-designed, evolvable API.

#### Query Efficiency

As demonstrated above, the query is far more efficient and maintainable. Redundancy is eliminated, reducing the potential for human error and making the query easier to read and understand. The client logic is simplified, as it can rely on the presence of the common fields for all items in the returned list.

#### Domain Modeling

Most importantly, the `Searchable` interface provides a vastly superior domain model. It moves beyond simply listing concrete data types and instead captures a shared *capability* or *behavior*. It establishes a powerful abstraction that communicates to API consumers that `Book`, `Movie`, and `Album` are not just disparate entities; they are all things that share the quality of being "searchable."

This shift from identity-based modeling ("What is this thing?") to capability-based modeling ("What can this thing do?") is a crucial step toward designing sophisticated and reusable schema components. An object can implement multiple interfaces, allowing for a rich, compositional model of its capabilities (e.g., `type Post implements Node & Timestamped & Commentable & Searchable`). This level of expressive power is impossible to achieve with union types and is fundamental to building a truly scalable and understandable GraphQL schema. The adoption of interfaces for polymorphic node collections is therefore not just a localized fix for a search problem; it is a strategic move towards a more mature and robust architectural philosophy.

## Advanced Architectural Considerations and Parallels

The decision between union and interface types transcends simple schema syntax; it has profound implications for system architecture, especially in modern distributed environments. Understanding these broader connections solidifies the argument for interfaces as the default choice for polymorphic entity modeling and reveals the deeper principles at play.

### Interfaces in a Federated Architecture

In a federated GraphQL architecture, the complete API schema (the "supergraph") is composed from the schemas of multiple independent backend services (the "subgraphs"). For example, a `Products` service might define the core `Product` type, while a `Reviews` service adds a `reviews` field to that `Product` type, and an `Inventory` service adds stock information.

Interfaces are the cornerstone of this architectural pattern. They provide the mechanism for defining a shared entity that can be referenced and extended across different subgraphs. A common pattern is for a core entity to implement the `Node` interface, providing a globally unique ID.

```graphql
# In the Products subgraph
type Product implements Node @key(fields: "id") {
  id: ID!
  name: String!
}

# In the Reviews subgraph
extend type Product @key(fields: "id") {
  id: ID! @external
  reviews: [Review]! # Adding reviews field
}

# Assuming Node and Review types exist for completeness in context of federation
interface Node {
  id: ID!
}
type Review {
  id: ID!
  rating: Int!
  comment: String
}
```

This model allows for clear separation of concerns while maintaining a unified, cohesive graph for clients. A union is fundamentally incompatible with this distributed model. The definition of a union is a closed, explicit list of all its members. This list would have to be defined in a single, centralized location. If the Products subgraph defined `union Thing = Product` and the Users subgraph wanted to add `User` to that union, it would create a circular dependency and a central point of coupling, completely defeating the purpose of federation. Interfaces, with their open and extensible contract, are the only viable mechanism for modeling shared entities in a distributed graph.

### Parallels with Graph Database Modeling

The principles guiding GraphQL schema design often find parallels in the world of native graph databases (e.g., Neo4j, DSE Graph). When modeling nodes in a graph database, a common decision is whether to use many highly specific labels or fewer, more generic labels with properties to differentiate types.

1.  **Multiple Specific Labels:** Creating distinct vertex labels for `(:Book)`, `(:Movie)`, and `(:Album)`. This approach is analogous to using a GraphQL union.
2.  **Generic Label with a Property:** Using a single label like `(:Media {type: 'book'})` or `(:Media {type: 'movie'})`. This is analogous to a GraphQL interface, where `Media` is the interface and `type` is a discriminating field.

Graph database best practices often caution against creating an excessive number of unique vertex labels, especially if the nodes share many properties and query patterns. Reusing a more generic label can improve storage efficiency and simplify traversal queries. For instance, instead of `recipeAuthor`, `bookAuthor`, and `reviewAuthor` labels, a single `author` label is often more effective. This external validation from a related domain reinforces the architectural soundness of preferring a shared abstraction (interface) over a collection of disparate types (union) for modeling conceptually related entities.

### Advanced Patterns: Marker Interfaces and Hybrid Approaches

The distinction between unions and interfaces is not always absolute. Advanced patterns exist that combine their characteristics to solve specific problems with greater elegance.

#### Marker Interfaces

It is possible to define an interface with no fields, such as `interface Searchable {}`. This is sometimes referred to as a "marker interface." Its sole purpose is to act as a semantic grouping for a set of types. While discussed in early GraphQL specification proposals as a potential alternative to unions, its practical utility is limited. Without any guaranteed common fields, it offers little advantage over a union for client-side querying and is generally less useful than a concrete interface that defines a meaningful contract.

#### The Hybrid Error Pattern: Synthesis of Safety and Flexibility

The most robust and sophisticated patterns often arise from composing fundamental primitives. The type-safe error handling pattern, previously identified as the canonical use case for union types, can be further enhanced by combining it with an interface.

```graphql
# The most robust error handling pattern
interface Error {
  message: String!
}

type DuplicateEmailError implements Error {
  message: String!
  email: String!
}

type InvalidInputError implements Error {
  message: String!
  field: String!
}

union CreateUserResult = CreateUserSuccess | DuplicateEmailError | InvalidInputError

# Assuming CreateUserSuccess and User are defined elsewhere, for example:
# type CreateUserSuccess { user: User! }
# type User { id: ID!, email: String!, fullName: String! }
```

This hybrid approach provides the best of both worlds. The union gives clients compile-time safety and the ability to perform exhaustive pattern matching on the specific error types. The interface provides a fallback contract for generic error handling. A client can now choose its level of engagement:

*   **A simple client** can handle all errors generically by querying for the fields on the `Error` interface: `... on Error { message }`. This client is flexible and will not break if a new error type is added to the union in the future.
*   **A sophisticated client** can use a `switch` on `__typename` to provide custom UI for `DuplicateEmailError` and `InvalidInputError`, while using a default case that falls back to handling the generic `Error` interface. This allows for both specific, rich error handling and future-proof flexibility.

This hybrid pattern elegantly resolves the inherent architectural tension between compile-time safety (the domain of unions) and runtime flexibility (the domain of interfaces). It demonstrates that the highest level of schema design is achieved not by rigidly choosing one tool over the other, but by deeply understanding their fundamental properties and composing them to create solutions that are simultaneously safe, flexible, and expressive.

## Recommendations and A Practical Decision Framework

The preceding analysis has established the significant architectural drawbacks of the Node Union Anti-Pattern and the clear superiority of interfaces for modeling polymorphic domain entities. This final section synthesizes these findings into a set of actionable recommendations and a practical decision framework to guide developers in crafting robust, scalable, and maintainable GraphQL schemas.

### A Decision Framework for Choosing an Abstract Type

When faced with a field that must return multiple object types, developers can use the following series of questions to determine the appropriate abstract type. This framework promotes a deliberate, architecture-aware design process.

1.  **Question: Is the set of possible types a closed, finite, and exhaustive representation of all possible outcomes for a single, discrete operation?**
    *   **Example:** The success and failure states of a single mutation (e.g., `CreateUserSuccess`, `DuplicateEmailError`).
    *   **Guidance:** If **YES**, a union is likely the correct choice. Its primary strength lies in modeling these "sum types," providing complete type safety and enabling exhaustive client-side pattern matching.
2.  **Question: Do the types represent core domain entities that share a common conceptual identity or set of capabilities from the client's perspective?**
    *   **Example:** A collection of `Books`, `Movies`, and `Articles` that are all "searchable" or a set of `Posts`, `Comments`, and `Users` that are all identifiable via a global `Node` system.
    *   **Guidance:** If **YES**, an interface is the strongly preferred choice. It allows you to model this shared identity as a formal contract, leading to cleaner queries and a more expressive domain model.
3.  **Question: Is it likely that this collection of types will need to evolve by adding new members in the future?**
    *   **Example:** A search feature that initially includes `Products` and `Brands` but may later need to include `Stores` and `Articles`.
    *   **Guidance:** If **YES**, an interface is almost certainly the correct choice. Adding a new implementing type to an interface is a non-breaking change for clients that are programmed against the interface's contract. Adding a new member to a union is a breaking change that requires coordinated client updates.
4.  **Question: Are you designing the schema for a distributed or federated architecture?**
    *   **Example:** An entity like `User` that is defined in an Accounts subgraph but needs to be extended with profile information from a Profiles subgraph.
    *   **Guidance:** If **YES**, you **must** use an interface to model shared entities across service boundaries. Unions are fundamentally incompatible with the decentralized nature of a federated graph.

### Best Practices for Schema Design

Adhering to a set of guiding principles will consistently lead to higher-quality schemas that stand the test of time.

*   **Design for the Client, Not the Database:** The most critical principle is to practice demand-oriented design. The GraphQL schema is a product for your client applications. Its structure should be dictated by the needs of the UI and the use cases it must support, not by the shape of the underlying database tables or microservice responses.
*   **Prefer Interfaces for Entity Collections:** As a general rule, any field that returns a list of different domain entities should use an interface. This pattern promotes evolvability, query efficiency, and a richer domain model.
*   **Use Unions for Discriminated Outcomes:** Reserve the use of union types for their canonical purpose: modeling the finite, discrete outcomes of an operation, most notably for type-safe error handling.
*   **Provide Clear Documentation:** Leverage GraphQL's built-in description strings (documentation comments) to explain the purpose of every type, field, interface, and union in the schema. Clear documentation is essential for API consumers and dramatically reduces the friction of adoption and integration.
*   **Plan for Evolution:** A schema is a living document that will change as the application's requirements evolve. Always make design choices that favor flexibility and backward compatibility. Prefer additive, non-breaking changes and use tools like deprecation (`@deprecated` directive) to manage the lifecycle of fields gracefully.

### Conclusion: Building Robust and Scalable GraphQL APIs

The "Node Union Anti-Pattern" represents more than a simple syntactic error; it is a misuse of a specialized tool for a general-purpose problem, stemming from a misunderstanding of the fundamental architectural principles that underpin a well-designed GraphQL API. A union creates a tightly coupled, brittle contract that is ill-suited for modeling an extensible set of domain entities. Its proper application lies in defining the closed, finite set of outcomes for a specific operation, where its compile-time safety provides immense value.

For the task of modeling polymorphic collections of core domain entities, the interface is the unequivocally superior tool. It establishes a flexible, capability-based contract that decouples clients from the server, enabling graceful schema evolution. It promotes cleaner, more efficient queries and encourages the development of a richer, more expressive domain model that captures shared behaviors rather than just disparate data shapes.

By internalizing the decision framework and best practices outlined in this report—and by embracing interfaces as the default mechanism for polymorphic entity modeling—development teams can construct GraphQL schemas that are not only powerful and flexible in their initial implementation but are also robust, maintainable, and scalable for years to come. The ultimate goal is a schema that serves as a stable, intuitive, and powerful contract, empowering client developers and forming the solid foundation of a successful application architecture.

## Works Cited

1.  Spicy Take 🌶️: Every issue or complaint against GraphQL can be traced back to poor schema design - Reddit, accessed September 1, 2025, [https://www.reddit.com/r/graphql/comments/1gwp08q/spicy\_take\_every\_issue\_or\_complaint\_against/](https://www.reddit.com/r/graphql/comments/1gwp08q/spicy_take_every_issue_or_complaint_against/)
2.  Schemas and Types - GraphQL, accessed September 1, 2025, [https://graphql.org/learn/schema/](https://graphql.org/learn/schema/)
3.  Design a GraphQL Schema So Good, It'll Make REST APIs Cry - Part 1 | Tailcall, accessed September 1, 2025, [https://tailcall.run/blog/graphql-schema/](https://tailcall.run/blog/graphql-schema/)
4.  GraphQL Best Practices, accessed September 1, 2025, [https://graphql.org/learn/best-practices/](https://graphql.org/learn/best-practices/)
5.  GraphQL: Union vs. Interface - Artsy Engineering, accessed September 1, 2025, [https://artsy.github.io/blog/2019/01/14/graphql-union-vs-interface/](https://artsy.github.io/blog/2019/01/14/graphql-union-vs-interface/)
6.  GraphQL Tour: Interfaces and Unions | by Clay Allsopp | The ..., accessed September 1, 2025, [https://medium.com/the-graphqlhub/graphql-tour-interfaces-and-unions-7dd5be35de0d](https://medium.com/the-graphqlhub/graphql-tour-interfaces-and-unions-7dd5be35de0d)
7.  Learn GraphQL: Interfaces & Unions, accessed September 1, 2025, [https://graphql.com/learn/interfaces-and-unions/](https://graphql.com/learn/interfaces-and-unions/)
8.  Combining GraphQL multiple queries to boost performance - Contentful, accessed September 1, 2025, [https://www.contentful.com/blog/graphql-multiple-queries/](https://www.contentful.com/blog/graphql-multiple-queries/)
9.  Understanding GraphQL Union types - Stack Overflow, accessed September 1, 2025, [https://stackoverflow.com/questions/54572432/understanding-graphql-union-types](https://stackoverflow.com/questions/54572432/understanding-graphql-union-types)
10. All About GraphQL Abstract Types. And Why Union Types are Far ..., accessed September 1, 2025, [https://medium.com/swlh/all-about-graphql-abstract-types-2da8f18e11a0](https://medium.com/swlh/all-about-graphql-abstract-types-2da8f18e11a0)
11. Unions and interfaces - Apollo GraphQL Docs, accessed September 1, 2025, [https://www.apollographql.com/docs/apollo-server/v2/schema/unions-interfaces](https://www.apollographql.com/docs/apollo-server/v2/schema/unions-interfaces)
12. Unions and Interfaces - Apollo GraphQL Docs, accessed September 1, 2025, [https://www.apollographql.com/docs/apollo-server/schema/unions-interfaces](https://www.apollographql.com/docs/apollo-server/schema/unions-interfaces)
13. Unions and Interfaces - Apollo GraphQL Docs, accessed September 1, 2025, [https://www.apollographql.com/docs/apollo-server/schema/unions-interfaces/](https://www.apollographql.com/docs/apollo-server/schema/unions-interfaces/)
14. Is there an alternative solution GraphQL resolveType for union types? - Stack Overflow, accessed September 1, 2025, [https://stackoverflow.com/questions/79150212/is-there-an-alternative-solution-graphql-resolvetype-for-union-types](https://stackoverflow.com/questions/79150212/is-there-an-alternative-solution-graphql-resolvetype-for-union-types)
15. Introspection - GraphQL, accessed September 1, 2025, [https://graphql.org/learn/introspection/](https://graphql.org/learn/introspection/)
16. Better GraphQL Error Handling with Typed Union Responses - DEV Community, accessed September 1, 2025, [https://dev.to/mnove/better-graphql-error-handling-with-typed-union-responses-1e1n](https://dev.to/mnove/better-graphql-error-handling-with-typed-union-responses-1e1n)
17. GraphQL Adoption Patterns, accessed September 1, 2025, [https://www.apollographql.com/docs/graphos/resources/guides/graphql-adoption-patterns](https://www.apollographql.com/docs/graphos/resources/guides/graphql-adoption-patterns)
18. Common Anti-Patterns in GraphQL Schema Design: A Comprehensive Guide - mobileLIVE, accessed September 1, 2025, [https://mobilelive.medium.com/common-anti-patterns-in-graphql-schema-design-a-comprehensive-guide-ec0997e95efb](https://mobilelive.medium.com/common-anti-patterns-in-graphql-schema-design-a-comprehensive-guide-ec0997e95efb)
19. Proposal: remove union types and allow interfaces to have no fields ..., accessed September 1, 2025, [https://github.com/graphql/graphql-spec/issues/236](https://github.com/graphql/graphql-spec/issues/236)
20. Getting the Best of TypeScript and GraphQL: Union Types | Hive, accessed September 1, 2025, [https://the-guild.dev/graphql/hive/blog/typescript-graphql-unions-types](https://the-guild.dev/graphql/hive/blog/typescript-graphql-unions-types)
21. Domain Modeling with Tagged Unions in GraphQL, ReasonML, and TypeScript, accessed September 1, 2025, [https://dev.to/ksaldana1/domain-modeling-with-tagged-unions-in-graphql-reasonml-and-typescript-2gnn](https://dev.to/ksaldana1/domain-modeling-with-tagged-unions-in-graphql-reasonml-and-typescript-2gnn)
22. Interface vs Union in GraphQL schema design - Stack Overflow, accessed September 1, 2025, [https://stackoverflow.com/questions/72826748/interface-vs-union-in-graphql-schema-design](https://stackoverflow.com/questions/72826748/interface-vs-union-in-graphql-schema-design)
23. GraphQL union and conflicting types - Stack Overflow, accessed September 1, 2025, [https://stackoverflow.com/questions/46774474/graphql-union-and-conflicting-types](https://stackoverflow.com/questions/46774474/graphql-union-and-conflicting-types)
24. How closely should your schema match your data-model? : r/graphql - Reddit, accessed September 1, 2025, [https://www.reddit.com/r/graphql/comments/16tjg9g/how\_closely\_should\_your\_schema\_match\_your/](https://www.reddit.com/r/graphql/comments/16tjg9g/how_closely_should_your_schema_match_your/)
25. Demand Oriented Schema Design - Apollo GraphQL Docs, accessed September 1, 2025, [https://www.apollographql.com/docs/graphos/schema-design/guides/demand-oriented-schema-design](https://www.apollographql.com/docs/graphos/schema-design/guides/demand-oriented-schema-design)
26. Graph anti-patterns | DataStax Enterprise, accessed September 1, 2025, [https://docs.datastax.com/en/dse/5.1/graph/anti-patterns.html](https://docs.datastax.com/en/dse/5.1/graph/anti-patterns.html)
27. Common anti-patterns in GraphQL schema design - LogRocket Blog, accessed September 1, 2025, [https://blog.logrocket.com/anti-patterns-graphql-schema-design/](https://blog.logrocket.com/anti-patterns-graphql-schema-design/)
</file>

<file path="docs/design/on--object-identification-in-graphql.md">
# **Object Identification in GraphQL: A Definitive Report on Specification, Convention, and Modern Practice**

## **Part I: The Foundational Distinction: Specification vs. Convention**

A frequent point of confusion in GraphQL schema design is the role and requirement of unique identifiers. The assumption that every object, or "node," must have a unique id field is pervasive, yet its origin is often misunderstood. This report begins by establishing the crucial distinction between the formal GraphQL language specification, which is minimalist and flexible, and the powerful community-driven conventions that have been built upon it to enable the rich ecosystem of tooling that defines modern GraphQL development.

### **Section 1.1: What the GraphQL Specification Mandates (and What It Doesn't)**

The foundational GraphQL specification is fundamentally unopinionated regarding the presence of a unique id field on object types. A GraphQL schema can be perfectly valid according to the official specification without defining a single id field on any of its types. The core language is concerned with describing the capabilities of an API through a strongly-typed schema, allowing clients to request the exact shape of data they need.1

The specification does provide a built-in scalar type called ID. Its definition explicitly notes its intended purpose: "A unique identifier, often used to refetch an object or as the key for a cache".1 The

ID type is serialized as a String, but its designation signals to developers and tools that its value is not intended to be human-readable.1 However, the specification merely provides this

ID type as a tool within the type system; it does not mandate its use on any object type.1 The design principles of GraphQL—being product-centric, hierarchical, and client-specified—focus on the structure and retrieval of data, not on prescribing a universal identity for every piece of data within that structure.2

This minimalism is a deliberate and critical architectural feature, not an oversight. The creators of GraphQL designed it to be a versatile query layer that could be placed in front of any number of disparate and heterogeneous data sources, including legacy databases, microservices, and third-party APIs.3 Many of these underlying systems may not have a concept of a unique, stable identifier for every piece of data they expose. For example, a data source might serve aggregated reports, transient log entries, or computed data that has no persistent identity. If the GraphQL specification had mandated a universal

id: ID\! field for all object types, it would have created a significant barrier to adoption, making it impossible to model these common data sources. By remaining unopinionated on object identity, the specification preserves the flexibility that is core to GraphQL's value proposition, allowing it to serve as a truly universal interface to existing systems.

### **Section 1.2: The Emergence of Convention: The Global Object Identification Specification**

The widespread assumption that every node requires a unique id stems not from the language specification but from a powerful and highly influential *convention*, formally captured in the GraphQL Global Object Identification (GOI) Specification.4 This convention was born from the practical necessities of building sophisticated client applications that required robust mechanisms for caching and data refetching.4

The origins of this specification are directly tied to Facebook's Relay, a JavaScript framework for building data-driven React applications.7 The GOI specification was, for a long time, known as the "Relay Global Object Identification Specification".8 The Relay framework's architecture is built on two core assumptions about a compliant GraphQL server: that it provides a mechanism for refetching an object by a unique identifier and a standardized way to page through connections.9 The former assumption is entirely dependent on the patterns defined in the GOI specification. To "elegantly handle for caching and data refetching," clients needed servers to expose object identifiers in a standardized way.5

This dynamic reveals a fundamental tension within the GraphQL ecosystem that has defined its evolution. The core specification provides a flexible, unopinionated language. However, to build powerful, feature-rich tools like automated caching clients (e.g., Relay, Apollo Client), those tools must impose their own, more stringent conventions on top of the language. The immense utility and improved developer experience offered by these tools led to the widespread adoption of their underlying conventions as community "best practices".8 This creates a powerful feedback loop: developers are often introduced to GraphQL through these opinionated client libraries and frameworks. They learn the conventions of the tools first and naturally assume them to be de jure requirements of the language itself. This explains why the user's initial query is so common; the de facto standard established by the tooling ecosystem is often mistaken for the de jure standard of the formal specification.

## **Part II: A Deconstruction of the Global Object Identification Pattern**

To understand why the Global Object Identification (GOI) convention has become a cornerstone of modern GraphQL development, it is necessary to deconstruct its components. The specification is simple in its definition but profound in its implications, establishing a formal contract between the server and client that enables a new class of application functionality.

### **Section 2.1: The Node Interface**

At the heart of the GOI specification is the Node interface. This interface establishes a simple yet powerful contract: any object type that can be uniquely identified and refetched by the client must implement this interface.4 The specification is precise and unambiguous in its requirements: "The server must provide an interface called

Node. That interface must include exactly one field, called id that returns a non-null ID".4

A schema that adheres to this specification would define the interface as follows:

GraphQL

\# An object with a Globally Unique ID  
interface Node {  
  \# The ID of the object.  
  id: ID\!  
}

An object type, such as User, would then implement this interface to signal its compliance with the contract:

GraphQL

type User implements Node {  
  id: ID\!  
  name: String\!  
  \#... other fields  
}

The most critical aspect of this contract is the nature of the id field. It must be a *globally unique identifier*.12 This means that an

id for a User object must not collide with an id for a Product object or any other object type in the entire schema. This guarantee of global uniqueness is what allows the refetching mechanism to be simple and universal, as the client can request any node without needing to know its type beforehand.

### **Section 2.2: The node Root Field**

The node root field is the functional counterpart to the Node interface. It provides a single, standardized entry point on the root Query type for clients to refetch any object in the graph, provided they know its globally unique ID.4 The specification mandates a root field named

node that accepts exactly one argument, id: ID\!, and returns the Node interface.6

The contract is explicit: if a client performs a query and receives an object that implements Node with a specific id, passing that same id value back to the node root field *must* return the identical object.4 A typical refetch query would look like this:

GraphQL

query RefetchUserQuery($userId: ID\!) {  
  node(id: $userId) {  
    id  
   ... on User {  
      name  
      email  
    }  
  }  
}

The specification includes a pragmatic "best effort" clause. It acknowledges that a server must make a best effort to fulfill the refetch request, but it may not always be possible. For example, the underlying database may be unavailable, or the requested object may have been deleted since it was last queried.4 This clause provides a necessary escape hatch for real-world operational failures.

### **Section 2.3: Guarantees of the Specification: Stability and Equality**

The GOI specification provides guarantees that extend beyond simple refetching, creating a foundation of data consistency that clients can rely upon. These guarantees of equality and stability transform the id from a simple identifier into a key that unlocks a powerful data consistency model.

The specification mandates object equality: "If two objects appear in a query, both implementing Node with identical IDs, then the two objects must be equal".4 This is a profound guarantee. It means that if a client requests

User with id "VXNlcjox" via a users list and also requests the same user via the node(id: "VXNlcjox") field in the same operation, the server promises that all fields queried on both representations of that user will be identical.

This equality is defined recursively through the principle of field stability. For any field queried on two objects with identical IDs, the results must be equal. This holds true whether the fields return scalars, enums, or even nested objects.4 This contract allows the client to trust the server's data implicitly. When a client receives data from multiple queries—for instance, a

viewer query might return a user object, and a post.author query might return what appears to be the same user object—it needs to know if these can be safely merged in its local cache. The GOI specification's id provides the key for this merge, and the equality and stability rules provide the *guarantee* that this merge is safe and correct. This trust is what enables a client to aggressively normalize its cache, confident that two objects with the same global ID are indeed the same canonical entity.

## **Part III: The Critical Role of Identifiers in the GraphQL Ecosystem**

The principles of object identification, particularly as formalized by the GOI specification, are not merely academic. Their adoption has profound and far-reaching consequences on the entire GraphQL development lifecycle, most notably on the client side, where they enable sophisticated patterns for state management, performance optimization, and architectural scalability.

### **Section 3.1: The Linchpin of Client-Side Caching**

Unique identifiers are the fundamental mechanism that enables modern GraphQL clients to perform data normalization. In a traditional REST architecture, the URL of a resource serves as a natural, globally unique identifier that can be used for HTTP caching.14 GraphQL, operating over a single endpoint, lacks this URL-based primitive. The

id field, as defined by convention, serves as its direct replacement, providing a globally unique key that clients can leverage to build rich, intelligent caches.14

Client libraries like Apollo Client and urql's graphcache use this identifier to transform the tree-like structure of a GraphQL query response into a flat, normalized data store.16 The process typically involves generating a unique cache key for each identifiable object by combining its

\_\_typename (a meta-field that returns the object's type name) with its key field, which is usually id or \_id. For example, a User object with an id of 123 would be stored in the cache under the key User:123.15

This normalized object is stored in a flat lookup table. When the same object—identified by the same cache key—appears in multiple query responses, its fields are merged into this single, canonical entry in the cache.16 This elegant process accomplishes two critical goals: it prevents data duplication, reducing the memory footprint of the application, and it ensures data consistency across the entire user interface. The cache can then fulfill future queries for that object, or even for individual fields of that object, directly from this local store without needing to make an unnecessary network request.16

### **Section 3.2: Consequences of Absent or Non-Unique Identifiers**

Operating a GraphQL API without a reliable system of unique identifiers severely cripples the capabilities of modern client libraries and forces developers back into a world of manual state management. This regression leads to a cascade of predictable and difficult-to-solve problems, including performance degradation, UI bugs, and a dramatic increase in application complexity.

* **Cache Ineffectiveness and Data Duplication:** If an object in a response lacks a unique identifier, the client cache cannot normalize it. Some clients may fall back to using the object's path within the query as a makeshift identifier.15 This is an extremely brittle strategy. The same conceptual object fetched via two different paths (e.g., through a  
  viewer query versus a posts.author query) will be treated as two distinct entities. This results in the same data being stored multiple times in the cache, bloating the application's memory usage.  
* **Inconsistent UI State:** The most user-facing consequence of failed normalization is a stale and inconsistent UI. If two components on the screen display data from the same logical entity but the client cannot identify it as such, an update to one will not be reflected in the other. For example, if a user updates their name in a profile settings view, a separate header component displaying their name will remain unchanged, showing stale data until a full page refresh occurs.  
* **Failure of Automatic Updates:** A key feature of clients like Apollo is their ability to automatically update the cache after a mutation. When a mutation returns an updated object, the client uses its unique ID to find the corresponding entry in the normalized cache and seamlessly merge the new data. This action triggers a reactive update in all UI components that are subscribed to that object or its fields.18 Without a unique ID, this entire automated process fails. The developer is forced to manually intervene, either by refetching all queries that might contain the updated data or by writing complex, imperative code to manually update the cache, reintroducing the very boilerplate and sources of error that GraphQL clients are designed to eliminate.  
* **Data Corruption from Non-Unique IDs:** The problem is even more severe if an identifier is present but not unique. For example, if an email field is used as a key but multiple users can share the same email, the client cache will incorrectly merge the data from these distinct entities into a single record. This leads to unpredictable data corruption, where one user's data overwrites another's, resulting in non-deterministic behavior that is extremely difficult to debug.21

### **Section 3.3: Beyond Caching: Data Refetching and Federation**

The benefits of unique identifiers extend beyond client-side caching to other critical architectural patterns that are central to building scalable and maintainable applications with GraphQL.

* **Standardized Data Refetching:** The GOI specification's node query provides a universal, efficient mechanism for refetching the latest state of any object in the graph.12 In an application without this pattern, if a client needs to refresh a single, deeply nested object (e.g., the author of a comment on a post), it is often forced to re-execute the original, potentially large and complex, query that first retrieved it. This is highly inefficient, consuming unnecessary server resources and network bandwidth. The  
  node query allows for a precise, targeted refetch of only the data that is needed.  
* **Enabling Federated Architectures:** In a modern microservices architecture, a single data entity may be composed of fields that are owned and served by different backend services. Apollo Federation is a powerful architecture for composing a unified supergraph from these distributed services (subgraphs). The lynchpin of this architecture is the @key directive, which is used in a subgraph's schema to declare the fields that uniquely identify an entity (e.g., type User @key(fields: "id")).21 The federated gateway uses this key to resolve and merge data for a single entity from multiple subgraphs. Without a reliable unique key, the gateway would have no way to understand that a  
  User object from the accounts subgraph is the same entity as a User object from the reviews subgraph. Thus, unique object identification is not just a best practice but a non-negotiable prerequisite for a federated graph.

The presence of reliable unique identifiers enables a fundamental paradigm shift in client-side development. It allows the application to move from an *imperative* model of data fetching ("fetch this query, then in the callback, manually find these five places in the UI state and update them") to a *declarative* one ("this component declares that it needs this data fragment for User:123"). In the declarative model, the client library takes on the responsibility of orchestrating the fetching, caching, and updating of data. This dramatically reduces boilerplate code, eliminates entire classes of state management bugs, and vastly simplifies application logic. The absence of unique identifiers forces developers back into the imperative world, negating one of the most significant practical advantages of using a modern GraphQL client ecosystem.

## **Part IV: Strategies for Implementing Global Identifiers**

Given the critical importance of unique identifiers, architects and developers must employ robust strategies for their implementation. The choice of strategy depends on the context of the project—whether it is a new (greenfield) application or an existing (brownfield) one—and the nature of the underlying data sources.

### **Section 4.1: Opaque Identifiers as a Best Practice**

A foundational best practice is that global IDs exposed by the GraphQL server should be treated as opaque strings by the client.9 The client should not attempt to parse or infer any information from the structure of the ID. The only valid use of the ID on the client is to store it and pass it back to the server for refetching or as an argument in a mutation.

Base64 encoding is a common and effective technique to enforce this principle.9 Encoding the ID makes it clear to consumers that it is an opaque token, not a simple database integer or a human-readable string. This practice creates a clean separation of concerns and a durable API contract. By treating the ID as opaque, the server retains the freedom to change its internal ID generation strategy in the future—for example, migrating from integer IDs to UUIDs—without breaking any clients, as long as the opaque ID remains a valid token for refetching. This adheres to Hyrum's Law, which observes that any observable behavior of an API will be depended upon by someone; making IDs opaque reduces the observable surface area and preserves future flexibility.12

### **Section 4.2: Synthesizing Global IDs**

In many real-world scenarios, especially when building a GraphQL layer over existing databases, the underlying data sources use type-specific, non-unique identifiers, such as auto-incrementing integers. In this common case, the GraphQL server is responsible for synthesizing a globally unique ID from the local, type-specific ID.

The most common and robust pattern is to combine the object's type name with its local ID and then encode the resulting string.7 For example, a

User object with a database primary key of 123 could have its global ID synthesized as base64('User:123'), which produces a unique and opaque string like VXNlcjoxMjM=.

The server's node resolver must then be able to perform the reverse operation. When it receives a global ID, it must first decode it (e.g., from Base64), parse the type name and the internal ID from the resulting string, and then use that information to route the request to the correct data source or database table to fetch the object.5 This pattern is used by numerous production systems and frameworks. For instance, Shopify's API generates IDs with a specific URI format like

gid://shopify/Product/123 25, while PostGraphile often uses a Base64-encoded JSON array like

\["Post", 1\] to represent the type and primary key.8

### **Section 4.3: Alternative Generation Schemes**

While synthesizing IDs from type names and local keys is a flexible and common approach, other strategies exist, each with its own set of trade-offs.

* **Database-Native UUIDs:** For greenfield projects where the database schema can be designed from the ground up, using Universally Unique Identifiers (UUIDs) as primary keys is often the simplest and most direct approach. Since UUIDs are, by design, globally unique, they can be exposed directly as the id field in the GraphQL schema without any need for synthesis or encoding.14  
* **Dedicated ID Generation Services:** In very large-scale, distributed systems, it can be beneficial to use a dedicated service for generating unique, sortable IDs. A well-known example is Twitter's Snowflake, which generates 64-bit IDs that are composed of a timestamp, a machine ID, and a sequence number. This approach guarantees uniqueness across a distributed system and provides the added benefit of being roughly time-sortable.12  
* **Type-Prefixed IDs (Stripe-style):** Another effective strategy, popularized by APIs like Stripe's, is to store IDs in the database with a short, human-readable prefix that indicates the object's type (e.g., user\_..., prod\_...).12 These IDs are already globally unique and can be exposed directly. This approach offers improved debuggability, as the type of an object can be identified at a glance from its ID, while still being simple for the GraphQL layer to implement.

The following table provides a comparative analysis of these strategies to aid in architectural decision-making.

| Strategy | Implementation Complexity | Caching Support (Client) | Refetching Capability | Federation Compatibility | Pros | Cons |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Database UUID | Low | Excellent | Excellent | Excellent | Simple for greenfield; inherently unique. | Not always available in legacy systems; not time-sortable. |
| Synthesized Global ID (e.g., base64(Type:ID)) | Medium | Excellent | Excellent | Excellent | Works with any backend ID system; enforces opacity. | Requires logic on server to encode/decode; adds slight overhead. |
| Type-Prefixed ID (e.g., user\_123) | Low-Medium | Excellent | Excellent | Excellent | Human-readable for debugging; clear type association. | Requires buy-in at the database level; can be hard to retrofit. |
| Composite Key (Synthesized) | High | Good (with client config) | Good (with custom resolver) | Good (with complex @key) | Can model existing database schemas without modification. | Violates Node interface simplicity; requires client-side logic (keyFields). |
| No Unique ID | N/A | Poor (No Normalization) | Poor (Requires full query refetch) | Incompatible | Simple for transient or non-entity data. | Breaks caching, automatic updates, and federation; leads to bugs. |

## **Part V: Advanced Identification: Handling Composite Keys and Schemas Without IDs**

While the Global Object Identification pattern provides a clean model for objects with single primary keys, real-world data models are often more complex. This section addresses advanced scenarios, demonstrating how to apply the principles of object identification to systems with composite keys and how to thoughtfully design schemas that include types that genuinely lack a stable identity.

### **Section 5.1: Modeling Composite Keys in GraphQL**

It is common for database tables, particularly join tables in a relational model, to be uniquely identified by a composite primary key—a combination of two or more columns.19 The GraphQL type system, however, has no native concept of a composite key. The

Node interface, with its single id: ID\! field, requires a single, atomic identifier.

To reconcile this mismatch, the recommended server-side strategy is to create a synthetic single ID by combining the constituent parts of the composite key into a single, opaque string. This is an extension of the synthesis pattern described previously. For example, consider a UserGroupMembership type that represents a row in a join table identified by userId and groupId. The GraphQL server can create a stable, unique ID by concatenating these values with the type name and encoding the result: base64('UserGroupMembership:user\_123:group\_456').12

This approach allows the UserGroupMembership type to correctly implement the Node interface, preserving all the downstream benefits of standardized caching and refetching. The node resolver on the server would be responsible for decoding this ID, parsing the userId and groupId, and using them to query the correct row in the database.

### **Section 5.2: Client-Side Solutions for Complex Identification**

In some situations, a GraphQL server may expose types with composite identities but fail to provide a single, synthetic id field. While this is a sub-optimal schema design, modern client libraries provide powerful tools to handle this scenario gracefully on the client side, enabling normalization even without server-side adherence to the GOI specification.

Apollo Client's InMemoryCache allows developers to define a TypePolicy for any given type in the schema. Within this policy, the keyFields property can be used to instruct the cache on which fields to use to generate its internal, unique identifier.16 For an object with a composite key composed of

tokenId and key, the client could be configured as follows:

JavaScript

const cache \= new InMemoryCache({  
  typePolicies: {  
    InterviewParticipantInfo: {  
      keyFields: \["tokenId", "key"\],  
    },  
  },  
});

With this configuration, the cache will automatically generate a stable identifier for InterviewParticipantInfo objects by combining the values of their tokenId and key fields, resulting in a cache key like InterviewParticipantInfo:{"tokenId":"abc","key":"xyz"}.19 This

keyFields configuration is highly flexible and can even reference fields within nested objects to construct the identifier.31

This choice between synthesizing a global ID on the server versus using keyFields on the client represents a significant architectural trade-off. A server-provided global id establishes a single, canonical identifier for all consumers of the API. It creates a clean, universal contract, ensuring that any client—web, mobile, or otherwise—will handle object identity consistently. The server is the single source of truth for identity. In contrast, a client-configured keyFields policy provides immense flexibility and allows a client to work effectively even with non-ideal schemas. However, it couples the client's caching strategy directly to the field structure of the GraphQL type. If multiple, independently developed clients consume the same API, each client team must independently and correctly implement the same keyFields logic. This creates a risk of divergence and inconsistency. If the definition of uniqueness for a type changes in the backend, a server-side ID requires a change in only one place. A client-side keyFields approach requires a coordinated update across all clients. Therefore, server-authoritative global IDs promote a more robust, decoupled, and maintainable architecture in a multi-client ecosystem, while client-side keyFields are a powerful but more tightly coupled solution.

### **Section 5.3: Designing for "ID-less" Types**

Not every type in a GraphQL schema represents a true "entity" that requires a stable, unique identifier. Some types represent transient data, aggregated results, or value objects where the concept of a persistent identity is not applicable. In these cases, it is perfectly acceptable and correct to design the schema without an id field for those types.

Examples include:

* An AnalyticsReport type that represents a computed aggregation of data over a time range.  
* A PageInfo object within a paginated connection, which contains metadata about the current page of results.  
* Connection Edge types, which are containers for the Node and cursor and do not typically have their own identity.

It is crucial, however, to understand the consequences of this design choice for client-side caching. When a client cache encounters an object in a response that lacks a key (either because it has no id field or no keyFields policy), it cannot normalize that object. Instead, the object is embedded directly within its parent object in the cache store.16 This means the object cannot be accessed directly by an ID, and if it appears in multiple places in a response, it will be stored multiple times. Updates made to one instance will not be reflected in others. This behavior is perfectly acceptable for true value objects but would lead to the consistency problems described earlier if applied to entities. Modern clients allow for this behavior to be configured explicitly; for example, Apollo Client can be instructed not to normalize a specific type by setting

keyFields: false in its TypePolicy.

## **Part VI: Conclusion and Strategic Recommendations**

This report has systematically deconstructed the role of unique identifiers in GraphQL, moving from the formal specification to community conventions and their profound impact on the modern development ecosystem. The analysis reveals that while the core language is flexible, the practical demands of building high-performance, consistent, and scalable applications have led to a strong convergence on a set of best practices centered around object identification.

### **Section 6.1: Synthesis of Findings**

The key findings of this analysis can be synthesized into five main points:

1. **Unique IDs are a Convention, Not a Specification Requirement:** The base GraphQL specification does not mandate unique id fields. This flexibility is an intentional design feature that allows GraphQL to be a versatile layer over diverse data sources.  
2. **The GOI Spec is the Cornerstone of the Modern Ecosystem:** The convention of using a globally unique id field, as formalized in the Global Object Identification Specification, is the foundational pattern that enables the most powerful features of modern client libraries, including normalized caching, automatic UI updates, and standardized data refetching.  
3. **Deviating from Convention Has Severe Consequences:** Choosing to design a schema without a reliable unique identification strategy for entities leads to predictable and severe problems, including poor performance, inconsistent UI state, data corruption, and an inability to adopt advanced architectural patterns like federation.  
4. **Robust Strategies Exist for Complex Data Models:** Even when backend data models do not align perfectly with the single-ID pattern (e.g., they use composite keys), robust server-side strategies exist to synthesize globally unique IDs, allowing these models to conform to the GOI contract.  
5. **Client-Side Tooling Offers Powerful Workarounds:** Advanced client libraries provide flexible configurations (e.g., keyFields) that can create normalized caches even from schemas that do not provide a canonical id field. However, this approach introduces architectural trade-offs regarding client coupling and maintainability in multi-client environments.

### **Section 6.2: A Decision Framework for Schema Design**

To apply these findings, architects and API designers should use a structured decision framework when approaching object identification in their GraphQL schema. The following questions can guide this process:

1. **Project Context: Greenfield vs. Brownfield?** For a new, greenfield project, the Global Object Identification specification should be adopted from day one. There is little justification for not doing so. For a brownfield project layering GraphQL over existing systems, the primary task is to determine the best strategy (from Part IV) for synthesizing global IDs from the existing primary keys.12  
2. **Data Source Reality: What do your primary keys look like?** Analyze the primary keys in your underlying data sources. Do they use UUIDs, integers, or composite keys? This analysis will directly inform which implementation strategy—direct exposure, synthesis, or composite key concatenation—is most appropriate.  
3. **Client Ecosystem: How many distinct clients will consume this API?** If the API will be consumed by multiple, independently developed clients (e.g., a web app, an iOS app, and an Android app), a server-authoritative global id is strongly preferred. This establishes a single, unambiguous contract for identity and prevents the need to duplicate and synchronize complex identification logic across all clients.  
4. **Future Architecture: Is federation a possibility?** If a distributed graph composed via Apollo Federation is a likely future evolution for the system's architecture, then establishing a unique key for each entity (@key) is non-negotiable from the outset. Designing with this in mind will prevent a costly migration later.21  
5. **Entity vs. Value Object: Does every type need an ID?** Critically evaluate each type in the schema. Does it represent a true "entity" that has a stable identity and can be updated, or is it a transient "value object" that simply holds data? This distinction determines where it is not only acceptable but correct to omit an id field.

In conclusion, while not mandated by the formal GraphQL specification, a GraphQL schema that does not provide a mechanism for uniquely identifying its core entities is fundamentally incomplete from the perspective of modern application development. The Global Object Identification Specification should be considered the default, foundational best practice for any production-grade GraphQL API. The immense, compounding benefits to the client-side experience—including performance, data consistency, and developer productivity—far outweigh the implementation effort on the server. One should only deviate from this pattern with a clear, deliberate, and comprehensive understanding of the significant trade-offs and consequences.

#### **Works cited**

1. Schemas and Types \- GraphQL, accessed September 1, 2025, [https://graphql.org/learn/schema/](https://graphql.org/learn/schema/)  
2. GraphQL Specification, accessed September 1, 2025, [https://spec.graphql.org/draft/](https://spec.graphql.org/draft/)  
3. GraphQL Specification, accessed September 1, 2025, [https://spec.graphql.org/October2021/](https://spec.graphql.org/October2021/)  
4. Global Object Identification \- GraphQL, accessed September 1, 2025, [https://graphql.org/learn/global-object-identification/](https://graphql.org/learn/global-object-identification/)  
5. GraphQL: understanding node interface. \- DEV Community, accessed September 1, 2025, [https://dev.to/augustocalaca/graphql-understanding-node-interface-33e](https://dev.to/augustocalaca/graphql-understanding-node-interface-33e)  
6. GraphQL Global Object Identification Specification \- Relay, accessed September 1, 2025, [https://relay.dev/graphql/objectidentification.htm](https://relay.dev/graphql/objectidentification.htm)  
7. Relay \- Hot Chocolate v12 \- ChilliCream GraphQL Platform, accessed September 1, 2025, [https://chillicream.com/docs/hotchocolate/v12/defining-a-schema/relay/](https://chillicream.com/docs/hotchocolate/v12/defining-a-schema/relay/)  
8. Globally Unique Object Identification ("id" / "nodeId") \- PostGraphile, accessed September 1, 2025, [https://postgraphile.org/postgraphile/5/node-id](https://postgraphile.org/postgraphile/5/node-id)  
9. GraphQL Server Specification \- Relay, accessed September 1, 2025, [https://relay.dev/docs/guides/graphql-server-specification/](https://relay.dev/docs/guides/graphql-server-specification/)  
10. Relay | Strawberry GraphQL, accessed September 1, 2025, [https://strawberry.rocks/docs/guides/relay](https://strawberry.rocks/docs/guides/relay)  
11. GraphQL \+ TypeScript \- Interfaces | NestJS \- A progressive Node.js framework, accessed September 1, 2025, [https://docs.nestjs.com/graphql/interfaces](https://docs.nestjs.com/graphql/interfaces)  
12. How to implement Global Object Identification | Sophia Willows, accessed September 1, 2025, [https://sophiabits.com/blog/how-to-implement-global-object-identification](https://sophiabits.com/blog/how-to-implement-global-object-identification)  
13. Object Identification \- GraphQL Ruby, accessed September 1, 2025, [https://graphql-ruby.org/schema/object\_identification](https://graphql-ruby.org/schema/object_identification)  
14. Caching \- GraphQL, accessed September 1, 2025, [https://graphql.org/learn/caching/](https://graphql.org/learn/caching/)  
15. Implementing Unique IDs in GraphQL | by Jonathan Shapiro | Medium, accessed September 1, 2025, [https://pixelfabsuite.medium.com/implementing-unique-ids-in-graphql-a42b91d15568](https://pixelfabsuite.medium.com/implementing-unique-ids-in-graphql-a42b91d15568)  
16. Configuring the cache | Ferry Graphql, accessed September 1, 2025, [https://ferrygraphql.com/docs/cache-configuration/](https://ferrygraphql.com/docs/cache-configuration/)  
17. Normalized Caching | urql Documentation \- Nearform, accessed September 1, 2025, [https://nearform.com/open-source/urql/docs/graphcache/normalized-caching/](https://nearform.com/open-source/urql/docs/graphcache/normalized-caching/)  
18. How to do cache update and invalidation the right way? : r/graphql \- Reddit, accessed September 1, 2025, [https://www.reddit.com/r/graphql/comments/1bahh8u/how\_to\_do\_cache\_update\_and\_invalidation\_the\_right/](https://www.reddit.com/r/graphql/comments/1bahh8u/how_to_do_cache_update_and_invalidation_the_right/)  
19. Composite \*\*Primary Keys\*\* – HowTo? (Prisma is fine, SDL generator fails), accessed September 1, 2025, [https://community.redwoodjs.com/t/composite-primary-keys-howto-prisma-is-fine-sdl-generator-fails/5435](https://community.redwoodjs.com/t/composite-primary-keys-howto-prisma-is-fine-sdl-generator-fails/5435)  
20. Cache GraphQL query with multiple ids using Apollo \- Stack Overflow, accessed September 1, 2025, [https://stackoverflow.com/questions/51637618/cache-graphql-query-with-multiple-ids-using-apollo](https://stackoverflow.com/questions/51637618/cache-graphql-query-with-multiple-ids-using-apollo)  
21. Is it necessary to have @key to be unique ? what will happen if @key is defined and it returns multiple same result? \- Schema Design \- Apollo Community, accessed September 1, 2025, [https://community.apollographql.com/t/is-it-necessary-to-have-key-to-be-unique-what-will-happen-if-key-is-defined-and-it-returns-multiple-same-result/6823](https://community.apollographql.com/t/is-it-necessary-to-have-key-to-be-unique-what-will-happen-if-key-is-defined-and-it-returns-multiple-same-result/6823)  
22. Is graphql's ID type necessary if I've set an unique identifier with dataIdFromObject in Apollo Client \- Stack Overflow, accessed September 1, 2025, [https://stackoverflow.com/questions/50027346/is-graphqls-id-type-necessary-if-ive-set-an-unique-identifier-with-dataidfromo](https://stackoverflow.com/questions/50027346/is-graphqls-id-type-necessary-if-ive-set-an-unique-identifier-with-dataidfromo)  
23. Loosening object identification GraphQL spec · Issue \#1061 · facebook/relay \- GitHub, accessed September 1, 2025, [https://github.com/facebook/relay/issues/1061](https://github.com/facebook/relay/issues/1061)  
24. GraphQL Global Object Identification: Node ID Specification | by Riley Conrardy | Medium, accessed September 1, 2025, [https://medium.com/@conrardy/graphql-global-object-identification-node-id-specification-6ae4fb1bd316](https://medium.com/@conrardy/graphql-global-object-identification-node-id-specification-6ae4fb1bd316)  
25. Global IDs in Shopify APIs, accessed September 1, 2025, [https://shopify.dev/docs/api/usage/gids](https://shopify.dev/docs/api/usage/gids)  
26. Global IDs | Hasura DDN Docs, accessed September 1, 2025, [https://hasura.io/docs/3.0/graphql-api/global-ids/](https://hasura.io/docs/3.0/graphql-api/global-ids/)  
27. Composite primary key or not? \- database design \- Stack Overflow, accessed September 1, 2025, [https://stackoverflow.com/questions/4737190/composite-primary-key-or-not](https://stackoverflow.com/questions/4737190/composite-primary-key-or-not)  
28. Creating a composite primary key \- possible? \- SQL Server \- ServiceStack Customer Forums, accessed September 1, 2025, [https://forums.servicestack.net/t/creating-a-composite-primary-key-possible/12483](https://forums.servicestack.net/t/creating-a-composite-primary-key-possible/12483)  
29. Composite @id fields \- GraphQL \- Discuss Dgraph, accessed September 1, 2025, [https://discuss.dgraph.io/t/composite-id-fields/13337](https://discuss.dgraph.io/t/composite-id-fields/13337)  
30. Is it possible to modify a graphql input model to reference an object using a composite key instead of id? \- Stack Overflow, accessed September 1, 2025, [https://stackoverflow.com/questions/58030435/is-it-possible-to-modify-a-graphql-input-model-to-reference-an-object-using-a-co](https://stackoverflow.com/questions/58030435/is-it-possible-to-modify-a-graphql-input-model-to-reference-an-object-using-a-co)  
31. Configuring the Apollo Client cache \- Apollo GraphQL Docs, accessed September 1, 2025, [https://www.apollographql.com/docs/react/caching/cache-configuration](https://www.apollographql.com/docs/react/caching/cache-configuration)
warning
Model
ThinkingThoughts
(experimental)
Auto
Expand to view model thoughts

chevron_right
</file>

<file path="docs/design/on--running-multiple-docker-compose-stacks.md">
# Architecting Modular and Maintainable Environments with Multiple Docker Compose Stacks

## Section 1: A Primer on Docker Compose for the Modern Engineer

Before architecting complex, multi-stack environments, it is essential to establish a solid foundation in the core principles of Docker Compose. For an engineer new to the tool, understanding its fundamental purpose, its application model, and its basic lifecycle commands is the first step toward mastering more advanced patterns. Docker Compose is a powerful orchestrator for single-host environments, but its default behaviors are designed for encapsulation and isolation. Recognizing this is key to understanding how to deliberately and strategically break that isolation to build interconnected, modular systems.

### 1.1 Introduction to Docker Compose: Orchestration on a Single Host

Docker Compose is a tool designed to define and run multi-container Docker applications. Its primary purpose is to simplify the management of an entire application stack by using a single, declarative YAML configuration file, conventionally named `compose.yaml` or `docker-compose.yml`. This file allows an engineer to define all the components of an application—its services, the networks that connect them, and the volumes that persist their data—and manage their complete lifecycle with a concise set of commands. It is the key to unlocking a streamlined development, testing, and even production workflow on a single Docker host.

The fundamental workflow for using Docker Compose can be distilled into a straightforward three-step process:

1.  **Define the Application Environment:** For any custom components of the application, a `Dockerfile` is created. This file serves as a blueprint, specifying all the dependencies and instructions needed to build a reproducible Docker image for that component.
2.  **Define Services in `compose.yaml`:** In the project's root directory, a `compose.yaml` file is created. This file defines the various services that constitute the application, configuring how they run and interact with one another.
3.  **Run the Application:** With the configuration defined, a single command, `docker compose up`, is used to create, start, and connect all the services, networks, and volumes as a cohesive application stack.

To begin, the host system must have Docker Engine and the Docker Compose CLI plugin installed. Historically, Docker Compose was a separate binary (`docker-compose`), but since the release of Compose V2, it has been integrated directly into the Docker CLI and is invoked as `docker compose`. Modern installations of Docker Desktop for Windows and macOS, as well as standard Docker Engine installations on Linux, include the Compose V2 plugin by default.

### 1.2 The Compose Application Model: Services, Networks, and Volumes

The power of Docker Compose lies in its application model, which abstracts the complexities of container management into three core concepts: services, networks, and volumes.

#### Services

A **service** is an abstract definition of a single, containerized component within the application. It represents a logical piece of functionality, such as a web frontend, a backend API, a database, or a caching layer. Each service is defined by a Docker image (either a pre-existing one from a registry like Docker Hub or one built from a local `Dockerfile`) and a set of runtime configurations. When the application is launched, Docker Compose ensures that one or more containers are created and run according to each service's definition, with all containers for a given service being identical.

For example, a simple web application that tracks page hits might consist of two services: a `web` service running a Python Flask application and a `redis` service for the counter.

```yaml
# compose.yaml
services:
  web:
    build: .
    ports:
      - "8000:5000"
  redis:
    image: "redis:alpine"
```

In this configuration, the `web` service is built from a `Dockerfile` in the current directory, and port `5000` inside the container is mapped to port `8000` on the host machine. The `redis` service simply uses the public `redis:alpine` image from Docker Hub.

#### Networks

By default, when Docker Compose starts an application, it automatically creates a single **bridge network** for that application. Every service defined in the `compose.yaml` file is attached to this network. This is a critical feature, as it provides seamless service discovery and communication. Containers on this network can reach each other by using the service name as a DNS hostname.

Following the previous example, the Python code within the `web` container can connect to the `Redis` service simply by referencing the hostname `redis` on its default port, `6379`, without needing to know the container's internal IP address. This automatic networking simplifies application code and configuration, as the connection details are abstracted away by Compose. The default network is named based on the "project name," which typically defaults to the name of the directory containing the `compose.yaml` file, with a `_default` suffix (e.g., `myapp_default`). This default behavior creates a sandboxed, isolated environment for the application stack. This isolation is the primary architectural characteristic that must be intentionally and explicitly managed to enable the sharing of resources between different stacks, which is the central goal of this report. The challenge is not merely about splitting a configuration file but about strategically connecting these otherwise isolated environments.

#### Volumes

Containers are ephemeral by default; any data written to a container's writable layer is lost when the container is removed. To persist data, Docker provides **volumes**, which are the preferred mechanism for managing persistent data generated by and used by containers. A volume is a directory managed by Docker that is stored on the host machine and mounted into a container. Crucially, a volume's lifecycle is independent of any container's lifecycle; data in a volume persists even after the container using it is deleted.

Docker distinguishes between two primary types of mounts:

*   **Named Volumes:** These are fully managed by Docker via the Docker API. They are created and referenced by a user-defined name. This is the recommended approach for most use cases as it is more portable and decouples the data from the host's filesystem structure.
*   **Bind Mounts:** These map a specific file or directory path from the host machine directly into a container. While useful for development (e.g., mounting source code), they create a tight coupling to the host's directory structure, making the configuration less portable.

In a `compose.yaml` file, named volumes are typically declared at the top level and then attached to services. For instance, a PostgreSQL database service would use a named volume to persist its data files:

```yaml
# compose.yaml
services:
  db:
    image: postgres:13
    volumes:
      - db_data:/var/lib/postgresql/data

volumes:
  db_data:
```

Here, a named volume `db_data` is defined at the top level and then mounted into the `db` service at the path `/var/lib/postgresql/data`. The first time `docker compose up` is run, Docker will create this volume if it doesn't already exist.

### 1.3 Essential Lifecycle Commands

Docker Compose provides a simple command-line interface (CLI) for managing the entire lifecycle of the application stack defined in the `compose.yaml` file.

*   **Bringing the Stack Up:** The `docker compose up` command is the primary command for starting the application. It creates the services, networks, and volumes (if they don't already exist) and starts the containers. By default, it runs in the foreground and streams the logs from all services to the terminal. Using the `-d` or `--detach` flag will start the containers in the background and leave them running.
*   **Bringing the Stack Down:** The `docker compose down` command is the counterpart to `up`. It stops and removes all containers, as well as the network created for the application. A critical point to note is that this command, by default, **does not** remove named volumes. This is a safety feature to prevent accidental data loss. To remove the named volumes along with the containers and network, the `--volumes` or `-v` flag must be explicitly added.
*   **Monitoring and Inspection:** Once the stack is running, several commands are available for inspection. `docker compose ps` lists the containers for the services in the project, showing their current status and port mappings. `docker compose logs` fetches and displays the log output from the services. Appending the `-f` or `--follow` flag will stream the logs in real-time, which is invaluable for debugging.

## Section 2: The Pitfalls of the Monolith: Why a Single Compose File Fails at Scale

While a single `compose.yaml` file is perfect for simple applications, its utility diminishes rapidly as the complexity and scale of an application grow. The user's intuition to avoid a "really messy massive docker compose" is well-founded. Managing a large, multi-component system within a single monolithic configuration file introduces significant technical debt and operational friction, ultimately hindering maintainability, collaboration, and flexibility. The core issue is not merely the file's size but the fact that it imposes a single, unified lifecycle on components that are logically and operationally distinct.

### 2.1 The Maintainability Crisis

As more services are added to a single `compose.yaml` file, it inevitably becomes a source of complexity and confusion.

*   **Cognitive Overhead:** A file with over 20 services can easily span 500 lines or more, making it incredibly difficult for an engineer to read, navigate, and build a mental model of the system. Simple tasks, such as verifying that a host port is not already in use, become a tedious exercise of searching through a massive, unwieldy file. This high cognitive load slows down development and increases the likelihood of errors.
*   **Configuration Drift and Redundancy:** In a large file, it is common for configuration snippets—such as environment variables, volume mounts, logging configurations, or resource limits—to be copied and pasted across multiple service definitions. This violates the "Don't Repeat Yourself" (DRY) principle and leads to configuration drift, where an update is applied to one service but forgotten in another. This redundancy makes the configuration brittle and difficult to maintain. While advanced YAML features like anchors and aliases can mitigate some repetition, they can also introduce their own layer of indirection and complexity, making the file harder to understand for those unfamiliar with the syntax.

### 2.2 The Collaboration Bottleneck

In modern software development, especially within a microservices architecture, different teams often have ownership over different services. A monolithic `compose.yaml` file directly conflicts with this model of distributed ownership, creating a central bottleneck for development.

*   **Team Ownership and Merge Conflicts:** When all service definitions reside in one file, it becomes a single point of contention for multiple teams. If the API team needs to add an environment variable at the same time the data science team is adjusting resource limits for a processing service, they will both be modifying the same file. This leads to frequent merge conflicts in version control and requires significant cross-team coordination for even minor changes.
*   **Increased Blast Radius:** A monolithic configuration means that all components share the same fate. A small syntax error or a misconfiguration in one service definition can prevent the entire application stack from starting when `docker compose up` is executed. This large "blast radius" means that a mistake made by one team can block the work of all other teams. By separating stacks into logical, independently managed files, the blast radius is minimized. A failure in the "monitoring" stack will not prevent the "API" stack from starting, allowing teams to work with greater autonomy and safety.

### 2.3 Operational Inflexibility

Perhaps the most significant drawback of a monolithic `compose.yaml` is the operational inflexibility it imposes. By defining all services within a single "project," Docker Compose treats them as a single, indivisible unit for most lifecycle operations.

*   **Lifecycle Entanglement:** Different logical parts of an application have different operational needs and cadences for change. For example, one might need to restart the logging services without interrupting the core API and database. With a single file, this is difficult. Commands like `docker compose restart` or `docker compose up --force-recreate` apply to the entire project. While it is possible to target a single service (e.g., `docker compose up -d <service_name>`), this does not address the need to manage logical *groups* of services independently. Splitting the configuration into multiple files is the mechanism to break this shared fate and enable independent lifecycle management for different parts of the application.
*   **Scalability Limitations:** Docker Compose is fundamentally a client-side tool designed for orchestrating containers on a single host; it is not a cluster orchestrator like Kubernetes or Docker Swarm. A monolithic file exacerbates these inherent limitations. For instance, scaling a service that exposes a fixed host port is impossible, as multiple containers cannot bind to the same host port. Furthermore, attempting to scale one part of the application (e.g., adding more API workers) requires a redeployment of the entire configuration file, which is inefficient and unnecessarily disruptive to unrelated services.

## Section 3: The Modular Stack Architecture: A Strategic Approach

To overcome the limitations of a monolithic configuration, the recommended approach is to adopt a modular stack architecture. This involves decomposing the application into logical components, each defined by its own independent Docker Compose file. These individual "stacks" can then be composed together to form the complete environment, communicating through shared, externally managed resources. This strategy not only solves the maintainability issues but also aligns better with microservice principles and team-based ownership.

### 3.1 Defining a "Stack": The Role of the Project Name

In the context of Docker Compose, a "stack" is synonymous with a "project." A Compose project is an isolated environment consisting of the services, networks, and volumes defined in a given `compose.yaml` (or set of merged files). To prevent collisions between different stacks running on the same Docker host, Compose uses a **project name** to namespace all created resources. By default, the project name is derived from the name of the directory containing the `compose.yaml` file. For example, a service named `api` in a project named `my-app` will result in a container named `my-app-api-1`.

To effectively manage multiple, independent stacks, it is crucial to explicitly control the project name rather than relying on the directory name. This provides clarity and prevents unintended resource sharing or conflicts. There are several ways to set a project name, but the most direct and flexible method for managing multiple stacks from a command line or script is the `-p` (or `--project-name`) flag.

The order of precedence for determining the project name is as follows, from highest to lowest:

1.  The `-p` command-line flag.
2.  The `COMPOSE_PROJECT_NAME` environment variable.
3.  The top-level `name:` attribute within the `compose.yaml` file.
4.  The base name of the project directory (the default behavior).
5.  The base name of the current working directory if no configuration file is specified.

For example, the command `docker compose -p my-api-stack -f api/compose.yaml up -d` will launch the services defined in `api/compose.yaml` under the project name `my-api-stack`, creating containers like `my-api-stack-api-1` and a network named `my-api-stack_default`.

### 3.2 Strategies for Composing Multiple Files

Docker Compose provides three primary mechanisms for working with multiple configuration files: merging, extending, and including. The choice between them is a fundamental architectural decision that impacts the modularity, coupling, and maintainability of the overall system.

#### Merging with `-f`

The most direct way to combine files is by specifying multiple `-f` flags on the command line. Compose merges the files in the order they are provided, with configurations in later files overriding or extending those in earlier files. This approach is best suited for applying environment-specific overrides, such as having a base `compose.yaml` with common service definitions and a `compose.prod.yaml` that adjusts settings for production (e.g., removing bind mounts, adding restart policies).

The merging rules are well-defined: single-value keys (like `image` or `command`) are replaced by the value from the later file, while multi-value keys (like `ports` or `expose`) have their lists concatenated. However, this method has a critical limitation for building modular systems from components in different directories: **all relative paths** (for build contexts, `.env` files, or bind mounts) are resolved relative to the location of the *first* file specified in the command. This makes the merging strategy brittle and ill-suited for composing stacks from a monorepo structure.

#### Extending with `extends`

The `extends` keyword allows a service to inherit its configuration from another service, either in the same file or a different one. This creates an inheritance-like relationship, which can be useful for reducing duplication (DRY) by defining a common base service and extending it with specific overrides.

```yaml
# common-services.yml
services:
  base_app:
    image: my-app-base
    environment:
      - COMMON_VAR=true
```

```yaml
# compose.yaml
services:
  web:
    extends:
      file: common-services.yml
      service: base_app
    ports:
      - "8080:80"
```

While powerful, `extends` can create tightly coupled configurations that are difficult to trace and understand, especially with multiple levels of inheritance. Docker's own documentation suggests this approach can introduce complexity and is often less flexible than other methods.

#### Composition with `include` (Recommended Modern Approach)

Introduced in Docker Compose 2.20, the `include` directive is the superior, modern solution for building a single application model from multiple, independent Compose files. It is designed specifically for the use case of composing an application from components managed by different teams or located in different parts of a repository.

The primary advantage of `include` is that it **correctly resolves relative paths for each included file from that file's own directory** before merging the configurations into the main application model. This robust path handling makes it the ideal choice for monorepos. Furthermore, `include` promotes a safer composition model by raising an error if any included files define conflicting resources (e.g., two files defining a service with the same name), which prevents unexpected silent overrides and forces explicit configuration decisions.

This approach embodies a "composition over inheritance" design principle. Each included file is treated as a self-contained, black-box component, leading to more decoupled, resilient, and team-friendly configurations.

| Feature                        | Merging (`-f` flag)                  | `extends` Keyword                                   | `include` Directive                                          |
| :----------------------------- | :----------------------------------- | :-------------------------------------------------- | :----------------------------------------------------------- |
| **Primary Use Case**           | Environment-specific overrides (dev, prod, test). | Sharing common configuration snippets within or across files (DRY). | Composing a single application from multiple, independent component files. **Ideal for monorepos.** |
| **Coupling**                   | Low. Files are loosely coupled, but order matters. | High. Creates an inheritance-like dependency.         | Low. Files are treated as black-box components with explicit dependencies. |
| **Relative Path Handling**     | **Brittle.** All paths are relative to the *first* file in the command. | Converts paths to be relative to the *base* file. Can be confusing. | **Robust.** Paths are resolved relative to *each file's own location* before merging. |
| **Conflict Handling**          | Last file specified silently wins. Can lead to unexpected behavior. | Local overrides win. Can create complex inheritance chains. | **Explicit.** Throws an error if resources conflict, forcing clarity. |
| **Recommendation**             | Suitable for simple environment overrides. | Use with caution; `include` is often a better alternative. | **Recommended modern approach for modular stack architecture.** |

## Section 4: Implementing Shared Networking Across Stacks

With a modular architecture where each logical component is its own stack, the next challenge is to enable communication between them. By default, each Docker Compose project creates its own isolated network. To bridge these isolated stacks, the "external network" pattern is employed, which decouples the network's lifecycle from any single stack and creates a shared communication layer.

### 4.1 The "External Network" Pattern

The core concept of the external network pattern is to create a Docker network manually, outside the control of any specific `compose.yaml` file. Each stack that needs to communicate is then configured to connect to this pre-existing, shared network instead of creating its own default one. This makes the network an independent, first-class citizen of the environment, which multiple stacks can then join.

Architecturally, it is a best practice to create this shared network with a custom, explicit name (e.g., `shared_proxy_net`) using the Docker CLI. An alternative, but less robust, approach is to use the default network of one stack (e.g., `stack-a_default`) and have other stacks connect to it. The former approach is superior because it creates a clean, decoupled contract; all stacks depend on a well-known, independent resource. The latter creates an implicit and brittle dependency, where `stack-b` is coupled to the implementation details (the project name) of `stack-a`. If `stack-a`'s directory is ever renamed, the configuration for `stack-b` will break.

### 4.2 Step-by-Step Implementation

The following steps detail how to create a shared network and configure two separate stacks—a reverse proxy and a backend API—to communicate over it.

#### Step 1: Create the Shared Network

This is a one-time setup action for the environment, performed using the Docker CLI.

*   **Command:** Create a standard bridge network with a descriptive name.

    ```bash
    docker network create my_shared_network
    ```

    This command creates a persistent bridge network that will not be removed when any individual stack is brought down.
*   **Verification:** Confirm that the network was created successfully.

    ```bash
    docker network ls
    ```

    The output should list `my_shared_network` among the available networks. For more detailed information, such as the subnet or connected containers, use the inspect command:

    ```bash
    docker network inspect my_shared_network
    ```

#### Step 2: Configure Stack A (Reverse Proxy) to Use the External Network

Modify the `compose.yaml` file for the first stack (e.g., `proxy/compose.yaml`) to connect to the newly created network.

*   **YAML Configuration:**

    ```yaml
    # proxy/compose.yaml
    services:
      proxy:
        image: nginx:alpine
        ports:
          - "80:80"
        networks:
          - shared_net # Connects this service to the network defined below

    networks:
      shared_net:
        name: my_shared_network
        external: true
    ```

*   **Annotation:**
    *   `services.proxy.networks`: This list attaches the `proxy` service to the network referenced as `shared_net`. A service can be attached to multiple networks if needed.
    *   `networks`: This top-level key is where networks used within the Compose file are defined.
    *   `shared_net`: This is the logical name for the network *within this Compose file*.
    *   `name: my_shared_network`: This is the crucial mapping. It tells Compose that the internal name `shared_net` refers to the actual Docker network named `my_shared_network`.
    *   `external: true`: This is the key directive that instructs Docker Compose *not* to create this network, but to use the existing one. If the network `my_shared_network` does not exist when `docker compose up` is run, Compose will return an error.

#### Step 3: Configure Stack B (Backend API) to Use the Same External Network

Repeat the same network configuration for the second stack's `compose.yaml` file (e.g., `api/compose.yaml`). The `networks` block will be identical.

*   **YAML Configuration:**

    ```yaml
    # api/compose.yaml
    services:
      api:
        build: .
        # No ports exposed to the host; communication is via the shared network
        networks:
          - shared_net

    networks:
      shared_net:
        name: my_shared_network
        external: true
    ```

#### Step 4: Launch and Test Communication

Launch both stacks independently, using distinct project names to ensure their resources are properly namespaced.

*   **Launch Commands:**

    ```bash
    docker compose -p proxy-stack -f proxy/compose.yaml up -d
    docker compose -p api-stack -f api/compose.yaml up -d
    ```

*   **Verification:** Once both stacks are running, services can communicate across stacks using their service names as hostnames, because they are on the same network which provides DNS resolution. To test this, one can execute a command inside the proxy container to reach the API service.
    1.  Find the container name for the proxy: `docker ps`
    2.  Execute a shell inside the proxy container: `docker exec -it <proxy_container_name> sh`
    3.  From inside the container, ping the API service using its service name as defined in `api/compose.yaml`:

        ```bash
        # Inside the proxy container
        ping api
        ```

    A successful ping confirms that the containers are on the same network and can resolve each other's names, validating the shared networking setup. The reverse proxy can now be configured to forward requests to `http://api:<port>`.

## Section 5: Implementing Shared Persistent Storage Across Stacks

Similar to networking, enabling data sharing between services in different stacks requires breaking the default volume isolation. The "external volume" pattern provides a robust and portable way to achieve this. It involves creating a Docker-managed named volume independently and then configuring multiple stacks to mount it, allowing them to share a common persistent data store.

### 5.1 The "External Volume" Pattern

The concept is analogous to the external network pattern. A Docker named volume is created using the Docker CLI, giving it a lifecycle that is completely decoupled from any individual Compose project. Services from different stacks can then be configured to mount this pre-existing, "external" volume, enabling them to read and write to the same data directory.

This pattern is ideal for numerous use cases in a microservices architecture, including:

*   A database container in a data stack writing to a volume that is also mounted by a backup stack for periodic backups.
*   An API service in an API stack writing user-uploaded files to a volume that a CDN or frontend stack reads from to serve the content.
*   Multiple services sharing a common set of configuration files or assets stored in a single volume.

A critical architectural decision is the choice between using an external **named volume** versus a shared **bind mount** (a specific host path). While a bind mount might seem simpler for local development (e.g., mounting `/data/shared` into multiple containers), it tightly couples the entire setup to a specific host filesystem structure. This severely hinders portability and reproducibility, as the configuration will fail on any machine—be it a colleague's laptop or a CI/CD runner—that does not have the exact same directory path. External named volumes, which are managed by the Docker daemon itself, abstract away the physical storage location, making them the superior choice for creating portable, consistent, and environment-agnostic multi-stack applications.

### 5.2 Step-by-Step Implementation

The following steps demonstrate how to create a shared named volume and configure two stacks—a data-producing API and a data-consuming backup service—to use it.

#### Step 1: Create the Shared Volume

This is a one-time setup action for the environment, performed using the Docker CLI.

*   **Command:** Create a Docker-managed named volume.

    ```bash
    docker volume create my_shared_data
    ```

    This command instructs the Docker daemon to create a new volume. The actual data will be stored in a directory within Docker's internal storage area (e.g., `/var/lib/docker/volumes/` on Linux), but this location should be considered an implementation detail and not be interacted with directly.
*   **Verification:** Confirm the volume's existence.

    ```bash
    docker volume ls
    ```

    The output will list `my_shared_data`. To see more details, including its mount point on the host, use the inspect command:

    ```bash
    docker volume inspect my_shared_data
    ```

#### Step 2: Configure Stack A (API Service) to Use the External Volume

Modify the `compose.yaml` for the service that will write data to the shared volume (e.g., `api/compose.yaml`).

*   **YAML Configuration:**

    ```yaml
    # api/compose.yaml
    services:
      api_service:
        build: .
        volumes:
          - shared_data:/app/uploads # Mounts the shared volume into the container

    volumes:
      shared_data:
        name: my_shared_data
        external: true
    ```

*   **Annotation:**
    *   `services.api_service.volumes`: This section mounts the volume into the container. The syntax `- shared_data:/app/uploads` maps the logical volume `shared_data` (defined below) to the `/app/uploads` path inside the container.
    *   `volumes`: This top-level key defines the named volumes used by this Compose file.
    *   `shared_data`: This is the logical name for the volume *within this Compose file*.
    *   `name: my_shared_data`: This maps the internal name `shared_data` to the actual Docker volume named `my_shared_data`.
    *   `external: true`: This critical directive tells Compose that the volume is pre-existing and should not be created. If the volume `my_shared_data` does not exist, Compose will raise an error.

#### Step 3: Configure Stack B (Backup Service) to Use the Same External Volume

Modify the `compose.yaml` for another service that needs to access the same data, such as a backup utility.

*   **YAML Configuration:**

    ```yaml
    # backup/compose.yaml
    services:
      backup_service:
        image: backup-service-image
        volumes:
          - shared_data:/data/to_backup # Mounts the same shared volume

    volumes:
      shared_data:
        name: my_shared_data
        external: true
    ```

    Note that the path inside the container (`/data/to_backup`) can be different from the path used in the API service's container. Both mount points will refer to the same underlying data on the host.

#### Step 4: Launch and Test Data Sharing

Launch both stacks using their respective project names.

*   **Launch Commands:**

    ```bash
    docker compose -p api-stack -f api/compose.yaml up -d
    docker compose -p backup-stack -f backup/compose.yaml up -d
    ```

*   **Verification:** To confirm that the volume is shared, create a file from one container and verify its existence from the other.
    1.  Find the container name for the API service: `docker ps`
    2.  Execute a shell inside the API container: `docker exec -it <api_container_name> sh`
    3.  Inside the API container, create a test file in the mounted directory:

        ```bash
        # Inside the API container
        echo "test data" > /app/uploads/test.txt
        exit
        ```

    4.  Find the container name for the backup service: `docker ps`
    5.  Execute a shell inside the backup container: `docker exec -it <backup_container_name> sh`
    6.  Inside the backup container, check for the existence and content of the file:

        ```bash
        # Inside the backup container
        cat /data/to_backup/test.txt
        ```

    The command should output "test data," confirming that both containers are successfully sharing the same persistent volume.

## Section 6: Managing the Multi-Stack Lifecycle and Best Practices

Architecting a modular system with multiple Docker Compose stacks is only the first step. Effective day-to-day management requires moving beyond the basic `docker compose up` command and adopting practices that embrace the modularity of the environment. This involves targeted CLI operations, handling inter-stack dependencies, robust configuration management, and leveraging wrapper scripts to simplify complex workflows. The operational patterns established for a multi-stack environment are as crucial as the initial configuration.

### 6.1 Targeted Stack Operations with the CLI

The primary benefit of a modular architecture is the ability to manage the lifecycle of each stack independently. This is achieved by consistently using the `-p` (project name) and `-f` (file) flags with standard Docker Compose commands.

*   **Viewing Logs for a Specific Stack:** To tail the logs for only the API stack without being inundated by output from the proxy or data stacks:

    ```bash
    docker compose -p api-stack -f api/compose.yaml logs --follow
    ```

*   **Restarting a Single Stack:** If a configuration change is made to the reverse proxy, only that stack needs to be restarted:

    ```bash
    docker compose -p proxy-stack -f proxy/compose.yaml restart
    ```

*   **Rebuilding and Recreating a Service in a Stack:** To apply code changes that require a new image build for the API service:

    ```bash
    docker compose -p api-stack -f api/compose.yaml up -d --build
    ```

These targeted commands reinforce the core principle of independent lifecycle management, minimizing disruption and allowing teams to operate on their components with autonomy.

### 6.2 Advanced: Managing Inter-Stack Dependencies

A significant limitation of the multi-stack architecture is that Docker Compose's `depends_on` feature **does not work across different projects**. A service in the `api` stack cannot use `depends_on` to wait for a database service in the `data` stack to become healthy. This requires more sophisticated strategies for managing startup order and service readiness.

*   **Solution 1: Application-Level Resilience (Production Best Practice):** The most robust solution is to build resilience directly into the application code. Services should be designed to handle the temporary unavailability of their dependencies. This typically involves implementing a retry loop with exponential backoff when establishing connections to other services like databases or message queues. This approach makes the system resilient to transient failures and restarts, which is essential for production environments.
*   **Solution 2: Scripted Startup Orchestration (Development/CI Solution):** For local development and CI environments where a predictable startup order is desired, a wrapper script can be used to orchestrate the launch sequence. This script explicitly starts stacks in the correct order and can wait for services to become healthy before proceeding.

    ```bash
    #!/bin/bash
    set -e # Exit immediately if a command exits with a non-zero status.

    echo "--- Starting Data Stack ---"
    docker compose -p data-stack -f data/compose.yaml up -d

    echo "--- Waiting for Database to be healthy ---"
    # This loop checks the health status of the database container
    # It requires a HEALTHCHECK to be defined in the data/compose.yaml
    while ! docker compose -p data-stack -f data/compose.yaml ps db | grep -q "healthy"; do
        echo "Database is unhealthy - sleeping"
        sleep 5;
    done
    echo "Database is healthy!"

    echo "--- Starting API and Infra Stacks ---"
    docker compose -p api-stack -f api/compose.yaml up -d
    docker compose -p proxy-stack -f infra/compose.yaml up -d

    echo "--- All stacks are up and running ---"
    ```

    This script enforces the dependency that the data stack must be healthy before the API stack is launched, solving the cross-stack dependency problem at the operational level.

### 6.3 Configuration Management with `.env` Files

To maintain clean and portable `compose.yaml` files, it is a critical best practice to externalize configuration values into `.env` files. This separates environment-specific settings (like ports, image tags, user IDs, or credentials) from the structural definition of the services.

*   **Security:** `.env` files should always be added to the project's `.gitignore` file to prevent sensitive information such as API keys or passwords from being committed to version control. While `.env` files are suitable for development, for production environments, using Docker Secrets is the recommended and more secure method for handling sensitive data.
*   **Shared Configuration:** A common `.env` file at the root of the project can be used to define variables that are shared across multiple stacks, ensuring consistency.

    ```ini
    #.env
    COMPOSE_DOCKER_CLI_BUILD=1
    DOCKER_BUILDKIT=1

    # Shared Resource Names
    SHARED_NETWORK_NAME=my_shared_network
    SHARED_VOLUME_NAME=my_shared_data

    # Image Tags
    API_IMAGE_TAG=latest
    ```

    The `compose.yaml` files can then reference these variables using interpolation:

    ```yaml
    # api/compose.yaml
    networks:
      shared_net:
        name: ${SHARED_NETWORK_NAME}
        external: true
    ```

### 6.4 Simplifying Operations with Wrapper Scripts (Makefile)

The CLI commands required to manage a multi-stack environment can become long, repetitive, and prone to error. A Makefile serves as an excellent tool to create simple, memorable aliases for these complex operations, providing a standardized and self-documenting interface for the entire team.

A well-crafted Makefile is more than a convenience; it is a crucial tool for encoding the operational logic and contracts (like startup order and shared resource management) that Docker Compose itself cannot express across project boundaries. It transforms a series of complex, imperative steps into a simple, declarative command (`make up`), which is the ultimate deliverable for an engineer looking for a robust and easy-to-use system.

*   **Example Makefile:**

    ```Makefile
    # Makefile for managing the multi-stack environment

    .PHONY: help setup up down clean logs-api
    .DEFAULT_GOAL := help

    # Load environment variables from .env file
    include .env
    export

    # Define project names and file paths
    PROXY_PROJECT := proxy-stack
    API_PROJECT   := api-stack
    DATA_PROJECT  := data-stack

    PROXY_COMPOSE_FILE := infra/compose.yaml
    API_COMPOSE_FILE   := api/compose.yaml
    DATA_COMPOSE_FILE  := data/compose.yaml

    help:
    	@echo "Usage: make [target]"
    	@echo "Targets:"
    	@echo "  setup      Create shared network and volume"
    	@echo "  up         Bring up all application stacks in the correct order"
    	@echo "  down       Bring down all application stacks"
    	@echo "  clean      Bring down stacks and remove shared resources"
    	@echo "  logs-api   Tail logs for the API stack"

    setup:
    	@echo "--- Creating shared resources ---"
    	@docker network create $(SHARED_NETWORK_NAME) >/dev/null 2>&1 || true
    	@docker volume create $(SHARED_VOLUME_NAME) >/dev/null 2>&1 || true

    up: setup
    	@echo "--- Bringing up all stacks ---"
    	docker compose -p $(DATA_PROJECT) -f $(DATA_COMPOSE_FILE) up -d
    	docker compose -p $(API_PROJECT) -f $(API_COMPOSE_FILE) up -d
    	docker compose -p $(PROXY_PROJECT) -f $(PROXY_COMPOSE_FILE) up -d
    	@echo "--- All stacks are running ---"

    down:
    	@echo "--- Bringing down all stacks ---"
    	docker compose -p $(PROXY_PROJECT) -f $(PROXY_COMPOSE_FILE) down --remove-orphans
    	docker compose -p $(API_PROJECT) -f $(API_COMPOSE_FILE) down --remove-orphans
    	docker compose -p $(DATA_PROJECT) -f $(DATA_COMPOSE_FILE) down --remove-orphans

    clean: down
    	@echo "--- Removing shared resources ---"
    	@docker network rm $(SHARED_NETWORK_NAME) >/dev/null 2>&1 || true
    	@docker volume rm $(SHARED_VOLUME_NAME) >/dev/null 2>&1 || true
    	@echo "--- Cleanup complete ---"

    logs-api:
    	docker compose -p $(API_PROJECT) -f $(API_COMPOSE_FILE) logs --follow
    ```

*   **Benefits:** This Makefile provides a self-documenting interface (`make help`), ensures that shared resources are created before stacks are started (`up` depends on `setup`), and offers simple commands for complex, multi-step operations, ensuring consistency for all team members.

## Section 7: A Complete Reference Implementation

This final section synthesizes all the preceding concepts into a single, concrete, and practical example. It serves as a blueprint that an engineer can adapt for their own projects, demonstrating a complete multi-stack web application with shared networking and storage, all managed by a central Makefile.

### 7.1 Scenario: A Multi-Stack Web Application

The reference application is composed of three logically distinct stacks:

*   **Component 1: The `infra` Stack:** This stack contains a Caddy reverse proxy. Its role is to be the single entry point for HTTP traffic and route requests to the appropriate backend service.
*   **Component 2: The `api` Stack:** This stack contains a backend API service built with Node.js. It handles business logic and requires a database connection.
*   **Component 3: The `data` Stack:** This stack contains a PostgreSQL database service. Its sole responsibility is data persistence.

These stacks will be interconnected using the following shared resources:

*   **Shared Network (`proxy_net`):** A network that allows the `infra` stack (Caddy) to communicate with the `api` stack (Node.js). The database will be isolated from this network for security.
*   **Shared Volume (`postgres_data`):** A named volume to ensure the data for the PostgreSQL database is persisted across container restarts and stack redeployments.

### 7.2 Directory Structure

A clear and logical directory structure is essential for managing a multi-stack project. Each stack resides in its own directory, containing its `compose.yaml` and any other necessary files like `Dockerfiles` or application source code.

```
/my-app/
├── Makefile
├── .env
├── api/
│   ├── compose.yaml
│   ├── Dockerfile
│   └── src/
│       └── index.js
├── data/
│   └── compose.yaml
└── infra/
    ├── compose.yaml
    └── Caddyfile
```

### 7.3 Annotated Configuration Files

Here are the complete, annotated contents for each configuration file in the project.

#### Shared Environment Configuration (`.env`)

This file, located at the project root, defines variables used across multiple stacks, ensuring consistency.

```ini
#.env
# Shared Resource Names
SHARED_PROXY_NETWORK=proxy_net
POSTGRES_DATA_VOLUME=postgres_data

# Data Stack Configuration
POSTGRES_DB=appdb
POSTGRES_USER=appuser
POSTGRES_PASSWORD=supersecretpassword
```

#### Data Stack (`data/compose.yaml`)

This stack manages only the PostgreSQL database, persisting its data in the shared external volume.

```yaml
# data/compose.yaml
services:
  db:
    image: postgres:14-alpine
    restart: always
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    # No ports exposed to the host; access is via a shared network with the API
    networks:
      - default # The API will connect to this stack's default network

volumes:
  postgres_data:
    name: ${POSTGRES_DATA_VOLUME}
    external: true
```

#### API Stack (`api/compose.yaml`)

This stack defines the Node.js API. It connects to two networks: its own default network to communicate with the database, and the shared proxy network to receive traffic from the reverse proxy.

```yaml
# api/compose.yaml
services:
  api:
    build: .
    restart: always
    environment:
      - DATABASE_URL=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
    depends_on:
      - db # This only works because db is in the same project
    networks:
      - data_default # Connect to the data stack's network
      - proxy_net    # Connect to the shared proxy network

networks:
  data_default:
    name: data-stack_default # Explicitly name the data stack's default network
    external: true
  proxy_net:
    name: ${SHARED_PROXY_NETWORK}
    external: true
```

*Note: This example shows connecting to another stack's default network. While possible, creating a dedicated `api_db_net` is often a cleaner pattern.*

#### Infrastructure Stack (`infra/compose.yaml`)

This stack manages the Caddy reverse proxy, which listens on port 80 and forwards traffic to the `api` service over the shared network.

```yaml
# infra/compose.yaml
services:
  proxy:
    image: caddy:2-alpine
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
    networks:
      - proxy_net

networks:
  proxy_net:
    name: ${SHARED_PROXY_NETWORK}
    external: true
```

#### Caddy Configuration (`infra/Caddyfile`)

```nginx
# infra/Caddyfile
{
    auto_https off
}

:80 {
    reverse_proxy api:3000
}
```

#### Central Makefile

This Makefile at the project root orchestrates the entire lifecycle of the multi-stack application.

```Makefile
# /my-app/Makefile

.PHONY: help setup up down clean logs-api
.DEFAULT_GOAL := help

# Load environment variables from .env file
include .env
export

# Define project names and file paths
INFRA_PROJECT := infra-stack
API_PROJECT   := api-stack
DATA_PROJECT  := data-stack

INFRA_COMPOSE_FILE := infra/compose.yaml
API_COMPOSE_FILE   := api/compose.yaml
DATA_COMPOSE_FILE  := data/compose.yaml

help:
	@echo "Usage: make [target]"
	@echo "Targets:"
	@echo "  setup      Create shared network and volume"
	@echo "  up         Bring up all application stacks in order"
	@echo "  down       Bring down all application stacks"
	@echo "  clean      Bring down stacks and remove shared resources"
	@echo "  logs-api   Tail logs for the API stack"

setup:
	@echo "--- Creating shared resources ---"
	@docker network create $(SHARED_PROXY_NETWORK) >/dev/null 2>&1 || true
	@docker volume create $(POSTGRES_DATA_VOLUME) >/dev/null 2>&1 || true

up: setup
	@echo "--- Bringing up Data Stack ---"
	docker compose -p $(DATA_PROJECT) -f $(DATA_COMPOSE_FILE) up -d
	@echo "--- Bringing up API Stack ---"
	docker compose -p $(API_PROJECT) -f $(API_COMPOSE_FILE) up -d
	@echo "--- Bringing up Infrastructure Stack ---"
	docker compose -p $(INFRA_PROJECT) -f $(INFRA_COMPOSE_FILE) up -d
	@echo "--- All stacks are running ---"

down:
	@echo "--- Bringing down all stacks ---"
	docker compose -p $(INFRA_PROJECT) -f $(INFRA_COMPOSE_FILE) down --remove-orphans
	docker compose -p $(API_PROJECT) -f $(API_COMPOSE_FILE) down --remove-orphans
	docker compose -p $(DATA_PROJECT) -f $(DATA_COMPOSE_FILE) down --remove-orphans

clean: down
	@echo "--- Removing shared resources ---"
	@docker network rm $(SHARED_PROXY_NETWORK) >/dev/null 2>&1 || true
	@docker volume rm $(POSTGRES_DATA_VOLUME) >/dev/null 2>&1 || true
	@echo "--- Cleanup complete ---"

logs-api:
	docker compose -p $(API_PROJECT) -f $(API_COMPOSE_FILE) logs --follow
```

### 7.4 Walkthrough: From Zero to Running

With the project structured and the Makefile in place, an engineer can get the entire environment running with a few simple commands.

1.  **Clone the Repository:**

    ```bash
    git clone <repository_url> my-app
    cd my-app
    ```

2.  **Bring Up the Entire Application:** The `make up` command will automatically run the `setup` target first to create the shared network and volume, and then bring up all three stacks in the correct order.

    ```bash
    make up
    ```

3.  **Verify and Monitor:** Check the logs for a specific stack to ensure it started correctly.

    ```bash
    make logs-api
    ```

4.  **Tear Down and Clean Up:** The `make clean` command provides a complete teardown, stopping and removing all containers and networks from all stacks, and finally removing the shared network and volume.

    ```bash
    make clean
    ```

This reference implementation provides a robust, maintainable, and scalable pattern for managing complex applications with Docker Compose, successfully avoiding the pitfalls of a monolithic configuration while enabling controlled sharing of essential resources.

#### Works cited

1.  Docker Compose - Docker Docs, https://docs.docker.com/compose/
2.  Introduction to Docker Compose | Baeldung on Ops, https://www.baeldung.com/ops/docker-compose
3.  How Compose works - Docker Docs, https://docs.docker.com/compose/intro/compose-application-model/
4.  The docker-compose.yml file | Divio Documentation, https://docs.divio.com/reference/docker-docker-compose/
5.  Getting started with Docker-compose, a quick tutorial - Geshan's Blog, https://geshan.com.np/blog/2024/04/docker-compose-tutorial/
6.  docker/compose: Define and run multi-container applications with Docker - GitHub, https://github.com/docker/compose
7.  Docker Compose Quickstart - Docker Docs, https://docs.docker.com/compose/gettingstarted/
8.  Docker Compose - What is It, Example & Tutorial - Spacelift, https://spacelift.io/blog/docker-compose
9.  Networking in Compose - Docker Docs, https://docs.docker.com/compose/how-tos/networking/
10. Migrate to Compose v2 - Docker Docs, https://docs.docker.com/compose/releases/migrate/
11. Services | Docker Docs, https://docs.docker.com/reference/compose-file/services/
12. Networks in Docker Compose - Medium, https://medium.com/@triwicaksono.com/networks-in-docker-compose-0943abe3de54
13. Communicating between different docker services in docker-compose - Stack Overflow, https://stackoverflow.com/questions/47648792/communicating-between-different-docker-services-in-docker-compose
14. Docker Compose: Features, Benefits, and Usage Guide - The Knowledge Academy, https://www.theknowledgeacademy.com/blog/docker-compose/
15. Volumes | Docker Docs, https://docs.docker.com/engine/storage/volumes/
16. Use Volumes to Manage Persistent Data With Docker Compose - Kinsta, https://kinsta.com/blog/docker-compose-volumes/
17. How to share data between host and containers using volumes in ..., https://stackoverflow.com/questions/40005409/how-to-share-data-between-host-and-containers-using-volumes-in-docker-compose
18. Define and manage volumes in Docker Compose - Docker Docs, https://docs.docker.com/reference/compose-file/volumes/
19. Need help with sharing volume between containers, docker-compose - Reddit, https://www.reddit.com/r/docker/comments/j6dd3f/need_help_with_sharing_volume_between_containers/
20. Shared volumes in Docker Compose 3 · GitHub, https://gist.github.com/jesugmz/bfe4c447ef7558614805f1f85a2ed867
21. Docker Compose Tutorial - Codecademy, https://www.codecademy.com/article/mastering-docker-compose
22. What does Flag do in Docker Compose? - GeeksforGeeks, https://www.geeksforgeeks.org/devops/flag-do-in-docker-compose/
23. docker-compose file has become too long - Stack Overflow, https://stackoverflow.com/questions/52727542/docker-compose-file-has-become-too-long
24. Docker Compose: Splitting one big yml file, carefully, and what about these extra thoughts, https://www.reddit.com/r/selfhosted/comments/1gyo1lk/docker_compose_splitting_one_big_yml_yml_file/
25. Mastering Docker Compose: Eliminate Redundancy with Anchors, Aliases, and Extensions, https://medium.com/codex/mastering-docker-compose-eliminate-redundancy-with-anchors-aliases-and-extensions-6be9bfbc1209
26. Docker Compose - Share named volume between multiple containers - Stack Overflow, https://stackoverflow.com/questions/44284484/docker-compose-share-named-volume-between-multiple-containers
27. Use multiple Compose files | Docker Docs, https://docs.docker.com/compose/how-tos/multiple-compose-files/
28. Improve Docker Compose Modularity with `include`, https://www.docker.com/blog/improve-docker-compose-modularity-with-include/
29. Should I make a huge docker-compose.yml or multiple ones ? : r/selfhosted - Reddit, https://www.reddit.com/r/selfhosted/comments/qm37vq/should_i_make_a_huge_dockercomposeyml_or_multiple/
30. One large compose file? : r/docker - Reddit, https://www.reddit.com/r/docker/comments/1i32i3z/one_large_compose_file/
31. Orchestrate Containers for Development with Docker Compose - CloudBees, https://www.cloudbees.com/blog/orchestrate-containers-for-development-with-docker-compose
32. Limitations of Compose | Docker Course Labs, https://docker.courselabs.co/labs/compose-limits/
33. Docker compose 'scale' command is not scaling across multiple machines - Stack Overflow, https://stackoverflow.com/questions/35376185/docker-compose-scale-command-is-not-scaling-across-multiple-machines
34. Scaling in Docker Compose with hands-on Examples, https://docker77.hashnode.dev/scaling-in-docker-compose-with-hands-on-examples
35. Specify a project name | Docker Docs, https://docs.docker.com/compose/how-tos/project-name/
36. Configure pre-defined environment variables in Docker Compose, https://docs.docker.com/compose/how-tos/environment-variables/envvars/
37. docker compose | Docker Docs, https://docs.docker.com/reference/cli/docker/compose/
38. Merge | Docker Docs, https://docs.docker.com/compose/how-tos/multiple-compose-files/merge/
39. 10 Best Practices for Writing Maintainable Docker Compose Files ..., https://dev.to/wallacefreitas/10-best-practices-for-writing-maintainable-docker-compose-files-4ca2
40. How to split docker-compose for local and production work? | by ..., https://medium.com/@yasen.ivanov89/how-to-split-docker-compose-for-local-and-production-work-b22e310096bd
41. Extend | Docker Docs, https://docs.docker.com/compose/how-tos/multiple-compose-files/extends/
42. How to Link Multiple Docker Compose Files | by mehdi hosseini - Medium, https://medium.com/@mehdi_hosseini/how-to-link-multiple-docker-compose-files-7250f10063a9
43. Include | Docker Docs, https://docs.docker.com/compose/how-tos/multiple-compose-files/include/
44. Communication between multiple docker-compose projects - Stack ..., https://stackoverflow.com/questions/38088279/communication-between-multiple-docker-compose-projects
45. Communication between multiple docker-compose projects, https://dev.to/iamrj846/communication-between-multiple-docker-compose-projects-223k
46. How to share network (or volume) config between multiple compose ..., https://www.reddit.com/r/docker/comments/1evb8ee/how_to_share_network_or_volume_config_between/
47. Docker Networking - Basics, Network Types & Examples - Spacelift, https://spacelift.io/blog/docker-networking
48. Networking | Docker Docs, https://docs.docker.com/engine/network/
49. How to create and manage Docker networks - Educative.io, https://www.educative.io/answers/how-to-create-and-manage-docker-networks
50. Networks - Docker Docs, https://docs.docker.com/reference/compose-file/networks/
51. How To Create And Use Networks In Docker Compose - Warp, https://www.warp.dev/terminus/docker-compose-networks
52. Share Volume Between Multiple Containers in Docker Compose | Baeldung on Ops, https://www.baeldung.com/ops/docker-share-volume-multiple-containers
53. How do I mount a host directory as a volume in docker compose - Stack Overflow, https://stackoverflow.com/questions/40905761/how-do-i-mount-a-host-directory-as-a-volume-in-docker-compose
54. Shared volume across multiple docker-compose projects [duplicate] - Stack Overflow, https://stackoverflow.com/questions/66042906/shared-volume-across-multiple-docker-compose-projects
55. Docker Volumes - Guide with Examples - Spacelift, https://spacelift.io/blog/docker-volumes
56. Docker Volumes: How to Create & Get Started - phoenixNAP, https://phoenixnap.com/kb/docker-volumes
57. docker volume create - Docker Docs, https://docs.docker.com/reference/cli/docker/volume/create/
58. How to run docker container with external volumes - Codebeamer, https://codebeamer.com/cb/wiki/5713519
59. Support for starting stacks in order; or, cross-stack dependencies; or, merged stacks; or, include another stack · portainer · Discussion #12752 · GitHub, https://github.com/orgs/portainer/discussions/12752
60. Defining your multi-container application with docker-compose.yml - .NET | Microsoft Learn, https://learn.microsoft.com/en-us/dotnet/architecture/microservices/multi-container-microservice-net-applications/multi-container-applications-docker-compose
61. Docker Compose - How to execute multiple commands? - Stack Overflow, https://stackoverflow.com/questions/30063907/docker-compose-how-to-execute-multiple-commands
62. Set environment variables | Docker Docs, https://docs.docker.com/compose/how-tos/environment-variables/set-environment-variables/
63. Handling Environment Variables in Docker Compose for Secure and Flexible Configurations, https://medium.com/@sh.hamzarauf/handling-environment-variables-in-docker-compose-for-secure-and-flexible-configurations-5ce6a5bb0412
64. Best practices | Docker Docs, https://docs.docker.com/compose/how-tos/environment-variables/best-practices/
65. Started dockermake (create Makefiles for docker/docker-compose) : r/opensource - Reddit, https://www.reddit.com/r/opensource/comments/1auw5fb/started_dockermake_create_makefiles_for/
66. Simplifying docker-compose operations using Makefile | by Khushbu ..., https://medium.com/freestoneinfotech/simplifying-docker-compose-operations-using-makefile-26d451456d63
67. Makefiles and Docker for Local Development | Cody Hiar, https://www.codyhiar.com/blog/makefiles-and-docker-for-local-development/
68. 4 Ways on How To Use Makefile - Jerry Ng, https://jerrynsh.com/4-levels-of-how-to-use-makefile/
</file>

<file path="docs/guac-demo-compose.yaml">
version: "3.8"

services:

  graphql:
    image: ghcr.io/guacsec/guac:v1.0.0
    command: "/opt/guac/guacgql --gql-debug"
    working_dir: /guac
    restart: on-failure
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:8080"]
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 5s

  collectsub:
    image: ghcr.io/guacsec/guac:v1.0.0
    command: "/opt/guac/guaccsub"
    working_dir: /guac
    restart: on-failure
    ports:
      - "2782:2782"
    healthcheck:
      test: [ "CMD", "wget", "--spider", "http://localhost:2782" ]
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 5s

  depsdev-collector:
    image: ghcr.io/guacsec/guac:v1.0.0
    command: "/opt/guac/guacone collect deps_dev -p --csub-addr=collectsub:2782 --gql-addr=http://graphql:8080/query"
    working_dir: /guac
    restart: on-failure
    environment:
      - DEPS_DEV_APIKEY
    depends_on:
      collectsub:
        condition: service_healthy
      graphql:
        condition: service_healthy

  cd-certifier:
    image: ghcr.io/guacsec/guac:v1.0.0
    command: "/opt/guac/guacone certifier cd -p --csub-addr=collectsub:2782 --gql-addr=http://graphql:8080/query"
    working_dir: /guac
    restart: on-failure
    depends_on:
      collectsub:
        condition: service_healthy
      graphql:
        condition: service_healthy

  osv-certifier:
    image: ghcr.io/guacsec/guac:v1.0.0
    command: "/opt/guac/guacone certifier osv -p --csub-addr=collectsub:2782 --gql-addr=http://graphql:8080/query"
    working_dir: /guac
    restart: on-failure
    depends_on:
      collectsub:
        condition: service_healthy
      graphql:
        condition: service_healthy

  guac-rest:
    image: ghcr.io/guacsec/guac:v1.0.0
    command: "/opt/guac/guacrest --rest-api-server-port=8081 --gql-addr=http://graphql:8080/query"
    working_dir: /guac
    restart: on-failure
    ports:
      - "8081:8081"
    depends_on:
      graphql:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "-O-",
          "http://localhost:8081/healthz"
        ]
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 5s
</file>

<file path="guac-mesh-graphql/scripts/extractor.ts">
// Sanity Check (standalone script)
// This script first does something silly, then runs useful infrastructure diagnostics.
// It does NOT invoke or depend on the real extractor logic. It is safe to run in any environment.

// --- Silly/goofy section ---
console.log('🦄 Welcome to the Graphtastic Silly Smooth Sanity Summary!');
console.log('Today, before we do anything useful, let us summon the power of the unicorn...');
const unicorn = [
  '           \\',
  '            \\',
  '             \\',
  '              >\\/7',
  '          _.-(6 6)-._',
  '         (=  Y  =)',
  '          /`-^--`\\',
  '         /     |\\\\',
  '        (  )-(  )\\\\',
  '         ""   ""',
];
unicorn.forEach(line => console.log(line));
console.log('✨ The unicorn has blessed your build. Proceeding with diagnostics...\n');

// --- Useful diagnostics section ---
const execSync = require('child_process').execSync;
function check(msg: string, fn: () => boolean) {
  try {
    if (fn()) {
      console.log(`✅ ${msg}`);
    } else {
      console.log(`❌ ${msg}`);
    }
  } catch (e) {
    console.log(`❌ ${msg} (error: ${e})`);
  }
}

check('Test harness is running', () => true);
check('Docker is available', () => {
  const output = execSync('docker --version').toString();
  return /Docker/.test(output);
});
check('Makefile is present', () => {
  const fs = require('fs');
  return fs.existsSync('../Makefile');
});
check('docker-compose.yml is present', () => {
  const fs = require('fs');
  return fs.existsSync('../docker-compose.yml');
});
try {
  execSync('make check-dockerfiles', { stdio: 'inherit', cwd: '..' });
  console.log('✅ All referenced Dockerfiles exist');
} catch (e) {
  console.log('❌ Some referenced Dockerfiles are missing');
}
try {
  execSync('make up', { stdio: 'inherit', cwd: '..' });
  console.log('✅ make up target runs (services start)');
} catch (e) {
  console.log('❌ make up target failed');
}
try {
  execSync('make down', { stdio: 'inherit', cwd: '..' });
  console.log('✅ make down target runs (services stop)');
} catch (e) {
  console.log('❌ make down target failed');
}
</file>

<file path="guac-mesh-graphql/scripts/extractor.wip.ts">
// Extractor: GUAC Mesh → RDF N-Quads for Dgraph
// Implements the ETL extraction phase per PLAN.md and design--guac-to-dgraph.md
// ...existing code from previous extractor.ts...

// (The full code is preserved here for future implementation.)

// --- (see previous extractor.ts for full implementation) ---
</file>

<file path="guac-mesh-graphql/.gitignore">
# TypeScript build artifacts
/dist
*.tsbuildinfo
</file>

<file path="guac-mesh-graphql/package.json">
{
  "name": "guac-mesh-graphql",
  "version": "1.0.0",
  "description": "Mesh transformation sidecar and extractor for GUAC Spoke",
  "main": "scripts/extractor.ts",
  "type": "module",
  "scripts": {
    "build": "tsc",
    "start:mesh": "ts-node-esm config/mesh.config.mts",
    "start:extractor": "ts-node-esm scripts/extractor.ts",
    "extract": "ts-node-esm scripts/extractor.ts"
  },
  "dependencies": {
    "graphql": "^16.11.0",
    "node-fetch": "^3.3.2"
  },
  "devDependencies": {
    "typescript": "^5.0.0",
    "ts-node": "^10.9.1",
    "@types/node": "^24.3.2",
    "@types/node-fetch": "^2.6.13"
  }
}
</file>

<file path="guac-mesh-graphql/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "esModuleInterop": true,
    "forceConsistentCasingInFileNames": true,
    "strict": true,
    "skipLibCheck": true,
    "outDir": "dist",
    "rootDir": ".",
    "resolveJsonModule": true,
    "allowSyntheticDefaultImports": true,
    "types": ["node"]
  },
  "include": [
    "scripts/**/*.ts",
    "config/**/*.ts"
  ],
  "exclude": [
    "node_modules",
    "dist"
  ]
}
</file>

<file path=".gitignore">
# Mac files
.DS_Store

# Node
node_modules/
dist/
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Build artifacts
build/

# Dgraph persistent data
dgraph-stack/
sboms/
build/
dgraph-stack/dgraph/
dgraph-stack/config/

# Environment
.env


AGENTS.md
GEMINI.md
PLAN.md
</file>

<file path="CONTRIBUTING.md">
# Contributor Attribution

We value every contribution and believe in giving credit where it's due. As part of your first pull request, please add your name or GitHub username to the [`CONTRIBUTORS.md`](./CONTRIBUTORS.md) file.
</file>

<file path="CONTRIBUTORS.md">
# Our Contributors

Graphtastic is an open-source project made possible by a community of contributors. We thank each and every one of them.

### All Contributors

A list of all contributors to the Graphtastic organization can be found here:
[https://github.com/orgs/graphtastic/people](https://github.com/orgs/graphtastic/people)

Individuals are encouraged to add their names here when making their first contribution.

* Matt Young ([@halcyondude](https://github.com/halcyondude))
</file>

<file path="docker-compose.yml">
# This file assembles all the layers of our Spoke.
name: subgraph-dgraph-software-supply-chain

include:
  - ./compose/dgraph.yml
  - ./compose/guac.yml
  - ./compose/mesh.yml
</file>

<file path="LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="LICENSE.code">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="LICENSE.docs">
Attribution-ShareAlike 4.0 International

=======================================================================

Creative Commons Corporation ("Creative Commons") is not a law firm and
does not provide legal services or legal advice. Distribution of
Creative Commons public licenses does not create a lawyer-client or
other relationship. Creative Commons makes its licenses and related
information available on an "as-is" basis. Creative Commons gives no
warranties regarding its licenses, any material licensed under their
terms and conditions, or any related information. Creative Commons
disclaims all liability for damages resulting from their use to the
fullest extent possible.

Using Creative Commons Public Licenses

Creative Commons public licenses provide a standard set of terms and
conditions that creators and other rights holders may use to share
original works of authorship and other material subject to copyright
and certain other rights specified in the public license below. The
following considerations are for informational purposes only, are not
exhaustive, and do not form part of our licenses.

     Considerations for licensors: Our public licenses are
     intended for use by those authorized to give the public
     permission to use material in ways otherwise restricted by
     copyright and certain other rights. Our licenses are
     irrevocable. Licensors should read and understand the terms
     and conditions of the license they choose before applying it.
     Licensors should also secure all rights necessary before
     applying our licenses so that the public can reuse the
     material as expected. Licensors should clearly mark any
     material not subject to the license. This includes other CC-
     licensed material, or material used under an exception or
     limitation to copyright. More considerations for licensors:
    wiki.creativecommons.org/Considerations_for_licensors

     Considerations for the public: By using one of our public
     licenses, a licensor grants the public permission to use the
     licensed material under specified terms and conditions. If
     the licensor's permission is not necessary for any reason--for
     example, because of any applicable exception or limitation to
     copyright--then that use is not regulated by the license. Our
     licenses grant only permissions under copyright and certain
     other rights that a licensor has authority to grant. Use of
     the licensed material may still be restricted for other
     reasons, including because others have copyright or other
     rights in the material. A licensor may make special requests,
     such as asking that all changes be marked or described.
     Although not required by our licenses, you are encouraged to
     respect those requests where reasonable. More considerations
     for the public:
    wiki.creativecommons.org/Considerations_for_licensees

=======================================================================

Creative Commons Attribution-ShareAlike 4.0 International Public
License

By exercising the Licensed Rights (defined below), You accept and agree
to be bound by the terms and conditions of this Creative Commons
Attribution-ShareAlike 4.0 International Public License ("Public
License"). To the extent this Public License may be interpreted as a
contract, You are granted the Licensed Rights in consideration of Your
acceptance of these terms and conditions, and the Licensor grants You
such rights in consideration of benefits the Licensor receives from
making the Licensed Material available under these terms and
conditions.


Section 1 -- Definitions.

  a. Adapted Material means material subject to Copyright and Similar
     Rights that is derived from or based upon the Licensed Material
     and in which the Licensed Material is translated, altered,
     arranged, transformed, or otherwise modified in a manner requiring
     permission under the Copyright and Similar Rights held by the
     Licensor. For purposes of this Public License, where the Licensed
     Material is a musical work, performance, or sound recording,
     Adapted Material is always produced where the Licensed Material is
     synched in timed relation with a moving image.

  b. Adapter's License means the license You apply to Your Copyright
     and Similar Rights in Your contributions to Adapted Material in
     accordance with the terms and conditions of this Public License.

  c. BY-SA Compatible License means a license listed at
     creativecommons.org/compatiblelicenses, approved by Creative
     Commons as essentially the equivalent of this Public License.

  d. Copyright and Similar Rights means copyright and/or similar rights
     closely related to copyright including, without limitation,
     performance, broadcast, sound recording, and Sui Generis Database
     Rights, without regard to how the rights are labeled or
     categorized. For purposes of this Public License, the rights
     specified in Section 2(b)(1)-(2) are not Copyright and Similar
     Rights.

  e. Effective Technological Measures means those measures that, in the
     absence of proper authority, may not be circumvented under laws
     fulfilling obligations under Article 11 of the WIPO Copyright
     Treaty adopted on December 20, 1996, and/or similar international
     agreements.

  f. Exceptions and Limitations means fair use, fair dealing, and/or
     any other exception or limitation to Copyright and Similar Rights
     that applies to Your use of the Licensed Material.

  g. License Elements means the license attributes listed in the name
     of a Creative Commons Public License. The License Elements of this
     Public License are Attribution and ShareAlike.

  h. Licensed Material means the artistic or literary work, database,
     or other material to which the Licensor applied this Public
     License.

  i. Licensed Rights means the rights granted to You subject to the
     terms and conditions of this Public License, which are limited to
     all Copyright and Similar Rights that apply to Your use of the
     Licensed Material and that the Licensor has authority to license.

  j. Licensor means the individual(s) or entity(ies) granting rights
     under this Public License.

  k. Share means to provide material to the public by any means or
     process that requires permission under the Licensed Rights, such
     as reproduction, public display, public performance, distribution,
     dissemination, communication, or importation, and to make material
     available to the public including in ways that members of the
     public may access the material from a place and at a time
     individually chosen by them.

  l. Sui Generis Database Rights means rights other than copyright
     resulting from Directive 96/9/EC of the European Parliament and of
     the Council of 11 March 1996 on the legal protection of databases,
     as amended and/or succeeded, as well as other essentially
     equivalent rights anywhere in the world.

  m. You means the individual or entity exercising the Licensed Rights
     under this Public License. Your has a corresponding meaning.


Section 2 -- Scope.

  a. License grant.

       1. Subject to the terms and conditions of this Public License,
          the Licensor hereby grants You a worldwide, royalty-free,
          non-sublicensable, non-exclusive, irrevocable license to
          exercise the Licensed Rights in the Licensed Material to:

            a. reproduce and Share the Licensed Material, in whole or
               in part; and

            b. produce, reproduce, and Share Adapted Material.

       2. Exceptions and Limitations. For the avoidance of doubt, where
          Exceptions and Limitations apply to Your use, this Public
          License does not apply, and You do not need to comply with
          its terms and conditions.

       3. Term. The term of this Public License is specified in Section
          6(a).

       4. Media and formats; technical modifications allowed. The
          Licensor authorizes You to exercise the Licensed Rights in
          all media and formats whether now known or hereafter created,
          and to make technical modifications necessary to do so. The
          Licensor waives and/or agrees not to assert any right or
          authority to forbid You from making technical modifications
          necessary to exercise the Licensed Rights, including
          technical modifications necessary to circumvent Effective
          Technological Measures. For purposes of this Public License,
          simply making modifications authorized by this Section 2(a)
          (4) never produces Adapted Material.

       5. Downstream recipients.

            a. Offer from the Licensor -- Licensed Material. Every
               recipient of the Licensed Material automatically
               receives an offer from the Licensor to exercise the
               Licensed Rights under the terms and conditions of this
               Public License.

            b. Additional offer from the Licensor -- Adapted Material.
               Every recipient of Adapted Material from You
               automatically receives an offer from the Licensor to
               exercise the Licensed Rights in the Adapted Material
               under the conditions of the Adapter's License You apply.

            c. No downstream restrictions. You may not offer or impose
               any additional or different terms or conditions on, or
               apply any Effective Technological Measures to, the
               Licensed Material if doing so restricts exercise of the
               Licensed Rights by any recipient of the Licensed
               Material.

       6. No endorsement. Nothing in this Public License constitutes or
          may be construed as permission to assert or imply that You
          are, or that Your use of the Licensed Material is, connected
          with, or sponsored, endorsed, or granted official status by,
          the Licensor or others designated to receive attribution as
          provided in Section 3(a)(1)(A)(i).

  b. Other rights.

       1. Moral rights, such as the right of integrity, are not
          licensed under this Public License, nor are publicity,
          privacy, and/or other similar personality rights; however, to
          the extent possible, the Licensor waives and/or agrees not to
          assert any such rights held by the Licensor to the limited
          extent necessary to allow You to exercise the Licensed
          Rights, but not otherwise.

       2. Patent and trademark rights are not licensed under this
          Public License.

       3. To the extent possible, the Licensor waives any right to
          collect royalties from You for the exercise of the Licensed
          Rights, whether directly or through a collecting society
          under any voluntary or waivable statutory or compulsory
          licensing scheme. In all other cases the Licensor expressly
          reserves any right to collect such royalties.


Section 3 -- License Conditions.

Your exercise of the Licensed Rights is expressly made subject to the
following conditions.

  a. Attribution.

       1. If You Share the Licensed Material (including in modified
          form), You must:

            a. retain the following if it is supplied by the Licensor
               with the Licensed Material:

                 i. identification of the creator(s) of the Licensed
                    Material and any others designated to receive
                    attribution, in any reasonable manner requested by
                    the Licensor (including by pseudonym if
                    designated);

                ii. a copyright notice;

               iii. a notice that refers to this Public License;

                iv. a notice that refers to the disclaimer of
                    warranties;

                 v. a URI or hyperlink to the Licensed Material to the
                    extent reasonably practicable;

            b. indicate if You modified the Licensed Material and
               retain an indication of any previous modifications; and

            c. indicate the Licensed Material is licensed under this
               Public License, and include the text of, or the URI or
               hyperlink to, this Public License.

       2. You may satisfy the conditions in Section 3(a)(1) in any
          reasonable manner based on the medium, means, and context in
          which You Share the Licensed Material. For example, it may be
          reasonable to satisfy the conditions by providing a URI or
          hyperlink to a resource that includes the required
          information.

       3. If requested by the Licensor, You must remove any of the
          information required by Section 3(a)(1)(A) to the extent
          reasonably practicable.

  b. ShareAlike.

     In addition to the conditions in Section 3(a), if You Share
     Adapted Material You produce, the following conditions also apply.

       1. The Adapter's License You apply must be a Creative Commons
          license with the same License Elements, this version or
          later, or a BY-SA Compatible License.

       2. You must include the text of, or the URI or hyperlink to, the
          Adapter's License You apply. You may satisfy this condition
          in any reasonable manner based on the medium, means, and
          context in which You Share Adapted Material.

       3. You may not offer or impose any additional or different terms
          or conditions on, or apply any Effective Technological
          Measures to, Adapted Material that restrict exercise of the
          rights granted under the Adapter's License You apply.


Section 4 -- Sui Generis Database Rights.

Where the Licensed Rights include Sui Generis Database Rights that
apply to Your use of the Licensed Material:

  a. for the avoidance of doubt, Section 2(a)(1) grants You the right
     to extract, reuse, reproduce, and Share all or a substantial
     portion of the contents of the database;

  b. if You include all or a substantial portion of the database
     contents in a database in which You have Sui Generis Database
     Rights, then the database in which You have Sui Generis Database
     Rights (but not its individual contents) is Adapted Material,
     including for purposes of Section 3(b); and

  c. You must comply with the conditions in Section 3(a) if You Share
     all or a substantial portion of the contents of the database.

For the avoidance of doubt, this Section 4 supplements and does not
replace Your obligations under this Public License where the Licensed
Rights include other Copyright and Similar Rights.


Section 5 -- Disclaimer of Warranties and Limitation of Liability.

  a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE
     EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS
     AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF
     ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,
     IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,
     WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR
     PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,
     ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT
     KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT
     ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.

  b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE
     TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,
     NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,
     INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,
     COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR
     USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN
     ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR
     DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR
     IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.

  c. The disclaimer of warranties and limitation of liability provided
     above shall be interpreted in a manner that, to the extent
     possible, most closely approximates an absolute disclaimer and
     waiver of all liability.


Section 6 -- Term and Termination.

  a. This Public License applies for the term of the Copyright and
     Similar Rights licensed here. However, if You fail to comply with
     this Public License, then Your rights under this Public License
     terminate automatically.

  b. Where Your right to use the Licensed Material has terminated under
     Section 6(a), it reinstates:

       1. automatically as of the date the violation is cured, provided
          it is cured within 30 days of Your discovery of the
          violation; or

       2. upon express reinstatement by the Licensor.

     For the avoidance of doubt, this Section 6(b) does not affect any
     right the Licensor may have to seek remedies for Your violations
     of this Public License.

  c. For the avoidance of doubt, the Licensor may also offer the
     Licensed Material under separate terms or conditions or stop
     distributing the Licensed Material at any time; however, doing so
     will not terminate this Public License.

  d. Sections 1, 5, 6, 7, and 8 survive termination of this Public
     License.


Section 7 -- Other Terms and Conditions.

  a. The Licensor shall not be bound by any additional or different
     terms or conditions communicated by You unless expressly agreed.

  b. Any arrangements, understandings, or agreements regarding the
     Licensed Material not stated herein are separate from and
     independent of the terms and conditions of this Public License.


Section 8 -- Interpretation.

  a. For the avoidance of doubt, this Public License does not, and
     shall not be interpreted to, reduce, limit, restrict, or impose
     conditions on any use of the Licensed Material that could lawfully
     be made without permission under this Public License.

  b. To the extent possible, if any provision of this Public License is
     deemed unenforceable, it shall be automatically reformed to the
     minimum extent necessary to make it enforceable. If the provision
     cannot be reformed, it shall be severed from this Public License
     without affecting the enforceability of the remaining terms and
     conditions.

  c. No term or condition of this Public License will be waived and no
     failure to comply consented to unless expressly agreed to by the
     Licensor.

  d. Nothing in this Public License constitutes or may be interpreted
     as a limitation upon, or waiver of, any privileges and immunities
     that apply to the Licensor or You, including from the legal
     processes of any jurisdiction or authority.


=======================================================================

Creative Commons is not a party to its public
licenses. Notwithstanding, Creative Commons may elect to apply one of
its public licenses to material it publishes and in those instances
will be considered the “Licensor.” The text of the Creative Commons
public licenses is dedicated to the public domain under the CC0 Public
Domain Dedication. Except for the limited purpose of indicating that
material is shared under a Creative Commons public license or as
otherwise permitted by the Creative Commons policies published at
creativecommons.org/policies, Creative Commons does not authorize the
use of the trademark "Creative Commons" or any other trademark or logo
of Creative Commons without its prior written consent including,
without limitation, in connection with any unauthorized modifications
to any of its public licenses or any other arrangements,
understandings, or agreements concerning use of licensed material. For
the avoidance of doubt, this paragraph does not form part of the
public licenses.

Creative Commons may be contacted at creativecommons.org.
</file>

<file path="NOTICE">
Copyright 2025 The Graphtastic Authors
</file>

<file path="compose/dgraph.yml">
# compose/dgraph.yml

services:
  dgraph-zero:
    image: dgraph/dgraph:latest
    container_name: dgraph-zero
    # Storage: bind mount only for now (see README for TODO re: volumes)
    volumes:
      - ./dgraph-stack/dgraph/zero:/dgraph
    ports:
      - "5081:5080"
      - "6081:6080"
    restart: on-failure
    command: dgraph zero --my=dgraph-zero:5080
    networks:
      - dgraph-net

  dgraph-alpha:
    image: dgraph/dgraph:latest
    container_name: dgraph-alpha
    # Storage: bind mount only for now (see README for TODO re: volumes)
    volumes:
      - ./dgraph-stack/dgraph/alpha:/dgraph
    ports:
      - "8081:8080"
      - "9081:9080"
    restart: on-failure
    command: dgraph alpha --my=dgraph-alpha:7080 --zero=dgraph-zero:5080 --security "whitelist=${DGRAPH_ALPHA_WHITELIST}"
    depends_on:
      - dgraph-zero
    networks:
      - dgraph-net
      - graphtastic_net

  dgraph-ratel:
    image: dgraph/ratel:latest
    container_name: dgraph-ratel
    ports:
      - "8001:8000"
    restart: on-failure
    networks:
      - dgraph-net
      - graphtastic_net

networks:
  dgraph-net:
    driver: bridge
  graphtastic_net:
    external: true

# TODO: Add support for Docker named volumes for Dgraph data
# volumes:
#   dgraph_data_zero:
#     external: false
#   dgraph_data_alpha:
#     external: false
</file>

<file path="compose/guac.yml">
# compose/guac.yml
#
# This file is a synthesized configuration based on the project's original intent
# (Postgres backend, private networking) and the updated service architecture
# introduced in GUAC v1.0.0, as seen in the guac-demo-compose.yaml reference.
#
services:
  guac-postgres:
    image: postgres:13
    container_name: guac-postgres
    networks:
      - guac_internal_net
    environment:
      - POSTGRES_DB=guac
      - POSTGRES_USER=guac
      - POSTGRES_PASSWORD=guac
    volumes:
      - ./dgraph-stack/guac-data:/var/lib/postgresql/data
    restart: on-failure
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U guac -d guac"]
      interval: 10s
      timeout: 5s
      retries: 5

  # NATS is kept for potential use by collectors, though the core graphql
  # service no longer uses it directly with the 'ent' backend.
  guac-nats:
    image: nats:2.9
    container_name: guac-nats
    networks:
      - guac_internal_net
    restart: on-failure

  # The collectsub service gathers data from various collectors.
  guac-collectsub:
    image: ghcr.io/guacsec/guac:v1.0.0
    container_name: guac-collectsub
    command: "/opt/guac/guaccsub"
    networks:
      - guac_internal_net
    restart: on-failure
    depends_on:
      - guac-nats

  # The graphql service, configured to use the 'ent' backend with Postgres.
  guac-graphql:
    image: ghcr.io/guacsec/guac:v1.0.0
    container_name: guac-graphql
    command: >
      /opt/guac/guacgql
      --gql-backend ent
      --db-driver postgres
      --db-address "postgres://guac:guac@guac-postgres:5432/guac?sslmode=disable"
      --gql-debug
    networks:
      - guac_internal_net
    restart: on-failure
    depends_on:
      guac-postgres:
        condition: service_healthy
    volumes:
      - ../sboms:/sboms

networks:
  guac_internal_net:
    driver: bridge
</file>

<file path="compose/tools.yml">
services:
  extractor:
    build: ./guac-mesh-graphql
    image: guac-mesh-extractor:latest
    command: ["node", "scripts/extractor.js"]
    volumes:
      - ../guac-mesh-graphql/build:/app/build
      - ../guac-mesh-graphql/scripts:/app/scripts
    environment:
      - MESH_ENDPOINT=http://guac-mesh-graphql:4000/graphql
    networks:
      - default
      - guac_internal_net
networks:
  guac_internal_net:
    external: true
</file>

<file path="guac-mesh-graphql/Dockerfile">
# syntax=docker/dockerfile:1
FROM node:20-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install --production
COPY . .
CMD ["node", "dist/scripts/extractor.js"]
</file>

<file path="guac-mesh-graphql/README.md">
# guac-mesh-graphql

## Overview

This directory contains the **GraphQL Mesh transformation sidecar** and related tooling for the Graphtastic GUAC Spoke. It is designed for:

- **Federation-aware GraphQL transformation** (via Mesh)
- **ETL extraction** from GUAC to Dgraph (via N-Quads)
- **Self-contained, modern TypeScript project structure**
- **Containerized and local development workflows**

## Key Patterns & Rationale

### 1. Localized TypeScript Project Structure

- All TypeScript config (`tsconfig.json`), build artifacts (`dist/`), and scripts are scoped to this subproject.
- This prevents config/build pollution in the monorepo root and enables independent development, testing, and CI.
- See: [TypeScript Project References](https://www.typescriptlang.org/docs/handbook/project-references.html)

### 2. Modern TypeScript & Node.js ESM

- Uses `module: NodeNext` and `moduleResolution: NodeNext` for full ESM compatibility with Node.js 20+.
- All scripts are written in TypeScript and run via [`ts-node-esm`](https://typestrong.org/ts-node/docs/imports/).
- Mesh config can be `.ts` or `.mts` and is referenced directly in scripts.
- See: [Node.js ESM Docs](https://nodejs.org/api/esm.html), [ts-node ESM](https://typestrong.org/ts-node/docs/imports/)

### 3. Containerized Tooling Pattern

- All build, schema composition, and extraction steps are run in containers by default (using `docker compose run`).
- For local development, set `USE_LOCAL_TOOLS=1` to run tools natively.
- This ensures reproducibility, fast onboarding, and CI/CD parity.
- See: [Docker Compose Override Pattern](https://docs.docker.com/compose/extends/)

### 4. Mesh Transformation Sidecar

- The `guac-mesh-graphql` service acts as the public interface for the Spoke, wrapping the internal GUAC GraphQL API.
- Mesh applies custom transforms (see `scripts/augment-transform.ts`) to synthesize global IDs and add Apollo Federation directives.
- See: [GraphQL Mesh Docs](https://www.graphql-mesh.com/docs/getting-started), [Mesh Transforms](https://www.graphql-mesh.com/docs/handlers/graphql#transforms)

### 5. ETL Extractor

- The `scripts/extractor.ts` script (currently a diagnostic/sanity check) will be replaced with a full ETL pipeline that:
  - Introspects the Mesh GraphQL schema
  - Fetches all core node and edge types
  - Serializes results as Dgraph N-Quads RDF
  - Outputs a gzipped RDF file for bulk loading into Dgraph
- See: [Dgraph Bulk Loader](https://dgraph.io/docs/deploy/bulk-loader/), [Dgraph RDF N-Quads Format](https://dgraph.io/docs/migration/rdf-operations/)

### 6. Developer Workflow

- All operational tasks are executed through the root `Makefile` (see monorepo root).
- Use `make extract` to run the extractor (containerized by default, local override supported).
- Use `make seed` to run the full pipeline: setup, ingest, extract, and bulk load.
- See: [GNU Make Manual](https://www.gnu.org/software/make/manual/make.html)

### 7. Extending to a Full Cluster

- The Dgraph Compose file is structured to make it easy to:
  - Start only Zero (for bulk loading)
  - Start only Alpha (after loading)
  - Scale to a full cluster (multi-Zero, multi-Alpha) using Compose profiles and scaling
- See: [Dgraph Cluster with Docker Compose](https://dgraph.io/docs/deploy/docker/#running-dgraph-in-a-cluster)

## Directory Structure

```shell
guac-mesh-graphql/
├── .gitignore
├── Dockerfile
├── package.json
├── tsconfig.json
├── scripts/
│   ├── extractor.ts
│   └── extractor.wip.ts
├── build/
└── ...
```

## Benchmarking Dataset: 1million RDF

For Dgraph ETL and loader benchmarking, we use the public [1million RDF dataset](https://github.com/hypermodeinc/dgraph-benchmarks/tree/main/data):

- **RDF Data:** `guac-mesh-graphql/benchmark/1million.rdf.gz` (16MB compressed)
- **Indexed Schema:** `guac-mesh-graphql/benchmark/1million.schema` (628 bytes)
- **No-Index Schema:** `guac-mesh-graphql/benchmark/1million-noindex.schema` (471 bytes)

### Example Schema (Indexed)

```
director.film        : [uid] @reverse @count .
actor.film           : [uid] @count .
genre                : [uid] @reverse @count .
initial_release_date : datetime @index(year) .
rating               : [uid] @reverse .
country              : [uid] @reverse .
loc                  : geo @index(geo) .
name                 : string @index(hash, term, trigram, fulltext) @lang .
starring             : [uid] @count .
performance.character_note : string @lang .
tagline              : string @lang .
cut.note             : string @lang .
rated                : [uid] @reverse .
email                : string @index(exact) @upsert .
```

### Example Schema (No-Index)

```
director.film        : [uid] .
actor.film           : [uid] .
genre                : [uid] .
initial_release_date : datetime .
rating               : [uid] .
country              : [uid] .
loc                  : geo .
name                 : string @lang .
starring             : [uid] .
performance.character_note : string @lang .
tagline              : string @lang .
cut.note             : string @lang .
rated                : [uid] .
email                : string .
```

### Usage

### Makefile Targets for Benchmarking

- `make fetch-benchmark-data` — Download the RDF and both schemas to `guac-mesh-graphql/benchmark/`.
- `make clean` — Remove all downloaded benchmarking data and build artifacts.
- `make wipe-dgraph-data` — Remove all Dgraph persistent data and build output (required before each bulk load benchmark).
- `make benchmark-bulk-indexed` — Benchmark Dgraph bulk loader with the indexed schema (wipes data first, then loads and times the operation).
- `make benchmark-bulk-noindex` — Benchmark Dgraph bulk loader with the no-index schema (wipes data first, then loads and times the operation).

These targets allow you to compare Dgraph bulk load performance with and without schema indexes, using a reproducible public dataset.

## Further Reading

- [GraphQL Mesh Docs](https://www.graphql-mesh.com/docs/getting-started)
- [Dgraph Bulk Loader](https://dgraph.io/docs/deploy/bulk-loader/)
- [Dgraph RDF N-Quads Format](https://dgraph.io/docs/migration/rdf-operations/)
- [Node.js ESM Modules](https://nodejs.org/api/esm.html)
- [ts-node ESM Support](https://typestrong.org/ts-node/docs/imports/)
- [Docker Compose Profiles](https://docs.docker.com/compose/profiles/)
- [Dgraph Cluster with Docker Compose](https://dgraph.io/docs/deploy/docker/#running-dgraph-in-a-cluster)
- [GNU Make Manual](https://www.gnu.org/software/make/manual/make.html)
</file>

<file path=".env.example">
# TOOLING MODE
# Set to 1 to run build/extract/composition tools natively (local dev/iteration). Default is containerized.
USE_LOCAL_TOOLS=1
# SHARED RESOURCES
EXTERNAL_NETWORK_NAME=graphtastic_net

# DGRAPH CONFIG
DGRAPH_ALPHA_WHITELIST=0.0.0.0/0
# Dgraph data storage mode: 'bind' (default, recommended for local dev) or 'volume' (for Docker named volumes)
DGRAPH_DATA_MODE=bind
# If using DGRAPH_DATA_MODE=volume, set the Docker volume name (default: dgraph_data)
DGRAPH_DATA_VOLUME=dgraph_data
# Add any necessary Dgraph environment variables here
</file>

<file path="compose/mesh.yml">
# compose/mesh.yml

services:
  guac-mesh-graphql:
    build: ./guac-mesh-graphql
    container_name: guac-mesh-graphql
    networks:
      - guac_internal_net
      - graphtastic_net
    ports:
      - "4000:4000"
    volumes:
      - ../guac-mesh-graphql/.meshrc.yml:/usr/src/app/.meshrc.yml
      - ./dgraph-stack/mesh-cache:/usr/src/app/.mesh

networks:
  graphtastic_net:
    external: true
</file>

<file path="Makefile">
# Graphtastic GUAC Spoke Makefile Control Plane
#
# This Makefile is the single source of truth for all developer and CI workflows in this project.
# All operational tasks—setup, orchestration, diagnostics, data pipeline, and benchmarking—must be run via these targets.
#
# === Configuration Variables ===
EXTERNAL_NETWORK_NAME ?= graphtastic_net
COMPOSE_FILE ?= docker-compose.yml

# Project-specific paths
DGRAPH_STACK_DIR := dgraph-stack
BUILD_DIR := build
SBOMS_DIR := sboms
BENCHMARK_DIR := guac-mesh-graphql/benchmark
SCHEMA_DIR := schema

# Dgraph bulk loader specific configuration
DGRAPH_BULK_LOADER_SERVICE := dgraph-bulk-loader
DGRAPH_BULK_ARGS := --map_shards=1 --reduce_shards=1

# === Main Targets ===
.PHONY: help setup up down clean status logs

help:
	@echo "Usage: make [target]"
	@echo ""
	@echo "=== Main Targets ==="
	@echo "  setup                - Prepare the local environment (e.g., check .env, create networks)."
	@echo "  up                   - Bring up all services (full stack) as defined by $(COMPOSE_FILE)."
	@echo "  down                 - Bring down all services."
	@echo "  clean                - Run 'down', then remove all persistent data and build artifacts."
	@echo "  status               - Show container, network, and volume status (diagnostics)."
	@echo "  logs                 - Tail logs for all running services."
	@echo ""
	@echo "=== Service Management ==="
	@echo "  svc-up SVC=compose/xx.yml [SERVICE=service]   - Bring up a specific compose file (and optionally a service)."
	@echo "  svc-down SVC=compose/xx.yml [SERVICE=service] - Bring down a specific compose file (and optionally a service)."
	@echo "  svc-logs SVC=compose/xx.yml [SERVICE=service] - Tail logs for a specific compose file (and optionally a service)."
	@echo ""
	@echo "=== Data Pipeline ==="
	@echo "  ingest-sboms         - Ingest SBOMs from the ./$(SBOMS_DIR) directory into GUAC."
	@echo "                         (Note: Targets GUAC's internal GraphQL API on localhost:8080 within its container)"
	@echo "  extract              - Run the ETL script to extract from Mesh and generate RDF."
	@echo "  seed                 - Perform a full, clean data seed from SBOMs to Dgraph (end-to-end pipeline)."
	@echo ""
	@echo "=== Benchmarking & Utilities ==="
	@echo "  fetch-benchmark-data - Download 1million RDF and schemas for benchmarking."
	@echo "  wipe-dgraph-data     - Remove all Dgraph persistent data and build output (for benchmarking)."
	@echo "  benchmark-bulk-indexed   - Benchmark Dgraph bulk loader with indexed schema (wipes data first)."
	@echo "  benchmark-bulk-noindex   - Benchmark Dgraph bulk loader with no-index schema (wipes data first)."
	@echo "  stop-alpha           - Stop only Dgraph Alpha containers."
	@echo "  start-alpha          - Start only Dgraph Alpha containers."
	@echo "  stop-zero            - Stop only Dgraph Zero containers."
	@echo "  start-zero           - Start only Dgraph Zero containers."
	@echo "  check-dockerfiles    - Check for missing Dockerfiles referenced in compose files."
	@echo "  check-envfile        - Ensure .env exists by copying from .env.example if needed."
	@echo ""
	@echo "=== Notes ==="
	@echo " - All targets are orchestrated via Docker Compose and follow the Principle of the Makefile Control Plane."
	@echo " - Never run 'docker' or 'docker compose' directly—always use these targets."
	@echo " - For more details, run: make help"

# --- Setup & Environment Checks ---

# Ensure .env exists by copying from .env.example if needed
.PHONY: check-envfile
check-envfile:
	@echo "Checking for .env file..."
	@if [ -f .env.example ] && [ ! -f .env ]; then \
	  echo ".env not found, copying from .env.example..."; \
	  cp .env.example .env; \
	fi

setup: check-envfile
	@echo "--- Initializing shared resources ---"
	# Create the external network required by some compose files (e.g., dgraph.yml, mesh.yml).
	# This uses a direct 'docker network create' as docker-compose itself does not create
	# networks declared as 'external: true'.
	@docker network create $(EXTERNAL_NETWORK_NAME) >/dev/null 2>&1 || true
	# GUAC Postgres uses bind mount only for persistent data (see compose/guac.yml)
	# TODO: Add volume creation logic for GUAC Postgres when/if volume mode is supported


# --- Core Service Management ---
up:
	docker compose -f $(COMPOSE_FILE) up -d

down:
	docker compose -f $(COMPOSE_FILE) down

clean:
	@echo "--- Performing a full cleanup ---"
	docker compose -f $(COMPOSE_FILE) down -v --remove-orphans
	rm -rf ./$(DGRAPH_STACK_DIR)
	rm -rf ./$(BUILD_DIR)
	rm -rf ./$(BENCHMARK_DIR)
	# TODO: Add volume removal logic for GUAC Postgres when volume mode is supported

status:
	@echo "--- Container Status ---"
	docker compose -f $(COMPOSE_FILE) ps
	@echo "--- Docker Networks ---"
	docker network ls
	@echo "--- Namespaced dgraph-net Network Inspect ---"
	docker network inspect subgraph-dgraph-software-supply-chain_dgraph-net | jq '.[0].Containers' || docker network inspect subgraph-dgraph-software-supply-chain_dgraph-net
	@echo "--- External graphtastic_net Network Inspect ---"
	docker network inspect $(EXTERNAL_NETWORK_NAME) | jq '.[0].Containers' || docker network inspect $(EXTERNAL_NETWORK_NAME)
	@echo "--- Volumes ---"
	docker volume ls
	@echo "--- Disk Usage ($(DGRAPH_STACK_DIR), $(BUILD_DIR), $(SBOMS_DIR)) ---"
	du -sh ./$(DGRAPH_STACK_DIR) 2>/dev/null || true
	du -sh ./$(BUILD_DIR) 2>/dev/null || true
	du -sh ./$(SBOMS_DIR) 2>/dev/null || true

logs:
	@echo "--- Tailing logs for all running services ---"
	docker compose -f $(COMPOSE_FILE) logs -f

# --- Generic Service Management ---
.PHONY: svc-up svc-down svc-logs
svc-up:
	@if [ -z "$(SVC)" ]; then \
		echo "Error: SVC (compose file) is required. Example: make svc-up SVC=compose/dgraph.yml"; exit 1; \
	fi; \
	docker compose -f $(SVC) up -d $(SERVICE)

svc-down:
	@if [ -z "$(SVC)" ]; then \
		echo "Error: SVC (compose file) is required. Example: make svc-down SVC=compose/dgraph.yml"; exit 1; \
	fi; \
	docker compose -f $(SVC) down $(SERVICE)

svc-logs:
	@if [ -z "$(SVC)" ]; then \
		echo "Error: SVC (compose file) is required. Example: make svc-logs SVC=compose/dgraph.yml"; exit 1; \
	fi; \
	docker compose -f $(SVC) logs -f $(SERVICE)

# --- Dgraph Specific Utilities ---
.PHONY: stop-alpha start-alpha stop-zero start-zero wipe-dgraph-data
stop-alpha:
	docker compose -f compose/dgraph.yml stop dgraph-alpha || true

start-alpha:
	docker compose -f compose/dgraph.yml start dgraph-alpha || true

stop-zero:
	docker compose -f compose/dgraph.yml stop dgraph-zero || true

start-zero:
	docker compose -f compose/dgraph.yml start dgraph-zero || true

# Remove Dgraph persistent data (for benchmarking bulk loader)
wipe-dgraph-data:
	rm -rf ./$(DGRAPH_STACK_DIR)
	rm -rf ./$(BUILD_DIR)


# --- Benchmarking & Data Utilities ---
.PHONY: fetch-benchmark-data benchmark-bulk-indexed benchmark-bulk-noindex

# Fetch benchmarking RDF and schema files for Dgraph
fetch-benchmark-data:
	@echo "Creating benchmark directory and fetching 1million RDF and schemas..."
	mkdir -p $(BENCHMARK_DIR)
	@echo "Downloading 1million.rdf.gz..."
	wget -O $(BENCHMARK_DIR)/1million.rdf.gz https://github.com/hypermodeinc/dgraph-benchmarks/raw/refs/heads/main/data/1million.rdf.gz
	@echo "Downloading 1million.schema..."
	wget -O $(BENCHMARK_DIR)/1million.schema https://github.com/hypermodeinc/dgraph-benchmarks/raw/refs/heads/main/data/1million.schema
	@echo "Downloading 1million-noindex.schema..."
	wget -O $(BENCHMARK_DIR)/1million-noindex.schema https://github.com/hypermodeinc/dgraph-benchmarks/raw/refs/heads/main/data/1million-noindex.schema
	@echo "Benchmark RDF and schema files downloaded to $(BENCHMARK_DIR)/."

# Internal macro to run Dgraph bulk loader benchmarks
define _run-benchmark-bulk
	$(MAKE) clean
	@echo "[Benchmark] Bringing up Dgraph Zero only..."
	docker compose -f compose/dgraph.yml up -d dgraph-zero
	@echo "[Benchmark] Bulk loading with $(1)..."
	time docker compose -f $(COMPOSE_FILE) run --rm \
	  -v $(CURDIR)/$(BENCHMARK_DIR):/dgraph/benchmark \
	  $(DGRAPH_BULK_LOADER_SERVICE) dgraph bulk -f /dgraph/benchmark/1million.rdf.gz -s $(2) $(DGRAPH_BULK_ARGS)
	$(MAKE) down
endef

benchmark-bulk-indexed:
	$(call _run-benchmark-bulk,indexed schema,/dgraph/benchmark/1million.schema)

benchmark-bulk-noindex:
	$(call _run-benchmark-bulk,no-index schema,/dgraph/benchmark/1million-noindex.schema)

# --- Data Pipeline ---
.PHONY: ingest-sboms extract seed

ingest-sboms:
	@echo "--- Ingesting SBOMs from ./$(SBOMS_DIR) into GUAC ---"
	# This command runs inside the guac-graphql container and targets GUAC's
	# own internal GraphQL API, which typically runs on localhost:8080 within the container.
	docker compose exec guac-graphql /opt/guac/guacone collect files --csub-addr guac-collectsub:2782 --gql-addr http://localhost:8080/query --add-vuln-on-ingest --add-license-on-ingest --add-eol-on-ingest /sboms

extract:
	@echo "--- Extracting from Mesh to RDF ---"
       # Containerized by default for reproducibility; set USE_LOCAL_TOOLS=1 for native execution
       @if [ "$$USE_LOCAL_TOOLS" = "1" ]; then \
	       echo "[local mode] Running extractor script..."; \
	       ts-node-esm guac-mesh-graphql/scripts/extractor.ts; \
       else \
	       echo "[container mode] Running extractor in Docker..."; \
	       docker compose -f compose/tools.yml run --rm extractor ts-node-esm guac-mesh-graphql/scripts/extractor.ts; \
       fi

seed: clean setup up
	@echo "--- Starting full data seed process ---"
	$(MAKE) ingest-sboms
	$(MAKE) extract
	@echo "--- Stopping Dgraph Alpha before bulk load (only Zero should be running) ---"
	$(MAKE) stop-alpha
	@echo "--- Running Dgraph Bulk Loader (initial import, not live loader) ---"
	# The following command runs the Dgraph Bulk Loader in a container via docker compose run,
	# mounting the RDF and schema files from the host into the dgraph-bulk-loader service.
	docker compose -f $(COMPOSE_FILE) run --rm \
		-v $(CURDIR)/$(BUILD_DIR):/dgraph/build \
		-v $(CURDIR)/$(SCHEMA_DIR):/dgraph/schema \
		$(DGRAPH_BULK_LOADER_SERVICE) dgraph bulk -f /dgraph/build/guac.rdf.gz -s /dgraph/schema/schema.txt $(DGRAPH_BULK_ARGS)
	@echo "--- Bulk load complete. ---"
	@echo "--- Starting Dgraph Alpha after bulk load ---"
	$(MAKE) start-alpha
	@echo "--- IMPORTANT: Copy the generated out/0/p directory to your Dgraph Alpha's data directory before starting Alpha if not already done. ---"
	@echo "For small datasets, start one Alpha, copy out/0/p, start Alpha, then add replicas. For larger datasets, copy to all Alpha nodes before starting them."
	@echo "See https://docs.hypermode.com/dgraph/admin/bulk-loader for details."
	@echo "--- Seed process finished. ---"

# --- Other Utilities ---
.PHONY: check-dockerfiles
check-dockerfiles:
	@echo "Checking for required Dockerfiles referenced in compose files..."
	@missing=0; \
	for compose in compose/*.yml; do \
	  grep '^\s*build:' $$compose | awk '{print $$2}' | while read dir; do \
	    if [ -n "$$dir" ] && [ ! -f "$$dir/Dockerfile" ]; then \
	      echo "❌ Missing Dockerfile: $$dir/Dockerfile (referenced in $${compose})"; \
	      missing=1; \
	    fi; \
	  done; \
	done; \
	if [ "$$missing" -eq 0 ]; then \
	  echo "✅ All referenced Dockerfiles are present."; \
	fi
</file>

<file path="docs/design/on--dgraph-docker-compose.md">
# Architecting a Production-Ready Dgraph Environment with Docker Compose

This document provides a comprehensive methodology for deploying and managing the Dgraph graph database using Docker Compose. The focus is on establishing a resilient, high-performance local development environment that mirrors production best practices, facilitates rapid iteration, and provides transparent management of configuration and persistent storage. The provided architecture and recommendations are specifically tailored to support demanding applications, such as large-scale social networks.

## Architecting a Resilient Dgraph Cluster with Docker Compose

### Introduction: Moving beyond the standalone image

Initial exploration of Dgraph often begins with the `dgraph/standalone` Docker image, a convenient tool for quick demonstrations. However, this all-in-one image, which bundles Dgraph Zero and Alpha processes into a single container, is explicitly designated as unsuitable for production environments and, by extension, for any serious development or testing workload. Relying on this image introduces significant architectural compromises, including a lack of service isolation, an inability to scale components independently, and opaque resource management. Furthermore, historical inconsistencies regarding the inclusion of the Ratel UI have created confusion and demonstrated the brittleness of such a monolithic approach.

We start with a production-like architecture from the outset. This involves a multi-service deployment where each component of the Dgraph cluster is managed as a distinct, containerized service. We will use docker compose initially to facilitate CI workflows as well as local iterative development, then generate kubernetes manifests. This approach ensures that the development environment accurately reflects the behavior and operational characteristics of a production deployment from the outset of the effort.

### The Core Components of a Dgraph Cluster

A standard Dgraph cluster is composed of three primary, independent components that work in concert:

**TODO: diagram**

* **Dgraph Zero: The Cluster Coordinator**
    Dgraph Zero serves as the control plane of the cluster. Its responsibilities include managing cluster metadata, distributing data shards (known as tablets) across Alpha nodes, and maintaining cluster-wide consensus using the Raft protocol. It orchestrates leader elections within replica groups and acts as the central authority for transaction timestamp assignment. Zero communicates with Alpha nodes over a gRPC port (default `5080`) and exposes an HTTP endpoint for administrative and metrics purposes (default `6080`).
* **Dgraph Alpha: The Data Workhorse**
    Dgraph Alpha is the data plane of the cluster. It is responsible for storing the graph data itself, including nodes, edges (stored in posting lists), and indexes. Alpha nodes execute all client queries and mutations, manage transaction logic, and serve data over both gRPC (default `9080`) and HTTP (default `8080`). The GraphQL API layer is an integral part of the Alpha node, making it the primary entry point for application interactions. Alphas are critically dependent on Zero nodes for cluster membership information and transaction coordination.
* **Dgraph Ratel: The Visual Interface**
    Ratel is an essential administrative and development tool that provides a graphical user interface for interacting with the Dgraph cluster. It allows engineers to execute DQL and GraphQL queries, perform mutations, visualize graph data, and manage the database schema. Ratel is a standalone web application delivered via its own Docker image, `dgraph/ratel`. It operates entirely client-side in the user's browser, connecting to the public HTTP endpoint of a Dgraph Alpha node (port `8080`) to perform its functions.

### Why Docker Compose is the Ideal Tool for Development

Docker Compose is a tool for defining and running multi-container Docker applications through a declarative YAML file. It is the ideal choice for orchestrating a local Dgraph cluster for several reasons. It codifies the entire multi-service architecture—including services, networking, and storage volumes—into a single, version-controllable `docker-compose.yml` file. This practice, a form of "Infrastructure as Code," ensures that every developer on a team can spin up an identical, reproducible environment with a single command. This declarative approach abstracts away the complexity of imperative `docker run` commands with their numerous flags and network configurations, resulting in a cleaner, more manageable, and less error-prone setup.

## The Definitive Docker Compose Configuration for Dgraph

This section presents a complete, annotated Docker Compose configuration designed for stability, persistence, and ease of management.

### Foundational Directory Structure

A well-organized directory structure is crucial for managing persistent data and configuration files. This structure isolates all Dgraph-related assets and aligns with the volume mappings defined in the `docker-compose.yml` file. Before proceeding, create the following directory structure in your project's root:

**TODO ensure is wrapped into the makefile, and located in .graphtastic or somesuch**

```bash
mkdir -p dgraph-stack/dgraph/zero
mkdir -p dgraph-stack/dgraph/alpha
mkdir -p dgraph-stack/config
```

This structure creates separate directories for Zero and Alpha data, preventing state corruption that can occur when services share volumes or when volumes persist incorrectly between different cluster instantiations.

### The Complete docker-compose.yml

This configuration defines the three core services, persistent volumes, and a dedicated network for inter-service communication.

**TODO: use external network**

```yaml
version: "3.8"

services:
  zero:
    image: dgraph/dgraph:latest
    container_name: dgraph_zero
    volumes:
      - ./dgraph-stack/dgraph/zero:/dgraph
    ports:
      - "5080:5080"
      - "6080:6080"
    restart: on-failure
    command: dgraph zero --my=zero:5080
    networks:
      - dgraph-net

  alpha:
    image: dgraph/dgraph:latest
    container_name: dgraph_alpha
    volumes:
      - ./dgraph-stack/dgraph/alpha:/dgraph
      # The config volume will be used in Section 3
      # - ./dgraph-stack/config/start-alpha.sh:/usr/local/bin/start-alpha.sh
    ports:
      - "8080:8080"
      - "9080:9080"
    restart: on-failure
    command: dgraph alpha --my=alpha:7080 --zero=zero:5080
    depends_on:
      - zero
    networks:
      - dgraph-net

  ratel:
    image: dgraph/ratel:latest
    container_name: dgraph_ratel
    ports:
      - "8000:8000"
    restart: on-failure
    networks:
      - dgraph-net

networks:
  dgraph-net:
    driver: bridge
```

### Services

* **The `zero` Service:**
  * `image: dgraph/dgraph:latest`: Uses the official, unified Dgraph image for the cluster components.
  * `volumes:./dgraph-stack/dgraph/zero:/dgraph`: Maps the dedicated host directory for Zero's persistent state into the container. Zero primarily writes to a `zw` subdirectory here, containing its write-ahead logs.
  * `ports`: Exposes port `5080` for Alpha-to-Zero communication and `6080` for its HTTP/metrics endpoint.
  * `command: dgraph zero --my=zero:5080`: This command instructs the container to start a Zero process. The `--my` flag is critical; `zero:5080` becomes the addressable name of this service *within the Docker network*, leveraging Docker's internal DNS for service discovery.
* **The `alpha` Service:**
  * `volumes:./dgraph-stack/dgraph/alpha:/dgraph`: Maps a *separate* host directory for Alpha's data. This isolation is paramount to prevent data corruption. Alpha stores the primary graph data and indexes in the `p` directory and its write-ahead logs in the `w` directory within this volume.
  * `ports`: Exposes port `8080` for the HTTP/GraphQL API and `9080` for the gRPC API.
  * `command: dgraph alpha --my=alpha:7080 --zero=zero:5080`: Starts an Alpha process. `--my=alpha:7080` sets its own address, and `--zero=zero:5080` tells it how to discover the Zero service using its service name. This reliable discovery mechanism avoids common networking issues associated with using `localhost` or hardcoded IP addresses.
* **The `ratel` Service:**
  * `image: dgraph/ratel:latest`: Uses the dedicated Ratel image, which is the correct practice as Ratel is no longer bundled with other images.
  * `ports`: Exposes port `8000`, the default for the Ratel UI, to the host machine.

### Persistent Storage Strategy: A Deep Dive

The configuration uses Docker bind mounts, directly mapping host directories into the containers. This strategy was chosen to satisfy the requirement for data to be easily visible and manageable on the local filesystem, offering greater transparency than Docker-managed named volumes. The table below details the purpose of each persistent directory.

| Host Path                     | Container Path | Service | Internal Directories | Purpose                                                                                                                              |
| :---------------------------- | :------------- | :------ | :------------------- | :----------------------------------------------------------------------------------------------------------------------------------- |
| `./dgraph-stack/dgraph/zero`  | `/dgraph`      | `zero`  | `zw`                 | Stores Zero's Raft write-ahead logs and cluster state. Critical for cluster membership and transaction coordination.                 |
| `./dgraph-stack/dgraph/alpha` | `/dgraph`      | `alpha` | `p`, `w`, `x`        | Stores Alpha's posting lists (`p`, the graph data/indices), Raft WALs (`w`), and live loader mappings (`x`). This is the core database data. |

### Networking (external)

**TODO update to selectively use both default networking, and optionally to participate in an external network to facilitate construction via multiple compose files**

A custom bridge network, `dgraph-net`, is defined and attached to all services. This is a Docker best practice that creates an isolated network for the application. Within this network, Docker provides automatic service discovery, allowing containers to resolve each other by their service name (e.g., the `alpha` container can reach the `zero` container at the hostname `zero`). This makes the `--zero=zero:5080` flag function reliably and renders the entire stack portable across different host machines without any changes to the configuration.

## Externalized Configuration for Rapid Development

Embedding configuration flags directly within the `command` section of the `docker-compose.yml` file is inflexible and hinders rapid iteration. Any change to a flag requires modifying the core infrastructure definition file. To address this, configuration should be decoupled from the service definition.

### Pattern 1: Using Environment Files for Simple Flags

**TODO: use .env.default-rename-me file to make this simpiler, consider wrapping into makefile**

For simple, single-value flags, Docker Compose's support for environment files provides a clean solution. For example, to manage the admin operations whitelist:

1. Create a file named `.env` in the same directory as your `docker-compose.yml`.
2. Add the following line to the `.env` file:

    ```ini
    DGRAPH_ALPHA_WHITELIST=0.0.0.0/0
    ```

3. Modify the `alpha` service's `command` in `docker-compose.yml` to use this variable:

    ```yaml
    command: dgraph alpha --my=alpha:7080 --zero=zero:5080 --security "whitelist=${DGRAPH_ALPHA_WHITELIST}"
    ```

Now, the whitelist can be modified in the `.env` file, and the change can be applied by simply restarting the container.

### Pattern 2: Using Mounted Scripts for Complex Configuration

While not ideal, this pattern is included for reference. We prefer declarative config wherever possible, vs. running scripts. For managing a larger set of configuration flags, a mounted startup script offers maximum flexibility and provides an accessible way to learn CLI commands for dgraph. This pattern moves the entire command logic into an external, version-controllable script.

1. Create a file named `start-alpha.sh` inside the `./dgraph-stack/config/` directory:

    ```bash
    #!/bin/sh
    # This script allows for complex flag management outside of docker-compose.yml
    # Make this script executable with: chmod +x start-alpha.sh

    dgraph alpha \
      --my=alpha:7080 \
      --zero=zero:5080 \
      --security "whitelist=0.0.0.0/0" \
      --limit "query-edge=1000000; mutations-nquad=1000000;" \
      --telemetry "reports=false; sentry=false;"
    ```

2. Make the script executable: `chmod +x ./dgraph-stack/config/start-alpha.sh`.
3. Modify the `alpha` service in `docker-compose.yml` to mount and execute this script:

    ```yaml
    services:
      alpha:
        #... other properties
        volumes:
          - ./dgraph-stack/dgraph/alpha:/dgraph
          - ./dgraph-stack/config/start-alpha.sh:/usr/local/bin/start-alpha.sh
        command: "start-alpha.sh"
        #... other properties
    ```

This approach completely decouples the runtime configuration of the Dgraph Alpha from the Docker Compose definition. Engineers can now freely edit the `start-alpha.sh` file to add or modify any of the extensive command-line flags Dgraph offers and apply them with a simple container restart.

## Cluster Initialization, Schema Management, and Verification

With the architecture defined, the final steps involve launching the cluster, applying an initial schema, and verifying its operation.

### Lifecycle Management with Docker Compose

The following commands are used to manage the Dgraph stack:

* **Start the cluster:** `docker-compose up -d`. The `-d` flag runs the containers in detached mode, freeing the terminal.
* **Monitor logs:** `docker-compose logs -f alpha`. Tailing the logs of a specific service is essential for observing its startup process and diagnosing any issues.
* **Stop and remove containers:** `docker-compose down`. This command stops and removes the containers and the network, but the data in the bind-mounted volumes on the host will persist.
* **Full reset:** `docker-compose down -v`. This command will also remove any named volumes associated with the stack. For a complete reset with bind mounts, one must manually delete the contents of the `./dgraph-stack/dgraph` directory. This is often necessary to resolve startup issues caused by stale or corrupt state from previous runs.

### Applying the Initial Schema via HTTP API

Before data can be loaded, a schema must be defined to specify predicates, their types, and any desired indexes. Dgraph management is API-driven, and the schema is applied by sending a `POST` request to the Alpha's `/alter` endpoint. This process can and should be automated in real-world projects.

1. Create a file named `schema.dql` in the `./dgraph-stack/config/` directory. (A sample schema for a social network is provided in the next section).
2. With the Dgraph cluster running, execute the following `curl` command from your terminal:

    ```bash
    curl -X POST localhost:8080/alter -d @./dgraph-stack/config/schema.dql
    ```

    This command reads the content of `schema.dql` and sends it as the request body. A successful operation will return a JSON response indicating success: `{"data":{"code":"Success","message":"Done"}}`.

### Connecting with Ratel for Verification

The final step is to use Ratel to visually confirm that the entire stack is operational and the schema has been applied correctly.

1. Open a web browser and navigate to `http://localhost:8000`.
2. In the Ratel connection modal, ensure the "Dgraph Server URL" is set to `http://localhost:8080`. This points Ratel to the exposed HTTP port of the `alpha` service.
3. After connecting, navigate to the "Schema" tab on the left-hand sidebar. The predicates and types defined in your `schema.dql` file should be visible.

A successful connection and schema view in Ratel provides a complete end-to-end validation of the architecture, confirming that networking, port mapping, service discovery, and volume persistence are all functioning correctly.

## Strategic Recommendations for a Social Networking Application

The following recommendations provide a starting point for designing a Dgraph-backed social network, focusing on schema design, indexing, and performance.

### Designing a Social Network Graph Schema (DQL)

When modeling for a graph database, the primary focus should be on the entities (nodes) and the rich relationships (edges) between them, rather than on tabular structures. The schema defines not just the data structure, but also the API contract for the application.
Below is a sample `schema.dql` file for a basic social network, which should be placed in `./dgraph-stack/config/schema.dql`.

```dql
# --- Scalar Predicates ---
username: string @index(hash) @upsert .
email: string @index(exact) @upsert .
displayName: string @index(term) .
bio: string .
avatar: string .
createdAt: datetime @index(hour) .
postText: string @index(fulltext) .

# --- UID Predicates (Relationships) ---
author: uid @reverse .
follows: uid @count @reverse .
likes: uid @count @reverse .

# --- Type Definitions ---
type User {
  username
  email
  displayName
  bio
  avatar
  createdAt
  follows
}

type Post {
  postText
  createdAt
  author
  likes
}
```

**Schema Deconstruction:**

* **`@upsert` Directive:** This is crucial for predicates that must be unique, such as `username` and `email`. It instructs Dgraph to check for conflicts on the indexed value during mutations, preventing duplicates.
* **`@reverse` Directive:** This directive is fundamental for efficient bidirectional traversal. Defining `author: uid @reverse` allows a query to easily find the author of a post. The automatically created reverse edge, `~author`, allows a query to just as easily find all posts created by a specific user. This is a core advantage of graph models for social data.
* **`@count` Directive:** This directive instructs Dgraph to maintain a real-time count of the number of edges for a given predicate on a node. This is extremely performant for features like displaying follower counts or like counts, as it avoids expensive traversal and aggregation queries at runtime.

### High-Performance Indexing Strategy

Proper indexing is the most critical factor for query performance in Dgraph. A query that filters on an un-indexed predicate will result in a full scan, leading to poor performance. The choice of index type must align with the intended query patterns.

| Predicate   | Data Type  | Recommended Index | Query Use Case / Application Feature                                                                 |
| :---------- | :--------- | :---------------- | :--------------------------------------------------------------------------------------------------- |
| `username`  | `string`   | `hash`            | **Profile Lookup:** Fast, case-sensitive exact match for `eq(username, "jane.doe")`.               |
| `displayName` | `string`   | `term`            | **User Search:** Keyword-based, case-insensitive search using `anyofterms(displayName, "Jane Doe")`. |
| `email`     | `string`   | `exact`           | **Uniqueness & Sorting:** Required for `@upsert`. The `exact` index also enables sorting and range queries. |
| `postText`  | `string`   | `fulltext`        | **Content Search:** Advanced, language-aware search with stemming and stop-word removal using `alloftext(postText, "graph databases")`. |
| `createdAt` | `datetime` | `hour`            | **Timelines/Feeds:** Efficient filtering and sorting of posts by time ranges, e.g., `ge(createdAt, "2024-01-01")`. |

### Performance and Scalability Considerations

* **Write Performance:** For applications with high write volumes, such as social media interactions (likes, follows, posts), it is essential to batch mutations. Sending a single mutation request containing 1,000 new edges is orders of magnitude more efficient than sending 1,000 separate requests. High write pressure can become a bottleneck, particularly due to the coordination required to assign transaction timestamps (`startTS`).
* **Query Optimization:** As Dgraph does not employ a sophisticated query optimizer, query structure significantly impacts performance. Queries should always start from the most specific entry point possible (e.g., a known user UID) and filter down, rather than starting with a broad scan of all nodes of a certain type.
* **Horizontal Scaling Architecture:** Dgraph achieves horizontal scaling by sharding data across multiple Alpha groups. The unit of sharding is the predicate. This means all data for a single predicate (e.g., `follows`) must reside within a single Alpha group. For a massive social network, a predicate like `follows` could grow exceptionally large, creating a vertical scaling bottleneck for the group that holds it, as it cannot be split further. This is an advanced architectural consideration that highlights a potential scaling limitation at extreme scale.

## Conclusion

This report has detailed a robust and reproducible methodology for running Dgraph with Docker Compose. By rejecting the simplistic `standalone` image in favor of a well-structured, multi-service architecture, developers can build a local environment that is stable, transparent, and aligned with production best practices.
The core principles of this approach are:

1. **Explicit Separation of Concerns:** Each Dgraph component (Zero, Alpha, Ratel) runs in its own container with isolated, dedicated persistent storage, preventing state corruption and simplifying debugging.
2. **Infrastructure as Code:** The entire stack is defined declaratively in a `docker-compose.yml` file, ensuring consistency and reproducibility across all development environments.
3. **Decoupled and Iterative Configuration:** By externalizing runtime flags into shell scripts or environment files, the configuration of Dgraph is decoupled from the infrastructure definition, enabling a rapid and frictionless development workflow.
4. **API-Driven Management:** The process of applying a schema via `curl` highlights Dgraph's API-first nature, paving the way for automated database migrations and administration.

For a social networking application, leveraging Dgraph's native graph features through a well-designed schema with directives like `@reverse` and `@count`, combined with a strategic indexing plan, provides a powerful foundation for building high-performance, relationship-centric features that are often complex and slow to implement in traditional relational databases. This methodology provides the architectural soundness required to build, test, and ultimately deploy such a system with confidence.

#### Works cited

1. hypermodeinc/dgraph: high-performance graph database for real-time use cases - GitHub, <https://github.com/hypermodeinc/dgraph>
2. Get Started - Quickstart Guide - Netlify, <https://release-v21-03--dgraph-docs-repo.netlify.app/docs/v21.03/get-started/>
3. Docker-compose dgraph/standalone, <https://discuss.dgraph.io/t/docker-compose-dgraph-standalone/14635>
4. Dgraph Labs - Docker Hub, <https://hub.docker.com/u/dgraph>
5. Getting Started docs say that standalone image contains Ratel, but it doesn't, <https://discuss.dgraph.io/t/getting-started-docs-say-that-standalone-image-contains-ratel-but-it-doesnt/16767>
6. Top 5 Dgraph Alternatives of 2025 - PuppyGraph, <https://www.puppygraph.com/blog/dgraph-alternatives>
7. Dgraph on Kubernetes: Why?. Horizontally Scaling a graph database… | by Joaquín Menchaca (智裕), <https://joachim8675309.medium.com/dgraph-on-kubernetes-why-cc7492a0f6f0>
8. Single Host Setup - Dgraph - Hypermode Docs, <https://docs.hypermode.com/dgraph/self-managed/single-host-setup>
9. Single Host Setup - Deploy - Netlify, <https://release-v21-03--dgraph-docs-repo.netlify.app/docs/v21.03/deploy/single-host-setup/>
10. API Endpoints - Dgraph - Hypermode, <https://docs.hypermode.com/dgraph/graphql/api>
11. Production Checklist - Deploy - Netlify, <https://release-v21-03--dgraph-docs-repo.netlify.app/docs/v21.03/deploy/production-checklist/>
12. Quickstart - Dgraph - Hypermode Docs, <https://docs.hypermode.com/dgraph/quickstart>
13. How To Setup Dgraph With Docker On Linux (Ubuntu) - Lion Blogger Tech, <https://www.lionbloggertech.com/how-to-setup-dgraph-with-docker-on-linux-ubuntu/>
14. dgraph/ratel - Docker Image, <https://hub.docker.com/r/dgraph/ratel>
15. How to Orchestrate Your Graph Application With Docker Compose - Memgraph, <https://memgraph.com/blog/how-to-orchestrate-your-graph-application-with-docker-compose>
16. Docker Compose Deployment - Hypermode Docs, <https://docs.hypermode.com/dgraph/self-managed/docker-compose>
17. How to start dgraph stack locally using compose, <https://discuss.dgraph.io/t/how-to-start-dgraph-stack-locally-using-compose/15820>
18. Dgraph Zero Docker volume - Users - Discuss Dgraph, <https://discuss.dgraph.io/t/dgraph-zero-docker-volume/2152>
19. Volumes - Docker Docs, <https://docs.docker.com/engine/storage/volumes/>
20. Command Reference - Dgraph - Hypermode, <https://docs.hypermode.com/dgraph/cli/command-reference>
21. Run Dgraph with Docker-Compose - YouTube, <https://www.youtube.com/watch?v=BZ84BmtmcW4>
22. Errors running dgraph/dgraph:master with Docker Compose, <https://discuss.dgraph.io/t/errors-running-dgraph-dgraph-master-with-docker-compose/11981>
23. Schema - Query language - Netlify, <https://release-v21-03--dgraph-docs-repo.netlify.app/docs/v21.03/query-language/schema/>
24. HTTP - Dgraph - Hypermode Docs, <https://docs.hypermode.com/dgraph/http>
25. Graph Data Models 101 - Dgraph - Hypermode, <https://docs.hypermode.com/dgraph/guides/graph-data-models-101>
26. Design a Schema for the App - Dgraph - Hypermode, <https://docs.hypermode.com/dgraph/guides/message-board-app/graphql/design-app-schema>
27. Using Dgraph as your database - DEV Community, <https://dev.to/sahilthakur7/using-dgraph-as-your-database-51o7>
28. Which side for add the @reverse is better? - Discuss Dgraph, <https://discuss.dgraph.io/t/which-side-for-add-the-reverse-is-better/13129>
29. Looking for Some Best Practices for Optimizing Queries in Dgraph?, <https://discuss.dgraph.io/t/looking-for-some-best-practices-for-optimizing-queries-in-dgraph/19741>
30. Optimizing Dgraph Write Performance with Cached startTS, <https://discuss.dgraph.io/t/optimizing-dgraph-write-performance-with-cached-startts/19820>
31. Mutate performance optimization - Discuss Dgraph, <https://discuss.dgraph.io/t/mutate-performance-optimization/5517>
32. Advice Needed on Optimizing Dgraph Query Performance - Users, <https://discuss.dgraph.io/t/advice-needed-on-optimizing-dgraph-query-performance/19464>
33. There are 500 million new tweets everyday. Is dgraph able to scale/shard that volume horizontally? is it true that if a predicate becomes large enough, the only way to deal with that is vertical scaling?, <https://discuss.dgraph.io/t/there-are-500-million-new-tweets-everyday-is-dgraph-able-to-scale-shard-that-volume-horizontally-is-it-true-that-if-a-predicate-becomes-large-enough-the-only-way-to-deal-with-that-is-vertical-scaling/16031>
</file>

<file path="docs/design/design--guac-to-dgraph.md">
# GUAC to Dgraph: A Subgraph Implementation Design

### Document Changelog

*   **v2.0 - September 7, 2025**
    *   **Strategic Reframing:** Re-contextualized the document to serve as the official internal implementation design for the `subgraph-dgraph-software-supply-chain` Spoke within the Graphtastic Platform.
    *   **Clarified Dual Purpose:** Explicitly stated the Spoke's dual role as both a standalone demonstrator for the CNCF Software Supply Chain Insights initiative and an integrated component of the Graphtastic supergraph.
    *   **Aligned with Tome Precepts:** Added diagrams and text to illustrate the "Spoke as a Black Box" principle, showing how the ETL pipeline is an encapsulated internal process.
    *   **Strengthened Federation Requirement:** Updated language to reflect that federation compliance is a primary, day-one requirement, not a future enhancement.
    *   **Integrated Developer Workflow:** Connected the tool's operational model to the Graphtastic `Makefile` and developer control plane conventions.

---

### Table of Contents

*   **1. Executive Summary & Strategic Goals**
    *   1.1. The Business Objective: A Unified, Persistent Software Supply Chain Graph
    *   1.2. Core Architectural Principles: Decoupling, Fidelity, and Performance
    *   1.3. Summary of Key Architectural Decisions
*   **2. Foundational Concepts: GraphQL as a Data Lingua Franca**
    *   2.1. The Impedance Mismatch: Understanding GraphQL's Design for Bulk Data
    *   2.2. The Macroscopic "N+1 Query Problem" and Its Avoidance
    *   2.3. On Unique Identifiers: Specification vs. Convention
        *   2.3.1. The Global Object Identification (GOI) Pattern
        *   2.3.2. Strategies for Synthesizing Globally Unique IDs
*   **3. The GUAC GraphQL Schema: An In-Depth Analysis**
    *   3.1. The GUAC Ontology: Deconstructing Nodes (Nouns) and Edges (Verbs)
    *   3.2. Classification of GUAC Types for Staged Migration
    *   3.3. Critical Finding: The Unsuitability of GUAC's `id` for Data Aggregation
    *   3.4. Identifying Natural Keys for Canonical Identification and Deduplication
*   **4. Target Architecture: The Dgraph GraphQL Endpoint**
    *   4.1. Automated Provisioning via Schema Definition Language (SDL)
    *   4.2. Schema Augmentation: Transforming the GUAC Schema for Dgraph
        *   4.2.1. Enabling Deduplication with the `@id` Directive
        *   4.2.2. Enabling High-Performance Queries with the `@search` Directive
    *   4.3. The Augmented GUAC Schema for Dgraph
*   **5. The Migration Pipeline: A Detailed Architectural Blueprint**
    *   5.1. High-Level Architecture: A Decoupled, Two-Phase ETL Process
    *   5.2. Phase 1: Data Extraction & Transformation (GUAC API → RDF Files)
        *   5.2.1. Core Engine: A Parallelized, Two-Stage Approach
        *   5.2.2. Query Strategy: High-Fidelity Pre-defined Queries
        *   5.2.3. Extractor Process Flow
    *   5.3. Phase 2: Data Loading (RDF Files → Dgraph)
        *   5.3.1. Initial Population: Dgraph Bulk Loader
        *   5.3.2. Incremental Updates: Dgraph Live Loader
        *   5.3.3. The Deduplication and Upsert Mechanism in Practice
*   **6. Implementation & Operational Guide**
    *   6.1. Recommended Technology Stack: Node.js & The Guild's Tooling
    *   6.2. Advanced Engineering for Performance and Resilience
        *   6.2.1. Resilient Extraction with Cursor-Based Pagination
        *   6.2.2. Maximizing Throughput with Buffering
        *   6.2.3. Reliability Patterns: Retries, Backoff, and Rate Limiting
    *   6.3. Configuration and Operationalization
*   **7. Post-Migration Validation & Integrity Audits**
    *   7.1. A Framework for Verifying Migration Success
    *   7.2. Validation Queries: Completeness, Integrity, and Deduplication
*   **8. Conclusion & Strategic Evolution**
    *   8.1. Summary of Architectural Recommendations
    *   8.2. Future Enhancement: Incremental (Delta) Migrations
    *   8.3. Future Enhancement: Evolving towards GraphQL Federation
*   **Appendix**
    *   A. The Guild's Ecosystem: Rationale for the Technology Stack
    *   B. List of Works Cited / Additional Resources

---

### 1. Executive Summary & Strategic Goals

This document outlines the architectural design for a high-performance, schema-driven Extract, Transform, Load (ETL) pipeline. The primary objective is to reliably and efficiently migrate large-scale graph datasets from one or more GUAC (Graph for Understanding Artifact Composition) instances into a single, persistent, and query-optimized Dgraph instance.

The following diagram illustrates the end-to-end vision, from individual open source projects generating supply chain metadata to the aggregation of that data into a unified, federated graph.

```mermaid
graph LR
    subgraph "Stage 1: Distributed Data Generation (Many Sources)"
        direction TB
        subgraph OSS Projects
            P1[Project A]
            P2[Project B]
            P3[...]
        end
        
        CI{"CI Process per Project<br/>(GUAC Ingest & RDF Export)"}
        
        P1 --> CI
        P2 --> CI
        P3 --> CI

        RDFs(Published RDF Artifacts)
        CI --> RDFs
    end

    subgraph "Stage 2: Centralized Aggregation (One Sink)"
        Loader["Internal ETL Pipeline<br/>(Dgraph Loaders)"] --> Dgraph[(Dgraph Database)]
    end

    subgraph "Stage 3: Unified API (One Endpoint)"
        Spoke["Dgraph-backed Spoke<br/>subgraph-dgraph-software-supply-chain"] --> Gateway(Supergraph Gateway)
    end

    RDFs -- "Input to" --> Loader
    Dgraph -- "Serves data via" --> Spoke
```

While the immediate demonstrator of this architecture is the GUAC-to-Dgraph migration, the underlying patterns and tooling are designed to be general-purpose, establishing a framework for leveraging GraphQL as a *lingua franca* for data interchange between disparate graph systems.

It is critical to note that this document describes the **internal architecture and data ingestion strategy for the `subgraph-dgraph-software-supply-chain` Spoke**, a component within the broader Graphtastic Platform. The ETL-to-RDF pipeline detailed herein is a deliberate, performance-driven implementation choice for populating this Spoke's stateful backend.

This component serves a dual purpose:
1.  To act as one of the reference implementations and as a **standalone demonstrator for the [CNCF Software Supply Chain Insights initiative](https://github.com/cncf/toc/issues/1709)**.
2.  To function as a fully compliant, federated **Spoke within the Graphtastic supergraph**.

Therefore, its design prioritizes both high-performance bulk data ingestion (as described here) and a federation-compliant GraphQL API as its external contract.

#### 1.1. The Business Objective: A Unified, Persistent Software Supply Chain Graph

GUAC instances generate invaluable software supply chain metadata. While GUAC can be used in ephemeral CI/CD build cycles, it is also suited for larger, centralized deployments. In either scenario, the default persistence layers may not be designed for the massive scale and complex analytical queries required when aggregating data across an entire organization.

The chosen platform for this persistent graph, aggregated from multiple GUAC isntances is Dgraph, a horizontally scalable, distributed GraphQL database. It is designed from the ground up for high-performance graph queries and provides essential features like ACID transactions, consistent replication, and linearizable reads. Dgraph is open source and licensed under the Apache-2.0 license, making it a suitable choice for this project.

The goal is to create a centralized, persistent Dgraph instance that serves as the canonical, aggregated source of truth for software supply chain information. This unified graph will enable advanced security analysis, dependency tracking, and vulnerability impact assessment at a scale not possible with isolated GUAC instances.

#### 1.2. Core Architectural Principles: Decoupling, Fidelity, and Performance

The design is founded on three core principles to ensure a robust and scalable solution:

1.  **Decoupling & Resilience**: The pipeline is separated into two distinct phases: extraction and loading. This modularity, connected via an intermediate file-based representation, ensures that a failure in one stage (e.g., a network issue during loading) does not require a costly restart of the entire process.

    This decoupling also facilitates a powerful open-source collaboration model. Individual open-source projects can run a CI job (e.g., a GitHub Action) that, after generating build artifacts like SBOMs, runs a local GUAC instance. This instance builds and enriches the graph for that specific project. The CI job then exports the complete graph as a compressed RDF file, which is published as a standard release artifact alongside binaries and source code. The centralized Dgraph instance can then fetch these RDF artifacts from many different projects and ingest them in a single bulk operation using the Dgraph Bulk Loader. For more continuous updates, the Live Loader will also be supported.
2.  **High Fidelity**: The extraction process is driven by the GUAC project's own pre-existing `.graphql` query files. This guarantees that data is fetched exactly as the source application's developers intended, improving reliability and significantly reducing long-term maintenance overhead.
3.  **Maximum Performance**: The architecture uses the most efficient tool for each job. For the massive initial data population, it leverages Dgraph's offline **Bulk Loader**. For all subsequent incremental updates, it uses the online **Live Loader**, ensuring maximum ingestion throughput in all scenarios.

#### 1.3. Summary of Key Architectural Decisions

*   **Core Architecture**: A decoupled, two-phase ETL pipeline where data is first extracted from the GUAC GraphQL API to compressed RDF files, and then loaded into Dgraph from those files.
*   **Technology Stack**: A Node.js and TypeScript environment to natively leverage the powerful, low-level tooling from The Guild's GraphQL ecosystem.
*   **Query Strategy**: A high-fidelity approach using GUAC's own pre-defined `.graphql` files as the primary source for extraction queries, with optional support for dynamic query generation.
*   **Data Deduplication**: A schema-driven "upsert" strategy, enabled by programmatically augmenting the source schema with Dgraph's `@id` directive on natural business keys. This pushes the complexity of deduplication down into the database layer, which is optimized for the task.

### 2. Foundational Concepts: GraphQL as a Data Lingua Franca

To architect a successful migration pipeline, one must first understand the nuances of using GraphQL—a protocol designed for selective data retrieval—for bulk data export. Within the Graphtastic ecosystem, we recognize two distinct domains of data exchange. **GraphQL is the lingua franca of the real-time API layer**, governing all inter-service communication through the supergraph. For the specific challenge of *bulk data ingestion*, **RDF serves as the high-performance lingua franca**, acting as the intermediate format between the extractor and the Dgraph loaders. This document focuses on the latter as an implementation strategy.

#### 2.1. The Impedance Mismatch: Understanding GraphQL's Design for Bulk Data

GraphQL's primary strength is empowering clients to request precisely the data they need, preventing the over-fetching common in traditional REST APIs. This is exceptionally efficient for user interfaces and client-side applications. However, a bulk data migration is the conceptual opposite: the goal is to retrieve *all* data for one or more types. This creates a fundamental "impedance mismatch" where a protocol optimized for fetching fragments is tasked with exporting an entire graph. This necessitates a purpose-built architecture that respects the protocol's constraints while achieving the goal of complete data extraction.

#### 2.2. The Macroscopic "N+1 Query Problem" and Its Avoidance

A naive attempt to export a graph by recursively traversing its relationships (e.g., fetch packages, then for each package, fetch its dependencies) results in the "N+1 query problem" at a massive scale. This leads to an exponential explosion of network requests that would cripple the source API.

The chosen architecture avoids this entirely by adopting a systematic, type-by-type extraction process. It fetches all instances of a given type (e.g., all `Artifact` nodes) in a single, efficient, paginated operation. Different types can be exracted and processed in parallel or serially depending on the GUAC source system's capacity.

#### 2.3. On Unique Identifiers: Specification vs. Convention

A critical aspect of data aggregation and caching is object identity. The GraphQL specification is intentionally unopinionated on this, providing an `ID` scalar type but not mandating its use.

##### 2.3.1. The Global Object Identification (GOI) Pattern

The widespread assumption that every object must have a globally unique `id` stems from a powerful community convention known as the Global Object Identification (GOI) Specification. This pattern, popularized by frameworks like Relay, establishes a contract that enables robust client-side caching, standardized data refetching, and advanced architectures like federation. It guarantees that an `id` is not just unique to its type, but unique across the entire schema.

##### 2.3.2. Strategies for Synthesizing Globally Unique IDs

When a source system does not provide globally unique and stable identifiers (as is the case with GUAC), a new canonical identifier must be synthesized. The most robust pattern is to combine the object's type name with its natural, business-logic key(s) and encode the result. For example, an `Artifact` with a SHA256 digest of `f0e0...` could be synthesized into an opaque ID like `base64('Artifact:f0e0...')`. This synthesized ID becomes the true global identifier for the entity in the target system.

### 3. The GUAC GraphQL Schema: An In-Depth Analysis

A deep understanding of the GUAC schema's structure and semantics is essential to inform the migration strategy, particularly the classification of types and the identification of natural keys for deduplication. For a complete reference, please consult the official [GUAC GraphQL Documentation](https://docs.guac.sh/graphql/) and the [GUAC Ontology Documentation](https://docs.guac.sh/guac-ontology/), which are the canonical sources for this information. This design document will summarize the key aspects relevant to the migration pipeline.

The following diagrams from the GUAC documentation are particularly useful for understanding the core concepts.

##### Figure 1: GUAC Ontology evidence tree.
![GUAC Evidence Tree](../images/guac-ontology-evidence-tree.png)
*Source: GUAC Ontology Documentation (https://docs.guac.sh/guac-ontology/)*

##### Figure 2:Package Tree Data Model.
![GUAC Package Version Data Model](../images/guac-package-tree-datamodel.png)
*Source: GUAC GraphQL Documentation (https://docs.guac.sh/graphql/)*

#### 3.1. The GUAC Ontology: Deconstructing Nodes (Nouns) and Edges (Verbs)

The GUAC documentation frames its ontology in terms of "nouns" (the core, independent entities) and "verbs" (the evidence-based relationships that connect them). This model aligns with the architectural need for a two-stage migration that preserves relational integrity. The "nouns" must be ingested in Stage 1 before the "verbs" that connect them can be created in Stage 2.

#### 3.2. Classification of GUAC Types for Staged Migration

To operationalize this concept, each GUAC type is classified for processing in either Stage 1 or Stage 2.

**Stage 1: Node Ingestion (The "Nouns")**

| GUAC Type | Description | Natural Key(s) for Deduplication |
| :---- | :---- | :---- |
| **Package** | A software package, represented as a pURL trie. | type, namespace, name, version, qualifiers, subpath (The full pURL components) |
| **Source** | A source code repository, represented as a trie. | type, namespace, name, tag/commit |
| **Artifact** | A specific file or binary. | algorithm, digest |
| **Builder** | An entity that builds an artifact. | uri |
| **Vulnerability** | A vulnerability, represented as a trie. | type, vulnerabilityID. (Replaces separate OSV/CVE/GHSA types). |
| **License** | A software license. | name (e.g., SPDX ID or LicenseRef-\<guid\>) |

**Stage 2: Edge Ingestion (The "Verbs")**

| GUAC Type | Description | Connects |
| :---- | :---- | :---- |
| **CertifyGood/Bad** | An assertion of trust about a subject. | Package/Source/Artifact |
| **CertifyVuln** | Links a package to a known vulnerability. | Package → Vulnerability |
| **CertifyVEXStatement** | A VEX statement about a vulnerability's impact. | Package/Artifact → Vulnerability |
| **IsDependency** | Represents a dependency between two packages. | Package → Package |
| **HasSourceAt** | Links a package to its source repository. | Package → Source |
| **HasSLSA** | SLSA attestation for an artifact. | Artifact → Builder, Artifact |
| **IsOccurrence** | Links an artifact to a package or source. | Artifact → Package/Source |
| **HashEqual** | Asserts two artifacts are the same. | Artifact → Artifact |
| **PkgEqual** | Asserts two packages are the same. | Package → Package |
| **VulnEqual** | Asserts two vulnerabilities are the same. | Vulnerability → Vulnerability |
| **HasSBOM** | Attestation of an SBOM for a subject. | Package/Artifact → (Contains other attestations) |
| **CertifyScorecard** | Attestation of an OSSF Scorecard for a source. | Source → (Scorecard data) |
| **CertifyLegal** | Attestation of legal/license info for a subject. | Package/Source → License |
| **HasMetadata** | Attestation of generic key-value metadata. | Package/Source/Artifact |
| **PointOfContact** | Attestation of contact information for a subject. | Package/Source/Artifact |
| **VulnerabilityMetadata** | Adds scoring (e.g., CVSS) to a vulnerability. | Vulnerability → (Scoring data) |

#### 3.3. Critical Finding: The Unsuitability of GUAC's `id` for Data Aggregation

A pivotal finding from the schema analysis is that the `id: ID!` field on GUAC objects is an **"opaque, backend-specific identifier and should not be shared across different GUAC instances."**

This is a critical architectural constraint. It means that the same conceptual entity (e.g., the `log4j-core` package) will have a different `id` value in each source GUAC instance. Using this field as a key for deduplication would fail catastrophically, creating duplicate entities instead of merging them.

**Therefore, the GUAC `id` field must be disregarded for identity purposes.** The migration pipeline must synthesize a new, canonical identifier for each entity based on its natural key, as defined in the table above.

#### 3.4. Identifying Natural Keys for Canonical Identification and Deduplication

The natural keys identified in the tables above are the cornerstone of the data aggregation strategy. They represent the business-logic definition of uniqueness for each entity and will be used in the target Dgraph schema to enable automatic, idempotent upserts.

### 4. Target Architecture: The Dgraph GraphQL Endpoint

The "Transform" phase of the ETL process adds significant value by creating a target Dgraph endpoint that is a functional and performance upgrade over the source. This is achieved through automated provisioning and strategic schema augmentation.

The following diagram illustrates this process, from acquiring the source schema to deploying an augmented, high-performance version to Dgraph.

**UPDATE GraphQL Mesh (https://the-guild.dev/graphql/mesh) will serve as the "augmentation script" in the diagram below (TODO: update diagram)**

```mermaid
graph TD
    subgraph "1. Schema Acquisition"
        A[GUAC GraphQL API]
        C[GUAC .graphql File]
        B([GUAC Schema SDL])
        A -- "Introspection" --> B
        C -- "File Load" --> B
    end

    subgraph "2. Schema Transformation"
        D{Augmentation Script}
        E([Augmentation Config<br/>e.g., YAML])
        F([Augmented Schema<br/>with @id, @search, @key])
        E -- "Provides rules to" --> D
        B -- "Input to" --> D
        D -- "Outputs" --> F
    end

    subgraph "3. Dgraph Provisioning"
        G[(Dgraph Instance)]
        H(Dgraph GraphQL Endpoint)
        F -- "POST /admin/schema" --> G
        G -- "Generates" --> H
    end
```

#### 4.1. Automated Provisioning via Schema Definition Language (SDL)

The migration pipeline begins by acquiring the source GUAC schema. The preferred method is to use GraphQL's standard introspection system. However, as introspection is often disabled in production for security, the tool must support a fallback mechanism where an operator can provide a pre-exported `.graphql` SDL file.

Once acquired, this SDL file is used to provision the target Dgraph instance. Dgraph is a native GraphQL database that can generate a complete, production-ready GraphQL API directly from an SDL schema via its `/admin/schema` endpoint.

#### 4.2. Schema Augmentation: Transforming the GUAC Schema for Dgraph

Simply copying the source schema is insufficient. The most critical value-add step is to programmatically transform the source SDL to inject Dgraph-specific directives before uploading it. This unlocks Dgraph's native features for performance and deduplication.

##### 4.2.1. Enabling Deduplication with the `@id` Directive

The core requirement of data deduplication is solved by adding the `@id` directive to the natural key fields of each "Node" type. This tells Dgraph to treat that field (e.g., `digest` on the `Artifact` type) as a unique identifier. When a mutation arrives, Dgraph will use this key to efficiently find and update an existing node instead of creating a duplicate.

##### 4.2.2. Enabling High-Performance Queries with the `@search` Directive

To ensure the aggregated graph is fast and responsive for analytical queries, the `@search` directive is added to fields that will be commonly used in filters. This instructs Dgraph to build the appropriate indexes, preventing slow, full-scan operations and enabling capabilities like full-text search, term matching, and regular expressions.

##### 4.2.3. Fulfilling the Federation Contract with the `@key` Directive

Beyond Dgraph-specific optimizations, we **must** also add the standard Apollo Federation `@key` directive to fulfill the Spoke's external contract with the supergraph. This directive serves a different but complementary purpose to Dgraph's `@id`.

*   **`@id` (Internal Contract):** This is a Dgraph-specific directive that enables its native upsert functionality and creates a unique index. It is an *internal contract* with the database engine itself.
*   **`@key` (External Contract):** This is a Federation specification directive that declares how an entity can be uniquely identified by an external federation gateway. It is an *external contract* that allows Dgraph to act as a compliant "subgraph" in a larger, distributed graph.
By adding the `@key` directive now, we gain significant future flexibility at virtually no cost. It makes our Dgraph instance "federation-ready" from day one. If we later choose to evolve the architecture, no schema changes will be required. The presence of `@key` has no negative impact on a standalone Dgraph instance's performance or functionality.

#### 4.3. The Augmented GUAC Schema for Dgraph

The following table provides a prescriptive model for the schema augmentation process, linking each directive to a specific project requirement.

| GUAC Type | Field(s) for @key | Proposed Dgraph Directives | Rationale / Federation Impact |
| :---- | :---- | :---- | :---- |
| **Package** | purl (on PackageVersion) | @id on purl | The full PURL is the only true globally unique identifier for a package version. This makes PackageVersion the entity that can be referenced by other subgraphs. |
| **Source** | type, namespace, name, tag, commit (on SourceName) | @id on each field | A composite key is needed. The SourceName level (which includes optional tag/commit) is the specific, addressable entity for federation. |
| **Artifact** | digest | @id @search(by: [hash]) | Simple, effective primary key for both Dgraph and Federation. |
| **Builder** | uri | @id @search(by: [hash]) | Simple, effective primary key. |
| **Vulnerability** | type, vulnerabilityID (on VulnerabilityID type) | @id on each field | A composite key is required. The VulnerabilityID node is the specific, addressable entity. |
| **License** | name | @id @search(by: [hash]) | The license identifier is the natural key. |

### 5. The Migration Pipeline: A Detailed Architectural Blueprint

#### 5.1. High-Level Architecture: A Decoupled, Two-Phase ETL Process

Before detailing the internal ETL process, it's important to visualize how this entire pipeline is encapsulated within the Spoke boundary, adhering to the 'Subgraphs Are Standalone' principle.

```mermaid
graph TD
    %% Diagram illustrating the "Spoke as a Black Box" principle.
    
    SupergraphGateway(Supergraph Gateway)

    subgraph "Spoke Boundary [subgraph-dgraph-software-supply-chain]"
        direction TB
        
        SpokeEndpoint("Public GraphQL Endpoint<br/>:8080/graphql")
        
        %% --- Internal ETL Process Components ---
        GUAC[GUAC API Source] --> Extractor[Extractor Tool]
        Extractor --> RDF([RDF Artifact])
        RDF --> Loader[Dgraph Loader]
        Loader --> Dgraph[(Dgraph Cluster)]
        %% -------------------------------------

        SpokeEndpoint -- "Serves data from" --> Dgraph
    end

    SupergraphGateway -- "Federated GraphQL Query" --> SpokeEndpoint
```

The following sections detail the architecture of the 'Internal ETL Process' shown above. The architecture separates data extraction from data loading via an intermediate file-based representation (compressed RDF N-Quads). This modularity makes the pipeline resilient, scalable, and allows for the aggregation of data from multiple GUAC sources before a single load operation.

```mermaid
graph LR
    %% Diagram showing the decoupled, two-phase ETL process.

    subgraph Sources
        GUAC1[GUAC Instance 1]
        GUAC2[GUAC Instance 2]
        GUACN[...]
    end

    subgraph Phase 1: Extract & Transform
        Extractor{Extractor Script}
    end

    subgraph Intermediate Artifacts
        RDF1([GUAC1_data.rdf.gz])
        RDF2([GUAC2_data.rdf.gz])
        RDFN([...])
    end

    subgraph Phase 2: Load
        Loader{Dgraph Loaders}
    end

    subgraph Target Database
        Dgraph[(Dgraph Cluster)]
    end

    GUAC1 --> Extractor
    GUAC2 --> Extractor
    GUACN --> Extractor

    Extractor --> RDF1
    Extractor --> RDF2
    Extractor --> RDFN

    RDF1 --> Loader
    RDF2 --> Loader
    RDFN --> Loader

    Loader --> Dgraph
```

#### 5.2. Phase 1: Data Extraction & Transformation (GUAC API → RDF Files)

The first phase consists of a custom script responsible for extracting the complete graph from a GUAC instance and transforming the data into a format optimized for Dgraph's loaders.

##### 5.2.1. Core Engine: A Parallelized, Two-Stage Approach

The extractor script operates in two distinct stages to ensure relational integrity, with the work in each stage parallelized for maximum performance.

*   **Stage 1 (Parallel Node Extraction)**: The script first identifies all "Node" types. A pool of concurrent workers processes these types in parallel. Each worker is responsible for extracting all data for a given type and streaming the transformed RDF data to an output file.
*   **Stage 2 (Parallel Edge Extraction)**: Only after all node extraction is complete does this stage begin. The worker pool then processes all "Edge" types in parallel, confident that the constituent nodes for every relationship already exist.

##### 5.2.2. Query Strategy: High-Fidelity Pre-defined Queries

To ensure robustness and fidelity, the primary extraction strategy leverages the pre-existing `.graphql` files from the GUAC project itself. The extractor script uses a library like The Guild's `@graphql-tools/load` to parse these known, optimized queries. This approach is simpler and guarantees the tool is always in sync with the GUAC API's intended usage.

As an advanced feature, the tool may include optional support for dynamic query generation (disabled by default) for use cases where pre-defined queries are not available.

##### 5.2.3. Extractor Process Flow

```mermaid
sequenceDiagram
   participant Extractor as Extractor Script
   participant GUAC_Files as GUAC Project Files (.graphql)
   participant GUAC_API as GUAC GraphQL API
   participant RDF_Files as RDF Output Files

   Extractor->>GUAC_Files: 1. Load Pre-defined Queries (`loadDocuments`)
   Extractor->>GUAC_API: 2. Load Schema via Introspection (`loadSchema`)
   GUAC_API-->>Extractor: Returns Schema
   Extractor->>Extractor: 3. Classify Types (Nodes vs. Edges)

   loop For Each Node Type (in parallel)
       Extractor->>GUAC_API: 4a. Execute Pre-loaded Query for Node Type (with pagination)
       GUAC_API-->>Extractor: Returns JSON Data for Nodes
       Extractor->>Extractor: 4b. Transform JSON to RDF N-Quads
       Extractor->>RDF_Files: 4c. Stream RDF to compressed output file (*.rdf.gz)
   end

   Note over Extractor: Synchronize: Wait for all Node workers to complete.

   loop For Each Edge Type (in parallel)
       Extractor->>GUAC_API: 5a. Execute Pre-loaded Query for Edge Type (with pagination)
       GUAC_API-->>Extractor: Returns JSON Data for Edges
       Extractor->>Extractor: 5b. Transform JSON to RDF N-Quads
       Extractor->>RDF_Files: 5c. Stream RDF to compressed output file (*.rdf.gz)
   end
```

#### 5.3. Phase 2: Data Loading (RDF Files → Dgraph)

This phase uses Dgraph's purpose-built native tooling to load the generated RDF files. The correct tool must be chosen based on the state of the target cluster.

```mermaid
graph TD
   A[Start Loading] --> B{Is Dgraph cluster new and empty?};
   B -- Yes --> C[Use 'dgraph bulk' loader ONCE for initial population];
   C --> F[Dgraph Populated];
   B -- No --> E[Use 'dgraph live' loader for incremental updates];
   E --> F;
```

##### 5.3.1. Initial Population: Dgraph Bulk Loader

*   **When to Use**: For the **one-time** initial population of a brand new, empty Dgraph cluster only.
*   **Process**: The Dgraph Alpha nodes are stopped. The `dgraph bulk` command is run, processing all generated RDF files offline to create the database's low-level data structures. For more details, see the [official Dgraph Bulk Loader documentation](https://docs.hypermode.com/dgraph/admin/bulk-loader).
*   **Rationale**: The bulk loader is by far the fastest method for importing large datasets into a new cluster because it bypasses the transactional overhead of live writes and builds the database's internal data structures directly.

##### 5.3.2. Incremental Updates: Dgraph Live Loader

*   **When to Use**: For all subsequent data loads from CI/CD pipelines into an existing, running Dgraph cluster.
*   **Process**: The `dgraph live` command is run, pointing to the new RDF file(s). The live loader connects to the running cluster and sends mutations to insert the new data. For more details, see the [official Dgraph Live Loader documentation](https://docs.hypermode.com/dgraph/admin/live-loader).
*   **Rationale**: The live loader is designed specifically to import data into a running Dgraph instance. It sends mutations through the standard API, which means it respects all transactional and consistency guarantees. Its upsert capability, driven by the `@id` directive in the schema, is essential for correctly merging and deduplicating data over time without requiring the ETL process to be aware of the data already in the database.

##### 5.3.3. The Deduplication and Upsert Mechanism in Practice

The elegance of this design is that the ETL pipeline itself is stateless regarding deduplication. It does not need to query the target to check if a node exists before loading.

By leveraging the `@id` directive in the augmented schema, the responsibility for deduplication is pushed down to Dgraph. Both the Bulk and Live loaders understand this schema directive and will automatically perform upserts, ensuring that entities with the same natural key are merged, not duplicated.

### 6. Implementation & Operational Guide

#### 6.1. Recommended Technology Stack: Node.js & The Guild's Tooling

The extractor script will be developed in a **Node.js/TypeScript** environment. This choice allows the project to directly leverage the powerful, mature, and well-maintained libraries from The Guild's open-source ecosystem. The following table summarizes the key tools and their roles in a modern GraphQL stack.

| Tool |Documentation | Description |
| :--- |:--- | :--- |
| **GraphQL Hive** |[the-guild.dev/graphql/hive](https://the-guild.dev/graphql/hive) | A schema registry for managing federated graphs, including features for monitoring, analytics, and detecting breaking changes. |
| **GraphQL Mesh** |[the-guild.dev/graphql/mesh](https://the-guild.dev/graphql/mesh) | A library for creating a unified GraphQL API from multiple, disparate sources like REST, gRPC, or other GraphQL schemas. Can act as a federated gateway. |
| **GraphQL CodeGen** |[the-guild.dev/graphql/codegen](https://the-guild.dev/graphql/codegen) | A tool that generates code (e.g., TypeScript types, React hooks) from a GraphQL schema and operations, ensuring type safety. |
| **GraphQL Tools** |[the-guild.dev/graphql/tools](https://the-guild.dev/graphql/tools) | A set of low-level, modular utilities for building, stitching, and mocking GraphQL schemas in a schema-first approach. |
| **GraphQL Inspector** |[the-guild.dev/graphql/inspector](https://the-guild.dev/graphql/inspector) | A tool for validating schemas, detecting breaking changes, and ensuring consistency between schemas and client-side operations. |
| **GraphQL Yoga** |[the-guild.dev/graphql/yoga-server](https://the-guild.dev/graphql/yoga-server) | A fully-featured, performant, and extendable GraphQL server for Node.js, often used to serve schemas created with GraphQL Tools. |

This project will primarily use `@graphql-tools/load` for schema and document loading, and the core `graphql` library for AST manipulation. However, the other tools are noted here as they form the strategic context for the overall architecture.

#### 6.2. Advanced Engineering for Performance and Resilience

##### 6.2.1. Resilient Extraction with Relay-Compliant Cursor-Based Pagination

To efficiently extract large datasets, the client must use cursor-based pagination. The most robust and widely adopted standard for this is the [Relay GraphQL Connections Specification](https://relay.dev/graphql/connections.htm). Adhering to this specification, rather than inventing a custom pagination model, provides numerous benefits.

**What is Relay?**

[Relay](https://relay.dev/) is a JavaScript framework for building data-driven React applications, originally developed by Facebook. It is highly opinionated and designed to be performant at scale. A key part of Relay is its set of conventions for a GraphQL schema, which enable powerful features like client-side caching, data refetching, and pagination. Even when not using the Relay client framework directly, adopting its schema conventions, particularly for pagination, is a best practice.

The core of Relay's pagination model is the "connection" pattern, which standardizes how relationships between nodes are queried. This pattern avoids the performance degradation and data consistency issues inherent in older offset-based techniques (`limit` and `offset`).

*Figure 3: A diagram illustrating the components of a Relay Connection, including edges, nodes, and page info.*
![Relay Connection Diagram](docs/images/relay-connection-spec.png)
*Source: Relay Documentation (https://relay.dev/)*

The client will be designed defensively: it will attempt to use cursor-based pagination first and fall back to offset-based pagination (with a warning) only if the source API does not support the superior Relay-compliant method.

##### 6.2.2. Maximizing Throughput with Buffering

During the transformation step, data should be buffered in memory before being written to the compressed output file. Writing to the filesystem in larger chunks is more efficient than performing many small write operations, improving the overall throughput of the extractor script.

##### 6.2.3. Reliability Patterns: Retries, Backoff, and Rate Limiting

All network clients interacting with the GUAC and Dgraph APIs must be wrapped in a robust error-handling layer. This layer will implement:

*   **Exponential Backoff & Retries**: For transient network errors or temporary server-side errors (HTTP 5xx).
*   **Rate Limiting Awareness**: The client must gracefully handle HTTP 429 "Too Many Requests" responses by pausing requests for the duration specified in the `Retry-After` header.

#### 6.3. Configuration and Operationalization

For operational flexibility and automation in CI/CD environments, all tunable parameters will be configurable via both command-line flags and environment variables.

| Parameter Name | CLI Flag | Environment Variable | Description | Default Value |
| :--- | :--- | :--- | :--- | :--- |
| **Source Endpoint URL** | `--source-url` | `GUAC_TO_DGRAPH_SOURCE_URL` | The URL of the source GUAC GraphQL API. | *(required)* |
| **Source Auth Token** | `--source-auth-token`| `GUAC_TO_DGRAPH_SOURCE_TOKEN` | Bearer token for authenticating with the source API. | `""` |
| **Schema File Path** | `--schema-file` | `GUAC_TO_DGRAPH_SCHEMA_FILE` | Path to a local SDL file. If not provided, use introspection. | *(optional)* |
| **Schema Config Path** | `--schema-config` | `GUAC_TO_DGRAPH_SCHEMA_CONFIG` | Path to a YAML file defining schema augmentations. | `""` |
| **Query Files Path** | `--query-files-path` | `GUAC_TO_DGRAPH_QUERY_FILES_PATH`| Path to the directory containing `.graphql` query files. | `"./queries"` |
| **Output File Path** | `--output-file` | `GUAC_TO_DGRAPH_OUTPUT_FILE` | Path for the generated compressed RDF file. | `"./output.rdf.gz"` |
| **Parallelism Level** | `--parallelism` | `GUAC_TO_DGRAPH_PARALLELISM` | The number of concurrent worker processes. | `8` |
| **Log Level** | `--log-level` | `GUAC_TO_DGRAPH_LOG_LEVEL` | Logging verbosity (debug, info, warn, error). | `"info"` |

In practice, this entire ETL pipeline is operated via the Spoke's `Makefile`, adhering to Graphtastic platform conventions. The extractor tool is containerized and invoked by a `make seed` target. The configuration parameters defined above are passed to the container via the Spoke's `compose.yaml` and `.env` files. This ensures the Spoke's data lifecycle is self-contained, reproducible, and seamlessly integrated into the overall developer control plane.

### 7. Post-Migration Validation & Integrity Audits

After a load operation, it is crucial to verify the integrity and completeness of the data in Dgraph. This is done by running a suite of GraphQL queries against the new endpoint.

#### 7.1. A Framework for Verifying Migration Success

The validation framework consists of three categories of tests:

1.  **Node Completeness**: Verifies that all independent entities were transferred (validates Stage 1).
2.  **Edge Integrity**: Confirms that the relationships between nodes were correctly reconstructed (validates Stage 2).
3.  **Deduplication**: Confirms that the core aggregation logic was successful by ensuring entities from multiple sources are represented only once.

#### 7.2. Validation Queries: Completeness, Integrity, and Deduplication

**Query 1: Data Completeness Check**
*Counts all `Artifact` nodes. This count should be compared against source counts.*

```graphql
query CountArtifacts {
  aggregateArtifact {
    count
  }
}
```

**Query 2: Relationship Integrity Check**
*Finds a specific package and traverses its `certifyVuln` relationship to find associated vulnerabilities.*

```graphql
query FindLog4jVulnerabilities {
  queryPackage(filter: { name: { eq: "log4j-core" } }) {
    versions {
      certifyVuln {
        vulnerability {
          ... on OSV {
            osvId
          }
          ... on CVE {
            cveId
          }
        }
      }
    }
  }
}
```

**Query 3: Deduplication Verification**
*Searches for a specific artifact known to exist in multiple sources. The expected result is a single node, not multiple.*

```graphql
query VerifyArtifactDeduplication {
  queryArtifact(filter: { digest: { eq: "f0e051b716a569871b6963897a8298a63c33b63298a003f32a243a8574d34f0f" } }) {
    algorithm
    digest
  }
}
```

### 8. Conclusion & Strategic Evolution

This architecture provides a robust, high-performance, and reusable solution for migrating GUAC data into a centralized Dgraph instance. By respecting GraphQL's design while leveraging purpose-built tooling for bulk data loading, it transforms isolated data silos into a unified, query-optimized graph.

#### 8.1. Summary of Architectural Recommendations

*   **Adopt the Decoupled, File-Based Pipeline**: For maximum resilience and performance.
*   **Synthesize Canonical Identifiers**: Do not use the source `id` field. Synthesize new IDs from natural keys to enable deduplication.
*   **Implement Schema Augmentation**: Programmatically inject Dgraph's `@id` and `@search` directives to create a high-performance target.
*   **Use High-Fidelity Pre-defined Queries**: Leverage the source project's own queries for reliability and low maintenance.

#### 8.2. Future Enhancement: Incremental (Delta) Migrations

The current design focuses on complete data migrations. A valuable future enhancement would be to support incremental, or "delta," migrations. This would require the source GUAC schema to include timestamp fields (`createdAt`, `updatedAt`). The extractor could then store the timestamp of its last run and query only for records that have changed since, transforming the pipeline from a batch tool into a near-real-time synchronization engine.

#### 8.3. Future Enhancement: Evolving towards GraphQL Federation

While this batch pipeline creates an invaluable analytical store, a more advanced, cloud-native evolution is GraphQL Federation. Instead of periodically moving data, each GUAC service could be enhanced to act as a federated subgraph. A gateway (such as The Guild's Hive Gateway) could then compose a unified, real-time supergraph. This pipeline is a necessary first step, and federation represents a strategic evolution towards a real-time, decentralized data architecture.

### Appendix

#### A. The Guild's Ecosystem: Rationale for the Technology Stack

The Guild is a collective of open-source developers who maintain many of the most critical libraries in the GraphQL JavaScript ecosystem. Their philosophy of modular, unopinionated, and spec-compliant tools makes their libraries a good fit for building this general-purpose migration pipeline.

*   **GraphQL Tools (`@graphql-tools/`)**: This "swiss army knife" provides the essential, low-level primitives for programmatically interacting with GraphQL. Its `loadSchema` and `loadDocuments` functions are central to the extractor's ability to reliably ingest the source schema and queries.
*   **GraphQL Mesh**: While not used in the initial version of this pipeline, GraphQL Mesh is a powerful tool for composing disparate data sources (REST, gRPC, databases) into a federated GraphQL schema. Future iterations of this data interchange framework could use Mesh as a powerful, declarative extractor component.
*   **GraphQL Hive & Hive Gateway**: Hive is an open-source schema registry for managing federated graphs, and Hive Gateway is a high-performance federation runtime. These tools represent the future state of this architecture should it evolve towards a real-time federated model as described in Section 8.3.

By building on the Node.js/TypeScript stack, this project gains access to this entire powerful, open-source, and well-supported ecosystem.

#### B. List of Works Cited / Additional Resources

This appendix provides a curated and categorized list of resources referenced throughout this design document. These links point to official specifications, technical documentation, and architectural discussions that form the foundation for the design choices made herein.

##### GraphQL Foundational Concepts & Specifications

*   [**GraphQL Official Specification**](https://spec.graphql.org/draft/) - The formal, canonical specification for the GraphQL language. Essential for understanding the core mechanics of the type system, query execution, and introspection.
*   [**Schemas and Types**](https://graphql.org/learn/schema/) - The official introduction to the GraphQL Schema Definition Language (SDL), including object types, scalars, and interfaces.
*   [**Introspection**](https://graphql.org/learn/introspection/) - The official documentation explaining how GraphQL's introspection system works, allowing a schema to be queried for its own structure. This is the mechanism that powers the pipeline's automated schema discovery.
*   [**Pagination**](https://graphql.org/learn/pagination/) - A high-level overview of GraphQL pagination concepts, including the arguments for using cursor-based pagination over offset-based methods for large, dynamic datasets.

##### GraphQL Architectural Patterns & Best Practices

*   [**Global Object Identification**](https://graphql.org/learn/global-object-identification/) - The official documentation page for the `Node` interface convention, which is the cornerstone of modern client-side caching and data refetching.
*   [**GraphQL Global Object Identification Specification (Relay)**](https://relay.dev/graphql/objectidentification.htm) - The formal specification for the Global Object Identification pattern, detailing the contract for the `Node` interface and the `node` root field.
*   [**How to implement Global Object Identification**](https://sophiabits.com/blog/how-to-implement-global-object-identification) - A practical guide on strategies for creating globally unique IDs, including the base64 encoding pattern referenced in this document.
*   [**Caching**](https://graphql.org/learn/caching/) - The official documentation explaining the role of unique identifiers in enabling client-side caching and data normalization.
*   [**GraphQL Federation**](https://graphql.org/learn/federation/) - An introduction to the concept of GraphQL Federation, the architectural pattern for composing a unified supergraph from multiple independent subgraphs. This is the strategic evolution path for this architecture.
*   [**Why You Should Disable GraphQL Introspection In Production**](https://www.apollographql.com/blog/why-you-should-disable-graphql-introspection-in-production) - A security-focused article explaining the rationale for disabling introspection in production environments, justifying the pipeline's need for a file-based schema fallback.

##### Project-Specific Technologies: GUAC & Dgraph

*   [**GUAC GitHub Repository**](https://github.com/guacsec/guac) - The official source code and community hub for the Graph for Understanding Artifact Composition (GUAC) project.
*   [**GUAC GraphQL Documentation**](https://docs.guac.sh/graphql/) - The official documentation for the GUAC GraphQL API, including schema details and query examples.
*   [**GUAC Ontology**](https://docs.guac.sh/guac-ontology/) - A detailed explanation of the "nouns" and "verbs" that make up the GUAC data model, which directly informs the pipeline's two-stage extraction strategy.
*   [**Dgraph GraphQL API Overview**](https://docs.hypermode.com/dgraph/graphql/overview) - The main documentation page for Dgraph's native GraphQL functionality.

##### The Guild's Ecosystem & Tooling

*   [**GraphQL Tools**](https://the-guild.dev/graphql/tools) - The official homepage for GraphQL Tools, the foundational "swiss army knife" library used for programmatic schema and document manipulation.
*   [**Loading GraphQL Schemas (GraphQL Tools)**](https://the-guild.dev/graphql/tools/docs/schema-loading) - The official documentation for the `@graphql-tools/load` functions used to introspect a remote schema.
*   [**Loading GraphQL Documents (GraphQL Tools)**](https://the-guild.dev/graphql/tools/docs/documents-loading) - The official documentation for loading and parsing `.graphql` files, which is the core of the pipeline's high-fidelity query strategy.
*   [**Introducing GraphQL Mesh v1 and Hive Gateway v1**](https://the-guild.dev/graphql/hive/blog/graphql-mesh-v1-hive-gateway-v1) - The blog post detailing the architectural philosophy of The Guild, including the strategic separation of data composition (Mesh) and serving (Gateway).

##### Dgraph Implementation & Administration

*   [**Dgraph Bulk Loader Documentation**](https://docs.hypermode.com/dgraph/admin/bulk-loader) - The official documentation for the `dgraph bulk` command, the high-performance offline tool used for the initial population of a new Dgraph cluster.
*   [**Dgraph Live Loader Documentation**](https://docs.hypermode.com/dgraph/admin/live-loader) - The official documentation for the `dgraph live` command, the online tool used for all subsequent, incremental data loads into a running cluster.
*   [**Mutate performance optimization (Dgraph Discuss)**](https://discuss.dgraph.io/t/mutate-performance-optimization/5517) - A critical forum post explaining best practices for optimizing write throughput in Dgraph, including the importance of batching mutations.
*   [**Dgraph Schema Directives Overview**](https://docs.hypermode.com/dgraph/graphql/schema/directives/overview) - The documentation detailing Dgraph-specific GraphQL directives like `@id` and `@search`, which are essential for the schema augmentation strategy.
*   [**Dgraph GraphQL and DQL schemas**](https://docs.hypermode.com/dgraph/graphql/schema/graphql-dql) - A guide explaining how Dgraph's GraphQL schema maps to its underlying DQL (Dgraph Query Language) schema predicates.

### **Appendix C: Analysis of GUAC Source GraphQL Files**

The migration pipeline's high-fidelity query strategy relies on using the pre-defined `.graphql` files from the GUAC source repository. This appendix provides a catalog and analysis of these files, located in the [`guac/pkg/assembler/graphql/schema/`](https://github.com/guacsec/guac/blob/main/pkg/assembler/graphql/schema) directory.

#### **C.1. Summary of GUAC GraphQL Schema Files**

This table provides a high-level overview of each schema file and its purpose. Files containing root `Query` definitions are the primary source for the extractor script.

| File Name | Description |
| :--- | :--- |
| **`articulation.graphql`** | Defines the `IsOccurence` type to represent an artifact that is an occurrence of a source or package. |
| **`backend.graphql`** | Defines types for backend-specific information, such as the `ID` type used internally by GUAC. |
| **`builder.graphql`** | Defines the `Builder` type, representing the entity that builds an artifact. |
| **`certification.graphql`** | Defines `CertifyGood` and `CertifyBad` types for trust assertions, and `CertifyScorecard`. **Contains root queries.** |
| **`certify_vex.graphql`** | Defines the `CertifyVEXStatement` type for Vulnerability Exploitability eXchange attestations. **Contains root queries.** |
| **`certify_vuln.graphql`** | Defines the `CertifyVuln` type, which links a package to a vulnerability. **Contains root queries.** |
| **`dependency.graphql`** | Defines the `IsDependency` type to represent a dependency between two packages. **Contains root queries.** |
| **`equality.graphql`** | Defines `PkgEqual`, `HashEqual`, and `VulnEqual` types for asserting that two entities are the same. **Contains root queries.** |
| **`license.graphql`** | Defines `License` and `CertifyLegal` types for software license information. **Contains root queries.** |
| **`occurrence.graphql`** | Defines `IsOccurrence` type, linking an artifact to a package/source. **Contains root queries.** |
| **`package.graphql`** | Defines the pURL-based `Package` type and its components (`PackageName`, `PackageVersion`, etc.). **Contains root queries.** |
| **`point_of_contact.graphql`**| Defines types for specifying a point of contact for a package, source, or artifact. **Contains root queries.** |
| **`slsa.graphql`** | Defines types for SLSA (Supply-chain Levels for Software Artifacts) attestations. **Contains root queries.** |
| **`source.graphql`** | Defines the `Source` type for source code repositories and their components. **Contains root queries.** |
| **`vulnerability.graphql`** | Defines vulnerability types (`OSV`, `CVE`, `GHSA`) and a `Vulnerability` union. **Contains root queries.** |

#### **C.2. Detailed GUAC Query Index**

This table lists every root query available in the GUAC API, its source file, and a human-readable description of what it retrieves. This serves as a definitive catalog for the extractor's pre-defined query library.

| Query Name | Source File | Description |
| :--- | :--- | :--- |
| `certifyGood` | `certification.graphql` | Finds `CertifyGood` attestations based on a subject, time, or other criteria. |
| `certifyBad` | `certification.graphql` | Finds `CertifyBad` attestations based on a subject, time, or other criteria. |
| `scorecards` | `certification.graphql` | Retrieves OpenSSF Scorecard results for a given source repository. |
| `certifyVEXStatement` | `certify_vex.graphql` | Finds `CertifyVEXStatement` attestations based on subject, vulnerability, or time. |
| `certifyVuln` | `certify_vuln.graphql` | Finds `CertifyVuln` records linking packages to vulnerabilities. |
| `isDependency` | `dependency.graphql` | Finds dependency relationships between packages. |
| `pkgEqual` | `equality.graphql` | Finds `PkgEqual` attestations that link two different package identifiers as being the same. |
| `hashEqual` | `equality.graphql` | Finds `HashEqual` attestations that link two different artifacts as being the same. |
| `vulnEqual` | `equality.graphql` | Finds `VulnEqual` attestations that link two different vulnerability IDs as being the same. |
| `licenses` | `license.graphql` | Finds `License` objects based on a filter. |
| `certifyLegal` | `license.graphql` | Finds `CertifyLegal` attestations linking subjects to licenses. |
| `isOccurrence` | `occurrence.graphql` | Finds `IsOccurrence` attestations linking artifacts to packages or sources. |
| `packages` | `package.graphql` | Finds `Package` objects based on a pURL filter. |
| `hasSourceAt` | `package.graphql` | Finds `HasSourceAt` attestations linking packages to their source repositories. |
| `hasSBOM` | `package.graphql` | Finds SBOM (Software Bill of Materials) attestations for a given subject. |
| `pointOfContact` |`point_of_contact.graphql`| Finds Point of Contact information (email, info, justification) for a given subject.|
| `hasSLSA` | `slsa.graphql` | Finds SLSA provenance attestations for a given subject. |
| `sources` | `source.graphql` | Finds `Source` objects based on a filter. |
| `artifacts` | `source.graphql` | Finds `Artifact` objects based on their algorithm and digest. |
| `osv` | `vulnerability.graphql` | Finds `OSV` vulnerability objects based on their ID. |
| `cve` | `vulnerability.graphql` | Finds `CVE` vulnerability objects based on their ID. |
| `ghsa` | `vulnerability.graphql` | Finds `GHSA` vulnerability objects based on their ID. |
| `vulnerabilities` |`vulnerability.graphql` | Finds vulnerabilities of any type, allowing filtering by a specific vulnerability ID without knowing its type. |
</file>

<file path="README.md">
# Subgraph: Dgraph Software Supply Chain

> ## 🚧 **Architectural Blueprints Ahead! 🚀**
>
> Welcome! This repository contains a **work-in-progress** implementation. While the code is still evolving, the architectural and design documentation here represents our north star for building a high-performance, federated graph for software supply chain security. We're excited to have you here and welcome you to explore the designs and join us in the discussions on CNCF Slack in [#initiative-supply-chain-security-insights](https://cloud-native.slack.com/archives/C09A8VBEUNM).

## 1. Overview

This repository contains the source code, operational configuration, and detailed architectural documentation for the `subgraph-dgraph-software-supply-chain`. This service is a foundational ["Spoke"](https://github.com/graphtastic/platform/blob/main/docs/design/tome--graphtastic-platform-docker-compose.md#31-the-hub-and-spoke-model) within the [Graphtastic Platform](https://github.com/graphtastic/platform/blob/main/README.md#the-graphtastic-platform), designed to provide a high-performance, federated GraphQL API for software supply chain security data.

### 1.1. Strategic Goals

This subgraph serves two primary, strategic purposes:

1.  **CNCF Demonstrator:** To act as a reference implementation and standalone demonstrator for the [CNCF Software Supply Chain Insights initiative](https://github.com/cncf/toc/issues/1709).
2.  **Federated Spoke:** To function as a fully compliant, federated Spoke within the Graphtastic supergraph, aggregating and exposing security data from multiple sources.

### 1.2. Core Technology

Our architecture is built on a foundation of powerful, cloud-native technologies:

*   **Data Source:** [GUAC (Graph for Understanding Artifact Composition)](https://guac.sh/) instances, which generate rich software supply chain metadata.
*   **Persistence Layer:** [Dgraph](https://dgraph.io/), a distributed, native GraphQL graph database chosen for its horizontal scalability and high-performance query capabilities.
*   **API Layer:** A GraphQL API designed from the ground up to be compliant with the [Apollo Federation](https://www.apollographql.com/docs/federation/) specification.
*   **Development Environment:** A modular, multi-stack [Docker Compose](https://docs.docker.com/compose/) environment, orchestrated with `make` for a seamless and reproducible developer experience.

## 2. Architecture

This subgraph follows a decoupled, schema-driven ETL (Extract, Transform, Load) architecture to ingest data. The entire ingestion pipeline is an internal implementation detail, encapsulated within the Spoke's boundary. This "Spoke as a Black Box" approach is a core principle of the Graphtastic Platform, ensuring that the Spoke's public contract is its GraphQL API and nothing more.

```mermaid
graph LR

    %% -- DIAGRAM DEFINITION --

    %% An external component, the entry point for all queries.
    SupergraphGateway(Supergraph Gateway)

    subgraph "Spoke Boundary [subgraph-dgraph-software-supply-chain]"
        direction LR

        %% The public-facing contract of this Spoke.
        SpokeEndpoint("fa:fa-plug Public GraphQL Endpoint<br/>:8080/graphql")

        subgraph "Internal ETL & Persistence"
            direction LR

            %% The internal data processing pipeline.
            GUAC[GUAC API Source] --> Mesh[GraphQL Mesh]
            Mesh --> Extractor[Extractor Tool]
            Extractor --> RDF([RDF Artifact])
            RDF --> Loader[Dgraph Loader]
            Loader --> Dgraph[(fa:fa-database Dgraph Cluster)]
        end

        %% Data flows from the internal Dgraph to the public endpoint.
        Dgraph -- "Serves data from" --> SpokeEndpoint

        %% CLIPPING FIX: This invisible node acts as a spacer, forcing the layout
        %% engine to allocate more horizontal space and preventing clipping.
        SpokeEndpoint --- InvisibleSpacer
    end

    %% -- ADDITIONAL CONTEXTUAL SUBGRAPHS --
    subgraph "Other Spokes (subgraphs)"
        direction TB
        GitHubArchive("fa:fa-github GitHub Archive")
        Blogs("fa:fa-rss Blogs")
    end

    %% Define the relationships to the new contextual sources.
    SupergraphGateway -- "Federates" --> GitHubArchive
    SupergraphGateway -- "Federates" --> Blogs

    %% Queries flow from the Gateway to our Spoke's endpoint.
    SupergraphGateway -- "Federated GraphQL Query" --> SpokeEndpoint

    %% -- ANNOTATION --
    %% A non-directional link positioned below the main flow clearly marks this as an annotation.
    Mesh -.- MeshNote(
        fa:fa-cogs Transforms Upstream API: <br/>
        - Fixes global identity <br/>
        - Replaces Node-Union anti-pattern <br/>
        - Adds federation directives
    )

    %% -- STYLING & THEME SUPPORT --
    %% classDef is used to create theme-aware styles for visual hierarchy.
    classDef external fill:#f8f9fa,stroke:#6c757d,stroke-width:2px,stroke-dasharray: 5 5,color:#495057
    classDef endpoint fill:#e7f5ff,stroke:#1c7ed6,stroke-width:3px,color:#1864ab
    classDef annotation fill:#fff9db,stroke:#fcc419,stroke-width:2px,color:#c28c0c
    classDef invisible stroke:none,fill:none

    %% Applying the classes to the nodes.
    class SupergraphGateway,GitHubArchive,Blogs external
    class SpokeEndpoint endpoint
    class MeshNote annotation
    class InvisibleSpacer invisible
```

### 2.1. Data Ingestion Flow

1.  **Extract & Transform:** A high-fidelity extractor tool queries one or more source GUAC GraphQL APIs. It uses GUAC's own pre-defined query files to ensure robustness. The extracted data is transformed into compressed RDF N-Quad files, a format optimized for Dgraph's loaders.
2.  **Load:** The generated RDF files are loaded into Dgraph using its high-performance native tooling (`dgraph bulk` for initial population and `dgraph live` for incremental updates).
3.  **Deduplication:** The Dgraph schema is programmatically augmented with `@id` directives on natural business keys. This pushes the responsibility for data deduplication down into the database layer, enabling idempotent upserts and ensuring that data from multiple GUAC sources is correctly merged into a single, canonical entity.

For a complete breakdown of this architecture, please see the core implementation design:

*   [**`docs/design/design--guac-to-dgraph.md`**](./docs/design/design--guac-to-dgraph.md)

## 3. Local Development

### 3.0. Makefile Targets: Developer Control Plane

All operational tasks are orchestrated via the Makefile. **Never run `docker` or `docker compose` directly—always use these targets.**

#### **Main Targets**

| Target                    | Description |
| :------------------------ | :---------- |
| `make help`               | Show this help message and a summary of all targets. |
| `make setup`              | Prepare the local environment (e.g., check `.env`, create necessary Docker networks). |
| `make up`                 | Bring up all services (full stack). |
| `make down`               | Bring down all services. |
| `make clean`              | Run `down`, then remove all containers, networks, persistent data, and benchmarking data. |
| `make status`             | Show container, network, and volume status (diagnostics). |
| `make logs`               | Tail logs for all running services. |

#### **Service Management**

| Target                                                     | Description |
| :--------------------------------------------------------- | :---------- |
| `make svc-up SVC=compose/xx.yml [SERVICE=service]`         | Bring up a specific compose file (and optionally a service). |
| `make svc-down SVC=compose/xx.yml [SERVICE=service]`       | Bring down a specific compose file (and optionally a service). |
| `make svc-logs SVC=compose/xx.yml [SERVICE=service]`       | Tail logs for a specific compose file (and optionally a service). |

#### **Data Pipeline**

| Target              | Description |
| :------------------ | :---------- |
| `make ingest-sboms` | Ingest SBOMs from the `./sboms` directory into GUAC. (Note: This targets GUAC's internal GraphQL API on `localhost:8080` *within its container*). |
| `make extract`      | Run the ETL script to extract from Mesh and generate RDF. |
| `make seed`         | Perform a full, clean data seed from SBOMs to Dgraph (end-to-end pipeline). |

#### **Benchmarking & Utilities**

| Target                          | Description |
| :------------------------------ | :---------- |
| `make fetch-benchmark-data`     | Download 1million RDF and schemas for benchmarking. |
| `make wipe-dgraph-data`         | Remove all Dgraph persistent data and build output (for benchmarking). |
| `make benchmark-bulk-indexed`   | Benchmark Dgraph bulk loader with indexed schema (wipes data first). |
| `make benchmark-bulk-noindex`   | Benchmark Dgraph bulk loader with no-index schema (wipes data first). |

#### **Other Utilities**

| Target                   | Description |
| :----------------------- | :---------- |
| `make stop-alpha`        | Stop only Dgraph Alpha containers. |
| `make start-alpha`       | Start only Dgraph Alpha containers. |
| `make check-dockerfiles` | Check for missing Dockerfiles referenced in compose files. |
| `make check-envfile`     | Ensure `.env` exists by copying from `.env.example` if needed. |

#### **Dual-Mode Tooling Pattern**

By default, all build, extraction, and schema composition tools are run in containers for reproducibility and CI/CD parity. For local development and fast iteration, you can run these tools natively by setting `USE_LOCAL_TOOLS=1` in your `.env` file:

```bash
echo "USE_LOCAL_TOOLS=1" >> .env
```

With this set, `make extract` and similar targets will run the scripts directly on your host (using your local Node.js environment). Remove or comment out this line to revert to containerized execution.

---

### 3.0. Storage Mode: Bind Mounts Only (for now)

Currently, GUAC Postgres data is stored in a local bind-mounted directory (`./dgraph-stack/guac-data`).

**TODO:** Add support for Docker named volumes for GUAC Postgres data.

-   Data is always stored in `./dgraph-stack/guac-data` on your host.
-   No Docker volume support for GUAC Postgres yet.

---

### 3.1. Network Architecture: Internal vs. External Access

This project uses two Docker networks for security and modularity:

*   **Internal network (`dgraph-net`)**: Used for private communication between Dgraph services (Zero, Alpha, Ratel). Not accessible from outside Docker.
*   **External network (`graphtastic_net`)**: Used to expose selected service endpoints to the host and to other stacks in the Graphtastic platform.

**Service network membership:**

| Service          | Internal (`dgraph-net`) | External (`graphtastic_net`) | Host Accessible? | Purpose                   |
| :--------------- | :---------------------: | :--------------------------: | :--------------: | :------------------------ |
| `dgraph-zero`    |            ✔            |              ✗               |        No        | Cluster coordination only |
| `dgraph-alpha`   |            ✔            |              ✔               |   Yes (API)      | GraphQL/DQL API, data plane |
| `dgraph-ratel`   |            ✔            |              ✔               |   Yes (UI)       | Web UI for admin/dev      |
| `guac-mesh-graphql` |            ✔            |              ✔               |   Yes (API)      | GraphQL Mesh endpoint     |

> **How are Ratel and GraphQL Mesh accessible from the host?**
>
> `dgraph-ratel` is accessible from your host because its service is attached to both the internal `dgraph-net` (for private communication with Dgraph Alpha) **and** the external `graphtastic_net` (which exposes its port to the host and other stacks). The Compose file maps port `8001` to your host, so you can open [http://localhost:8001](http://localhost:8001) in your browser.
>
> Similarly, `guac-mesh-graphql` is attached to `graphtastic_net` and maps port `4000` to your host, making it accessible at [http://localhost:4000/graphql](http://localhost:4000/graphql).

<details>
<summary><strong>How to debug the internal Docker network from the host</strong></summary>

Sometimes you need to inspect or debug services that are only on the internal Docker network (`dgraph-net`). Here are some useful techniques:

**1. Run a temporary debug container on the internal network:**

```bash
docker run -it --rm --network subgraph-dgraph-software-supply-chain_dgraph-net busybox sh
# or for more tools:
docker run -it --rm --network subgraph-dgraph-software-supply-chain_dgraph-net nicolaka/netshoot
```

You can now use tools like `ping`, `nslookup`, or `curl` to reach other containers by their service name (e.g., `dgraph-zero:5080`, `dgraph-alpha:8080`).

**2. Inspect the network and connected containers:**

```bash
docker network inspect subgraph-dgraph-software-supply-chain_dgraph-net
```

This will show which containers are attached and their internal IP addresses.

**3. Exec into a running service container:**

```bash
docker exec -it <container_name> sh
# Example:
docker exec -it dgraph-alpha sh
```

**4. Port-forward a service for temporary host access:**

If you need to access a service that's only on the internal network, you can use `docker port` or set up a temporary port-forward:

```bash
# Example: Forward dgraph-zero's HTTP port to your host
docker run --rm -it --network host alpine/socat TCP-LISTEN:16080,fork TCP:dgraph-zero:6080
# Now access http://localhost:16080 from your host
```

These techniques let you debug, inspect, or interact with internal-only services without changing your Compose files.

</details>

**How to access services:**

*   **From your host (browser or curl):**
    *   Dgraph Ratel UI: [http://localhost:8001](http://localhost:8001)
    *   Dgraph GraphQL Endpoint: [http://localhost:8081/graphql](http://localhost:8081/graphql)
    *   Dgraph DQL API: [http://localhost:8081](http://localhost:8081)
    *   GUAC Mesh GraphQL Endpoint: [http://localhost:4000/graphql](http://localhost:4000/graphql)

*   **From inside a Docker container on `dgraph-net`:**
    *   Use service names: `dgraph-alpha:8080`, `dgraph-zero:5080`, `dgraph-ratel:8000`

*   **From inside a Docker container on `graphtastic_net`:**
    *   Use service names: `dgraph-alpha:8080`, `dgraph-ratel:8000`, `guac-mesh-graphql:4000`
    *   `dgraph-zero` is **not** available on this network for security/isolation.

**Why this matters:**

*   Only the API and UI endpoints you need are exposed to the host and other stacks. All cluster-internal traffic (e.g., Zero <-> Alpha) is isolated for security and reliability.

**Example: Accessing Dgraph from another container**

If you have a service on `graphtastic_net` (e.g., a Mesh gateway), you can connect to Dgraph Alpha at `dgraph-alpha:8080`.

If you are running a script inside the `dgraph-net` network, you can use the same service names, but only `dgraph-alpha` and `dgraph-ratel` are reachable from the external network.

This project uses a modular, multi-stack Docker Compose architecture orchestrated by a central `Makefile` to provide a simple and consistent developer experience.

### 3.1. Prerequisites

*   Docker Engine and the Docker Compose CLI plugin
*   `make`

### 3.2. Getting Started

**WARNING: this doesn't work yet! (WIP)**

1.  **Clone the repository:**

    ```bash
    git clone <this-repo-url>
    cd <this-repo-directory>
    ```

2.  **Configure the environment:**
    Copy the example environment file and customize it if necessary.

    ```bash
    cp .env.example .env
    ```

3.  **Launch the full stack:**
    This single command will create the shared Docker networks and volumes, then bring up the Dgraph cluster, the API service, and any other required components in the correct order.

    ```bash
    make up
    ```

4.  **Verify the services:**
    *   **Dgraph Ratel UI:** [http://localhost:8001](http://localhost:8001)
    *   **Dgraph GraphQL Endpoint:** [http://localhost:8081/graphql](http://localhost:8081/graphql)
    *   **GUAC Mesh GraphQL Endpoint:** [http://localhost:4000/graphql](http://localhost:4000/graphql)

## 3.3. Local vs. Containerized Tooling

By default, all build, extraction, and schema composition tools are run in containers for reproducibility and CI/CD parity. For local development and fast iteration, you can run these tools natively by setting `USE_LOCAL_TOOLS=1` in your `.env` file:

```bash
echo "USE_LOCAL_TOOLS=1" >> .env
```

With this set, `make extract` and similar targets will run the scripts directly on your host (using your local Node.js environment). Remove or comment out this line to revert to containerized execution.

This dual-mode workflow is documented in `PLAN.md` and enforced in the `Makefile`.

---

5.  **Ingest SBOMs and Run the ETL Pipeline**

To ingest SBOMs and load them into Dgraph, follow these steps:

**Step 1: Place your SBOM files**

Copy your SBOM files (e.g., `.spdx.json`, `.cyclonedx.json`) into the `./sboms` directory:

```bash
cp /path/to/your/sbom1.spdx.json ./sboms/
cp /path/to/your/sbom2.cyclonedx.json ./sboms/
```

**Step 2: Ingest SBOMs into GUAC**

Run the following command to ingest all SBOMs in the `./sboms` directory:

```bash
make ingest-sboms
```

**Step 3: Wait for GUAC to finish processing**

Monitor the GUAC containers to ensure ingestion is complete. You can check logs with:

```bash
docker compose -f compose/guac.yml logs -f guac-collectd guac-api guac-graphql```

Once logs indicate processing is finished (or after a reasonable wait), proceed.

**Step 4: Export data from Mesh to RDF (N-Quads)**

Run the extractor script (now located in `guac-mesh-graphql/scripts/extractor.ts`) to pull data from the Mesh GraphQL gateway and generate RDF N-Quads:

```bash
make extract
```

This will create a compressed RDF file in `guac-mesh-graphql/build/guac.rdf.gz`.

**Step 5: Seed Dgraph with the extracted RDF**

The full pipeline (including all steps above) can be run with:

```bash
make seed```

This will clean the environment, bring up all services, ingest SBOMs, extract RDF (from `guac-mesh-graphql/build/guac.rdf.gz`), and (when implemented) load into Dgraph.
> **Note:** All Mesh transformation and extraction logic is now contained within the `guac-mesh-graphql/` subdirectory. This ensures proper Compose layering and allows `compose/guac.yml` to mount and access the extractor and its outputs directly.

---

6.  **Tear down the environment:**
To stop all containers and remove the Docker network, run:

```bash
make down
```

7.  **Perform a full cleanup:**
To stop containers and **permanently delete all persistent data**, run:

```bash
make clean
```

## 4. Project Documentation & Design Philosophy

This repository is not just a collection of code; it is a curated set of architectural patterns and design documents that form our engineering standard. The following documents are essential reading for any contributor to understand not just the "how," but the "why" behind our approach.

### 4.1. Platform Architecture & Philosophy

To understand how this subgraph fits into the larger ecosystem, we highly recommend reading the core Graphtastic Platform documentation.

| Document                                                                                               | Description                                                                                                                                                                                                                                                                                             |
| :----------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Graphtastic Platform README](https://github.com/graphtastic/platform/blob/main/README.md)             | The main entry point for the entire Graphtastic Platform. It outlines the vision of a unified supergraph composed of independent **Spokes**, the central role of the `Makefile` **developer control plane**, and our documentation philosophy centered on **Tomes**. This is the best place to start! |
| [Tome: Graphtastic Platform Docker Compose](https://github.com/graphtastic/platform/blob/main/docs/design/tome--graphtastic-platform-docker-compose.md) | The formal design "Tome" that details the architectural principles and conventions for our multi-stack Docker Compose environments. It explains the "why" behind the modular setup detailed in our practical guides, ensuring every Spoke provides a consistent and seamless developer experience.         |

### 4.2. Core Architectural Designs

| Document                                           | Description                                                                                                                                                                                            |
| :------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`design--guac-to-dgraph.md`](./design--guac-to-dgraph.md) | The definitive implementation plan for this subgraph. It details the schema analysis, the two-phase ETL pipeline, data deduplication strategies, and the schema augmentation process for Dgraph. |

### 4.3. Development and Operations Guides

| Document                                                              | Description                                                                                                                                                                                     |
| :-------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`on--dgraph-docker-compose.md`](./on--dgraph-docker-compose.md)                 | A comprehensive guide to our production-ready Dgraph cluster setup using Docker Compose, detailing the roles of Zero, Alpha, and Ratel, and our strategy for persistent storage.                |
| [`on--running-multiple-docker-compose-stacks.md`](./on--running-multiple-docker-compose-stacks.md) | The architectural blueprint for our modular local development environment. It explains why we avoid monolithic Compose files and how we implement shared networking and storage across stacks. |

### 4.4. GraphQL Primers

| Document                                                                | Description                                                                                                                                                                                                                               |
| :---------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`on--object-identification-in-graphql.md`](./on--object-identification-in-graphql.md) | An in-depth report on the **Global Object Identification (GOI)** specification. This is essential reading to understand how we enable client-side caching, data refetching, and federation via unique `id` fields. |
| [`on--node-union-antipattern.md`](./on--node-union-antipattern.md)                 | A critical analysis of why our schema consistently prefers **`interface`** types over `union` types for modeling polymorphic collections of entities, a decision crucial for schema evolvability and maintainability.        |

## 5. Contributing

Contributions are welcome! Please see our contributing guidelines for more information on how to get involved.

## Licensing

This project is dual-licensed to enable broad code adoption while ensuring our documentation and knowledge base remain open for the community. Project copyright and contributor attribution are managed in our [`NOTICE`](./NOTICE) and [`CONTRIBUTORS.md`](./CONTRIBUTORS.md) files.

*   **Code is licensed under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).** This permissive license allows free use in both open-source and commercial products. The full license text is in [`LICENSE.code`](./LICENSE.code).

*   **Documentation is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).** This requires **Attribution** for our contributors and that derivative works are shared back under the same **ShareAlike** terms. The full license text is in [`LICENSE.docs`](./LICENSE.docs).
</file>

</files>
